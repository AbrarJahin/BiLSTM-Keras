phrase,sentances
data storage,Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval.
hash functions,"Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. !! Use of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision). !! Hash functions rely on generating favourable probability distributions for their effectiveness, reducing access time to nearly constant. !! High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. !! Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval."
function interaction,"Use of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision)."
statistical properties,"Use of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision)."
randomization functions,"Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. !! In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed."
intelligent agent,"Intelligent agents are often described schematically as an abstract functional system similar to a computer program. !! In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. !! In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. !! Leading AI textbooks define ""artificial intelligence"" as the ""study and design of intelligent agents"", a definition that considers goal-directed behavior to be the essence of intelligence. !! They may be simple or complex a thermostat is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
artificial intelligence,"Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. !! In artificial intelligence (AI) and philosophy, AI alignment and the AI control problem are aspects of how to build AI systems such that they will aid rather than harm their creators. !! Applied Artificial Intelligence is a peer-reviewed scientific journal covering applications of artificial intelligence in management, industry, engineering, administration, and education, as well as evaluations of existing AI systems and tools and their economic, social, and cultural impact. !! Artificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states. !! Textual case-based reasoning (TCBR) is a subtopic of case-based reasoning, in short CBR, a popular area in artificial intelligence. !! Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (not by definition) recurrent in its implementation. !! The objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of artificial intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents). !! Cognitive computing (CC) refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. !! In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. !! Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. !! AI Bridging Cloud Infrastructure (ABCI) is a planned supercomputer being built at the University of Tokyo for use in artificial intelligence, machine learning, and deep learning. !! In artificial intelligence, model-based reasoning refers to an inference method used in expert systems based on a model of the physical world. !! The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI. !! Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! Network capable: ability to communicate and bundle (product bundling) with another product (business) or product setsThe vision of smart products poses questions relevant to various research areas, including marketing, product engineering, computer science, artificial intelligence, economics, communication science, media economics, cognitive science, consumer psychology, innovation management and many more. !! In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. !! the problem of creating 'artificial intelligence' will substantially be solved"". !! Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. !! Robotic process automation (RPA) is a form of business process automation technology based on metaphorical software robots (bots) or on artificial intelligence (AI)/digital workers. !! In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. !! Computational archaeology is also known as ""archaeological informatics"" (Burenhult 2002, Huggett and Ross 2004) or ""archaeoinformatics"" (sometimes abbreviated as ""AI"", but not to be confused with artificial intelligence). !! On January 7, 2019, following an Executive Order on Maintaining American Leadership in Artificial Intelligence, the White Houses Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications, which includes ten principles for United States agencies when deciding whether and how to regulate AI. !! Deep Learning Studio is a software tool that aims to simplify the creation of deep learning models used in artificial intelligence. !! Leading AI textbooks define ""artificial intelligence"" as the ""study and design of intelligent agents"", a definition that considers goal-directed behavior to be the essence of intelligence. !! Knowledge representation and reasoning (KRR, KR&R, KR) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. !! Artificial intelligence in fiction often crosses the line to apparent artificial intuition, although it can't be shown if the intent of the fiction creator was to show a simulation of intuition or that real artificial intuition is part of the story's AI, because this depends on the internal structure of the programming of the AI, which is not usually shown in stories. !! Some popular accounts use the term ""artificial intelligence"" to describe machines that mimic ""cognitive"" functions that humans associate with the human mind, such as ""learning"" and ""problem solving"", however, this definition is rejected by major AI researchers. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! Argument technology is a sub-field of artificial intelligence that focuses on applying computational techniques to the creation, identification, analysis, navigation, evaluation and visualisation of arguments and debates. !! The expression is used in the second paragraph with a footnote claiming that ""computational logic"" is ""surely a better phrase than 'theorem proving', for the branch of artificial intelligence which deals with how to make machines do deduction efficiently"". !! Ontology Learning and Population: Bridging the Gap between Text and Knowledge, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2008. !! Fuzzy logic has been applied to many fields, from control theory to artificial intelligence. !! In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. !! Sources disagree about exactly what constitutes ""real"" intelligence as opposed to ""simulated"" intelligence and therefore whether there is a meaningful distinction between artificial intelligence and synthetic intelligence. !! In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. !! Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics. !! The voice and style of the contentThe use of content intelligence is therefore connected to the science of big data and artificial intelligence. !! Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. !! The Artificial Intelligence of Things (AIoT) is the combination of Artificial intelligence (AI) technologies with the Internet of things (IoT) infrastructure to achieve more efficient IoT operations, improve human-machine interactions and enhance data management and analytics. !! In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents[2]. !! The concept of rational agents can be found in various disciplines such as artificial intelligence, cognitive science, decision theory, economics, ethics, game theory, and the study of practical reason. !! The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. !! Embodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. !! In artificial intelligence and related fields, an argumentation framework is a way to deal with contentious information and draw conclusions from it using formalized arguments. !! In 1972 the Metamathematics Unit at the University of Edinburgh was renamed The Department of Computational Logic in the School of Artificial Intelligence. !! Within computer science, bio-inspired computing relates to artificial intelligence and machine learning. !! Similarity learning is an area of supervised machine learning in artificial intelligence. !! Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Although no commercially successful general-purpose computer hardware has used a dataflow architecture, it has been successfully implemented in specialized hardware such as in digital signal processing, network routing, graphics processing, telemetry, and more recently in data warehousing, and artificial intelligence. !! Moravec's paradox is the observation by artificial intelligence and robotics researchers that, contrary to traditional assumptions, reasoning requires very little computation, but sensorimotor and perception skills require enormous computational resources. !! Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems. !! and have been common in fiction, as in Mary Shelley's Frankenstein or Karel apek's R. U. R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence. !! In artificial intelligence, genetic programming (GP) is a technique of evolving programs, starting from a population of unfit (usually random) programs, fit for a particular task by applying operations analogous to natural genetic processes to the population of programs. !! Artificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic. !! Computer algebra systems began to appear in the 1960s and evolved out of two quite different sourcesthe requirements of theoretical physicists and research into artificial intelligence. !! Virtual intelligence is the term given to artificial intelligence that exists within a virtual world. !! The founding concepts behind learning classifier systems came from attempts to model complex adaptive systems, using rule-based agents to form an artificial cognitive system (i. e. artificial intelligence). !! A cognitive architecture refers to both a theory about the structure of the human mind and to a computational instantiation of such a theory used in the fields of artificial intelligence (AI) and computational cognitive science. !! Ontology Learning from Text: Methods, Evaluation and Applications, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2005. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
intelligent agents,"In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents[2]. !! Intelligent agents are often described schematically as an abstract functional system similar to a computer program. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. !! Leading AI textbooks define ""artificial intelligence"" as the ""study and design of intelligent agents"", a definition that considers goal-directed behavior to be the essence of intelligence."
cognitive modeling,"Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
cognitive science,"Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! Artificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic. !! The concept of rational agents can be found in various disciplines such as artificial intelligence, cognitive science, decision theory, economics, ethics, game theory, and the study of practical reason. !! Embodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. !! Network capable: ability to communicate and bundle (product bundling) with another product (business) or product setsThe vision of smart products poses questions relevant to various research areas, including marketing, product engineering, computer science, artificial intelligence, economics, communication science, media economics, cognitive science, consumer psychology, innovation management and many more. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
intelligent agent paradigm,"Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
computer program,"Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of the structured control flow constructs of selection (if/then/else) and repetition (while and for), block structures, and subroutines. !! Intelligent agents are often described schematically as an abstract functional system similar to a computer program. !! A program transformation is any operation that takes a computer program and generates another program. !! In computing, lightweight software also called lite program and lightweight application, is a computer program that is designed to have a small memory footprint (RAM usage) and low CPU usage, overall a low usage of system resources. !! Data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a computer program. !! In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). !! A real data type is a data type used in a computer program to represent an approximation of a real number. !! Automatic vectorization, in parallel computing, is a special case of automatic parallelization, where a computer program is converted from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once. !! Structured concurrency is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by using a structured approach to concurrent programming. !! A central processing unit (CPU), also called a central processor, main processor or just processor, is the electronic circuitry that executes instructions comprising a computer program. !! In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions, or algorithms, that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer. !! Time travel debugging or time traveling debugging is the process of stepping back in time through source code to understand what is happening during execution of a computer program. !! An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program. !! In computer programming, a thread pool is a software design pattern for achieving concurrency of execution in a computer program. !! In computer science, a software agent is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. !! In computer performance, the instruction path length is the number of machine code instructions required to execute a section of a computer program."
computer programming,"In computer programming, the Schwartzian transform is a technique used to improve the efficiency of sorting a list of items. !! In computer programming, a runtime system, also called runtime environment, primarily implements portions of an execution model. !! In computer science and computer programming, access level denotes the set of permissions or restrictions provided to a data type. !! In computer programming, bidirectional transformations (bx) are programs in which a single piece of code can be run in several ways, such that the same data are sometimes considered as input, and sometimes as output. !! In computer programming, a code smell is any characteristic in the source code of a program that possibly indicates a deeper problem. !! In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. !! In computer programming, a free-form language is a programming language in which the positioning of characters on the page in program text is insignificant. !! In computer programming, DLL injection is a technique used for running code within the address space of another process by forcing it to load a dynamic-link library. !! In computer programming and software design, code refactoring is the process of restructuring existing computer codechanging the factoringwithout changing its external behavior. !! Programming languages are one kind of computer language, and are used in computer programming to implement algorithms. !! In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. !! In computer programming, run-time type information or run-time type identification (RTTI) is a feature of some programming languages (such as C++, Object Pascal, and Ada) that exposes information about an object's data type at runtime. !! In computer programming, boilerplate code, or simply boilerplate, are sections of code that are repeated in multiple places with little to no variation. !! In computer programming, lazy initialization is the tactic of delaying the creation of an object, the calculation of a value, or some other expensive process until the first time it is needed. !! In computer programming, string interpolation (or variable interpolation, variable substitution, or variable expansion) is the process of evaluating a string literal containing one or more placeholders, yielding a result in which the placeholders are replaced with their corresponding values. !! The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. !! In computer programming, symbolic programming is a programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data. !! The state pattern is used in computer programming to encapsulate varying behavior for the same object, based on its internal state. !! In computer programming, a forward declaration is a declaration of an identifier (denoting an entity such as a type, a variable, a constant, or a function) for which the programmer has not yet given a complete definition. !! In computer programming, addressing modes are primarily of interest to those who write in assembly languages and to compiler writers. !! Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of keyvalue pairs and looking up the value associated with a given key. !! In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. !! In computer programming, a thread pool is a software design pattern for achieving concurrency of execution in a computer program. !! An XOR linked list is a type of data structure used in computer programming. !! In some areas of computer programming, dead code is a section in the source code of a program which is executed but whose result is never used in any other computation. !! In computer programming, feature-oriented programming (FOP) or feature-oriented software development (FOSD) is a programming paradigm for program generation in software product lines (SPLs) and for incremental development of programs. !! In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods. !! Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. !! In computer programming, the proxy pattern is a software design pattern. !! Computer programming is the process of performing a particular computation (or more generally, accomplishing a specific computing result), usually by designing/building an executable computer program. !! The uniform access principle of computer programming was put forth by Bertrand Meyer (originally in Object-Oriented Software Construction). !! In computer programming, undefined behavior (UB) is the result of executing a program whose behavior is prescribed to be unpredictable, in the language specification to which the computer code adheres. !! Computer science is generally considered an area of academic research and distinct from computer programming. !! An object hierarchy is a concept from computer programming. !! In computer programming, a scientific programming language can refer to two degrees of the same concept. !! In computer programming, abstraction inversion is an anti-pattern arising when users of a construct need functions implemented within it but not exposed by its interface. !! In computer programming, an anonymous function (function literal, lambda abstraction, lambda function, lambda expression or block) is a function definition that is not bound to an identifier. !! In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits. !! In computer programming, bounds checking is any method of detecting whether a variable is within some bounds before it is used. !! Lazy loading (also known as asynchronous loading) is a design pattern commonly used in computer programming and mostly in web design and development to defer initialization of an object until the point at which it is needed. !! In computer programming, primary clustering is one of two major failure modes of open addressing based hash tables, especially those using linear probing."
scientific programming language,"In this sense, C/C++ and Python can be considered scientific programming languages. !! In a stronger sense, a scientific programming language is one that is designed and optimized for the use of mathematical formula and matrices. !! In a wide sense, a scientific programming language is a programming language that is used widely for computational science and computational mathematics. !! In computer programming, a scientific programming language can refer to two degrees of the same concept. !! Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
programming language,"It is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. !! A ""type"" in type theory has a role similar to a ""type"" in a programming language: it dictates the operations that can be performed on a term and, for variables, the possible values it might be replaced with. !! However, Smith argues that this model is not true structured concurrency as the programming language is unaware of the joining behavior, and is thus unable to enforce safety. !! The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm. !! A scripting language or script language is a programming language for a runtime system that automates the execution of tasks that would otherwise be performed individually by a human operator. !! A semantic resolution tree is a tree used for the definition of the semantics of a programming language. !! In a wide sense, a scientific programming language is a programming language that is used widely for computational science and computational mathematics. !! In any programming language that implements short-circuit evaluation, the expression x and y is equivalent to the conditional expression if x then y else x, and the expression x or y is equivalent to if x then x else y. !! Although this is a very useful property, it has a drawback: a programming language with the normalization property cannot be Turing complete, otherwise one could solve the halting problem by seeing if the program type-checks. !! A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output. !! A particularly interesting example of the use of partial evaluation, first described in the 1970s by Yoshihiko Futamura, is when prog is an interpreter for a programming language. !! In computer programming, a free-form language is a programming language in which the positioning of characters on the page in program text is insignificant. !! A lambda calculus system with the normalization property can be viewed as a programming language with the property that every program terminates. !! More generally ""primitive data types"" may refer to the standard data types built into a programming language."
computational mathematics,"Computational mathematics involves mathematical research in mathematics as well as in areas of science where computation plays a central and essential role, and emphasizes algorithms, numerical methods, and symbolic computations. !! Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. !! Foundations of Computational Mathematics: Special Volume. !! In a wide sense, a scientific programming language is a programming language that is used widely for computational science and computational mathematics. !! The essence of computational science is the application of numerical algorithms and computational mathematics. !! Computational Mathematics: An Introduction to Numerical Approximation. !! Computational mathematics may also refer to the use of computers for mathematics itself."
mathematical formula,"In a stronger sense, a scientific programming language is one that is designed and optimized for the use of mathematical formula and matrices."
scientific method,"Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
scientific programming languages,"Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
scientific language,"Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
computer graphics,"Computer Graphics International (CGI) is one of the oldest annual international conferences on computer graphics. !! With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bzier curves. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics. !! In the geometry of computer graphics, a vertex normal at a vertex of a polyhedron is a directional vector associated with a vertex, intended as a replacement to the true geometric normal of the surface. !! Today, computer graphics is a core technology in digital photography, film, video games, cell phone and computer displays, and many specialized applications. !! Vector graphics, as a form of computer graphics, is the set of mechanisms for creating visual images directly from geometric shapes defined on a Cartesian plane, such as points, lines, curves, and polygons. !! The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. !! In computer graphics, texture filtering or texture smoothing is the method used to determine the texture color for a texture mapped pixel, using the colors of nearby texels (pixels of the texture). !! The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization. !! An abstract graphical data type (AGDT) is an extension of an abstract data type for computer graphics. !! The non-artistic aspects of computer graphics are the subject of computer science research. !! A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. !! Computer graphics deals with generating images with the aid of computers. !! The development of kinetic data structures was motivated by computational geometry problems involving physical objects in continuous motion, such as collision or visibility detection in robotics, animation or computer graphics."
polygonal approximations,"Vertex normals can also be computed for polygonal approximations to surfaces such as NURBS, or specified explicitly for artistic purposes."
error message,"End-users may see a stack trace displayed as part of an error message, which the user can then report to a programmer. !! Error messages are used when user intervention is required, to indicate that a desired operation has failed, or to relay important warnings (such as warning a computer user that they are almost out of hard disk space). !! An error message is information displayed when an unforeseen problem occurs, usually on a computer or other device. !! On modern operating systems with graphical user interfaces, error messages are often displayed using dialog boxes. !! Error messages are seen widely throughout computing, and are part of every operating system or computer hardware device. !! Proper design of error messages is an important topic in usability and other fields of humancomputer interaction."
sibling calls,Sibling calls do not appear in a stack trace.
hurwitz stable,Hadamard product: The Hadamard (coefficient-wise) product of two Hurwitz stable polynomials is again Hurwitz stable.
hadamard product,"If A and B are each real-valued matrices, the Frobenius inner product is the sum of the entries of the Hadamard product. !! Hadamard product: The Hadamard (coefficient-wise) product of two Hurwitz stable polynomials is again Hurwitz stable."
information technology,"System integration is defined in engineering as the process of bringing together the component sub-systems into one system (an aggregation of subsystems cooperating so that the system is able to deliver the overarching functionality) and ensuring that the subsystems function together as a system, and in information technology as the process of linking together different computing systems and software applications physically or functionally, to act as a coordinated whole. !! Enterprise modelling constructs can focus upon manufacturing operations and/or business operations; however, a common thread in enterprise modelling is an inclusion of assessment of information technology. !! In information technology, a notification system is a combination of software and hardware that provides a means of delivering a message to a set of recipients. !! In information technology, lossy compression or irreversible compression is the class of data encoding methods that uses inexact approximations and partial data discarding to represent the content. !! In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. !! Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge. !! The field of Artificial Immune Systems (AIS) is concerned with abstracting the structure and function of the immune system to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. !! Solution architecture, term used in information technology with various definitions such as; ""A description of a discrete and focused business operation or activity and how IS/IT supports that operation"". !! The National Cyber Security Policy 2013 is a policy framework by the Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard ""information, such as personal information (of web users), financial and banking information and sovereign data""."
machine learning,"Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! In communication networks, cognitive network (CN) is a new type of data network that makes use of cutting edge technology from several research areas (i. e. machine learning, knowledge representation, computer network, network management) to solve some problems current networks are faced with. !! In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries. !! Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. !! AI Bridging Cloud Infrastructure (ABCI) is a planned supercomputer being built at the University of Tokyo for use in artificial intelligence, machine learning, and deep learning. !! Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning. !! Boltzmann machines with unconstrained connectivity have not proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems. !! In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! Among the most used adaptive algorithms is the Widrow-Hoffs least mean squares (LMS), which represents a class of stochastic gradient-descent algorithms used in adaptive filtering and machine learning. !! One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. !! Structural risk minimization (SRM) is an inductive principle of use in machine learning. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! Data science is related to data mining, machine learning and big data. !! Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. !! Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e. g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. !! Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. !! Decision tree learning or induction of decision trees is one of the predictive modelling approaches used in statistics, data mining and machine learning. !! Factor analysis is commonly used in psychometrics, personality psychology, biology, marketing, product management, operations research, finance, and machine learning. !! Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. !! In machine learning, weighted majority algorithm (WMA) is a meta learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts. !! Meta-heuristics and machine learning are used to address the complexity of program optimization. !! Agent mining is an interdisciplinary area that synergizes multiagent systems with data mining and machine learning. !! Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. !! In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. !! Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. !! In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. !! Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. !! In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. !! In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory. !! Within computer science, bio-inspired computing relates to artificial intelligence and machine learning. !! Data stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery. !! Parity learning is a problem in machine learning. !! In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. !! In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. !! In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length. !! In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. !! Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning. !! In machine learning, one-class classification (OCC), also known as unary classification or class-modelling, tries to identify objects of a specific class amongst all objects, by primarily learning from a training set containing only the objects of that class, although there exist variants of one-class classifiers where counter-examples are used to further refine the classification boundary. !! Beyond quantum computing, the term ""quantum machine learning"" is also associated with classical machine learning methods applied to data generated from quantum experiments (i. e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. !! The Quantum Artificial Intelligence Lab (also called the Quantum AI Lab or QuAIL) is a joint initiative of NASA, Universities Space Research Association, and Google (specifically, Google Research) whose goal is to pioneer research on how quantum computing might help with machine learning and other difficult computer science problems. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. !! While the basic idea behind stochastic approximation can be traced back to the RobbinsMonro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning. !! In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
categorical label,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
pattern recognition task,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
sequence labeling,"The most common statistical models in use for sequence labeling make a Markov assumption, i. e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. !! Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. !! Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. !! A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. !! In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
algorithmic assignment,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
input sentence,"A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document."
speech tagging,"A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. !! Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."
sequence labeling task,"A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document."
statistical inference,"The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. !! Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. !! Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. !! Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available."
sequence labeling algorithms,"Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence."
markov assumption,"The most common statistical models in use for sequence labeling make a Markov assumption, i. e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain."
markov chain,"A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). !! The most common statistical models in use for sequence labeling make a Markov assumption, i. e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. !! A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. !! Markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics. !! The simplest Markov model is the Markov chain. !! A continuous-time process is called a continuous-time Markov chain (CTMC). !! In mathematics, a stochastic matrix is a square matrix used to describe the transitions of a Markov chain. !! A hidden Markov model is a Markov chain for which the state is only partially observable or noisily observable. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! In the mathematical theory of probability, an absorbing Markov chain is a Markov chain in which every state can reach an absorbing state."
matrix decompositions,"There are many different matrix decompositions; each finds use among a particular class of problems. !! Analogous scale-invariant decompositions can be derived from other matrix decompositions, e. g. , to obtain scale-invariant eigenvalues. !! Refers to variants of existing matrix decompositions, such as the SVD, that are invariant with respect to diagonal scaling."
state machines,Finite-state machines are of two typesdeterministic finite-state machines and non-deterministic finite-state machines. !! The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. !! State diagrams can be used to graphically represent finite-state machines (also called finite automata).
block lanczos algorithm,"In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices. !! The block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in integer factorization algorithms such as the quadratic sieve and number field sieve, and its development has been entirely driven by this application."
integer factorization algorithms,"Trial division is the most laborious but easiest to understand of the integer factorization algorithms. !! The block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in integer factorization algorithms such as the quadratic sieve and number field sieve, and its development has been entirely driven by this application."
global view,"In systems engineering, the system usability scale (SUS) is a simple, ten-item attitude Likert scale giving a global view of subjective assessments of usability."
system usability scale,"System Usability Scale (SUS) Score Calculator !! In systems engineering, the system usability scale (SUS) is a simple, ten-item attitude Likert scale giving a global view of subjective assessments of usability."
systems engineering,"In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements. !! Meta-process modeling is a type of metamodeling used in software engineering and systems engineering for the analysis and construction of models applicable and useful to some predefined problems. !! In systems engineering, the system usability scale (SUS) is a simple, ten-item attitude Likert scale giving a global view of subjective assessments of usability."
regression analysis,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. !! Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis. !! In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). !! Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis. !! Symbolic Regression (SR) is a type of regression analysis that searches the space of mathematical expressions to find the model that best fits a given dataset, both in terms of accuracy and simplicity. !! In regression analysis, ""mean squared error"", often referred to as mean squared prediction error or ""out-of-sample mean squared error"", can also refer to the mean value of the squared deviations of the predictions from the true values, over an out-of-sample test space, generated by a model estimated over a particular sample space."
supervised learning models,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis."
support-vector machine,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. !! It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support-vector machines, although given enough samples the algorithm still performs well. !! Support-vector machine weights have also been used to interpret SVM models in the past. !! More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. !! Posthoc interpretation of support-vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences."
dimensional space,"Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution. !! Sparse distributed memory is a mathematical representation of human memory, and uses high-dimensional space to help model the large amounts of memory that mimics that of the human neural network. !! Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. !! Six degrees of freedom (6DOF) refers to the freedom of movement of a rigid body in three-dimensional space. !! More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. !! The metric structures in computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology diffeomorphometry, the metric space study of coordinate systems via diffeomorphisms."
vector machine models,Posthoc interpretation of support-vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.
genetic representation,"Genetic representation can encode appearance, behavior, physical qualities of individuals. !! In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods. !! Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. !! The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size. !! Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation."
physical qualities,"Genetic representation can encode appearance, behavior, physical qualities of individuals."
evolutionary computation,"In evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. !! In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. !! Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. !! Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. !! Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation. !! Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes. !! Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation."
known classes,Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.
genetic representations,Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.
information theory,"Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to practical disciplines (including the design and implementation of hardware and software). !! In statistics and information theory, a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions. !! Information theory is the scientific study of the quantification, storage, and communication of digital information. !! In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! In other words, it is shown within algorithmic information theory that computational incompressibility ""mimics"" (except for a constant that only depends on the chosen universal programming language) the relations or inequalities found in information theory. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL). !! Gray, R. M. (2011), Entropy and Information Theory, Springer. !! In probability theory and information theory, adjusted mutual information, a variation of mutual information may be used for comparing clusterings. !! Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security. !! Additive white Gaussian noise (AWGN) is a basic noise model used in information theory to mimic the effect of many random processes that occur in nature. !! In information theory, units of information are also used to measure information contained in messages and the entropy of random variables. !! A key measure in information theory is entropy. !! In probability theory and in particular in information theory, total correlation (Watanabe 1960) is one of several generalizations of the mutual information. !! Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. !! The online textbook: Information Theory, Inference, and Learning Algorithms, by David J. C. MacKay, discusses the BCJR algorithm in chapter 25. !! In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! Information Theory has provided successful methods for alignment-free sequence analysis and comparison. !! In telecommunication, information theory, and coding theory, forward error correction (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. !! In statistics, probability theory, and information theory, a statistical distance quantifies the distance between two statistical objects, which can be two random variables, or two probability distributions or samples, or the distance can be between an individual sample point and a population or a wider sample of points."
digital information,"Information theory is the scientific study of the quantification, storage, and communication of digital information."
scientific study,"Information theory is the scientific study of the quantification, storage, and communication of digital information."
algorithmic complexity theory,"Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security."
algorithmic information theory,"Algorithmic information theory principally studies complexity measures on strings (or other data structures). !! Unlike classical information theory, algorithmic information theory gives formal, rigorous definitions of a random string and a random infinite sequence that do not depend on physical or philosophical intuitions about nondeterminism or likelihood. !! Informally, from the point of view of algorithmic information theory, the information content of a string is equivalent to the length of the most-compressed possible self-contained representation of that string. !! Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security. !! Algorithmic information theory (AIT) is a branch of theoretical computer science that concerns itself with the relationship between computation and information of computably generated objects (as opposed to stochastically generated), such as strings or any other data structure. !! In other words, it is shown within algorithmic information theory that computational incompressibility ""mimics"" (except for a constant that only depends on the chosen universal programming language) the relations or inequalities found in information theory. !! In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation."
data compression,"In data compression and the theory of formal languages, the smallest grammar problem is the problem of finding the smallest context-free grammar that generates a given string of characters (but no other string). !! Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization. !! Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL). !! Practical streaming media was only made possible with advances in data compression, due to the impractically high bandwidth requirements of uncompressed media. !! Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data."
zip files,"Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL)."
error detection,"Error detection is most commonly realized using a suitable hash function (or specifically, a checksum, cyclic redundancy check or other algorithm). !! Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization. !! Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL). !! Error detection is the detection of errors caused by noise or other impairments during transmission from the transmitter to the receiver. !! Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases. !! If only error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. !! In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! Error detection and correction codes are often used to improve the reliability of data storage media."
gaming systems,"Expanding outside of virtual reality and gaming systems, a newer segment of video game rehabilitation focuses on games that can be played on mobile phones or tablet computers - mobile apps."
virtual reality,"An immersive virtual musical instrument, or immersive virtual environment for music and sound, represents sound processes and their parameters as 3D entities of a virtual reality so that they can be perceived not only through auditory feedback but also visually in 3D and possibly through tactile as well as haptic feedback, using 3D interface metaphors consisting of interaction techniques such as navigation, selection and manipulation (NSM). !! Expanding outside of virtual reality and gaming systems, a newer segment of video game rehabilitation focuses on games that can be played on mobile phones or tablet computers - mobile apps."
computer skills,"Another challenge in video game rehabilitation, can result to lack of computer skills on the part of therapists, lack of support infrastructure, expensive equipment, inadequate communication infrastructure, and patient safety concerns."
temporal database,"A uni-temporal database has one axis of time, either the validity range or the system time range. !! Temporal databases are in contrast to current databases (not to be confused with currently available databases), which store only facts which are believed to be true at the current time. !! A temporal database stores data relating to time instances. !! In order to handle changes in the information content anchor modeling emulates aspects of a temporal database in the resulting relational database schema. !! Temporal databases could be uni-temporal, bi-temporal or tri-temporal. !! Richard Snodgrass proposed in 1992 that temporal extensions to SQL be developed by the temporal database community."
temporal databases,"Temporal databases are in contrast to current databases (not to be confused with currently available databases), which store only facts which are believed to be true at the current time."
current databases,"Temporal databases are in contrast to current databases (not to be confused with currently available databases), which store only facts which are believed to be true at the current time."
regularized logistic regression,Regularized logistic regression is specifically intended to be used in this situation.
global solutions,"Deterministic global optimization is a branch of numerical optimization which focuses on finding the global solutions of an optimization problem whilst providing theoretical guarantees that the reported solution is indeed the global one, within some predefined tolerance."
deterministic global optimization,"Deterministic global optimization is a branch of numerical optimization which focuses on finding the global solutions of an optimization problem whilst providing theoretical guarantees that the reported solution is indeed the global one, within some predefined tolerance. !! Deterministic global optimization methods require ways to rigorously bound function values over regions of space. !! The term ""deterministic global optimization"" typically refers to complete or rigorous (see below) optimization methods. !! Deterministic global optimization methods typically belong to the last two categories. !! Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem."
numerical optimization,"Deterministic global optimization is a branch of numerical optimization which focuses on finding the global solutions of an optimization problem whilst providing theoretical guarantees that the reported solution is indeed the global one, within some predefined tolerance."
optimization methods,"Increasingly, operations research uses stochastic programming to model dynamic decisions that adapt to events; such problems can be solved with large-scale optimization and stochastic optimization methods. !! Stochastic optimization (SO) methods are optimization methods that generate and use random variables. !! Nonlinear optimization methods are widely used in conformational analysis. !! The term ""deterministic global optimization"" typically refers to complete or rigorous (see below) optimization methods. !! More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution."
global minimum,"Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem."
mathematical model,"Sparse distributed memory (SDM) is a mathematical model of human long-term memory introduced by Pentti Kanerva in 1988 while he was at NASA Ames Research Center. !! A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. !! Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem. !! In computer science, an abstract data type (ADT) is a mathematical model for data types."
optimization problem,"In a genetic algorithm, a population of candidate solutions (called individuals, creatures, organisms, or phenotypes) to an optimization problem is evolved toward better solutions. !! Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem. !! In optimization theory, semi-infinite programming (SIP) is an optimization problem with a finite number of variables and an infinite number of constraints, or an infinite number of variables and a finite number of constraints."
lexicographic code,"Lexicographic codes or lexicodes are greedily generated error-correcting codes with remarkably good properties. !! In particular, the codewords in a binary lexicographic code of distance d encode the winning positions in a variant of Grundy's game, played on a collection of heaps of stones, in which each move consists of replacing any one heap by at most d 1 smaller heaps, and the goal is to take the last stone. !! The binary lexicographic codes are linear codes, and include the Hamming codes and the binary Golay codes. !! The theory of lexicographic codes is closely connected to combinatorial game theory. !! Following C generate lexicographic code and parameters are set for the Golay code (N=24, D=8)."
binary lexicographic codes,"The binary lexicographic codes are linear codes, and include the Hamming codes and the binary Golay codes."
golay code,"Following C generate lexicographic code and parameters are set for the Golay code (N=24, D=8)."
combinatorial game theory,The theory of lexicographic codes is closely connected to combinatorial game theory.
binary lexicographic code,"In particular, the codewords in a binary lexicographic code of distance d encode the winning positions in a variant of Grundy's game, played on a collection of heaps of stones, in which each move consists of replacing any one heap by at most d 1 smaller heaps, and the goal is to take the last stone."
minimum k-cut,"Inapproximability of Maximum Edge Biclique, Maximum Balanced Biclique and Minimum k-Cut from the Small Set Expansion Hypothesis. !! In mathematics, the minimum k-cut, is a combinatorial optimization problem that requires finding a set of edges whose removal would partition the graph to at least k connected components. !! Guttmann-Beck, N. ; Hassin, R. (1999), ""Approximation algorithms for minimum k-cut"" (PDF), Algorithmica, pp."
approximation algorithms,"The design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case. !! In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. !! The field of approximation algorithms, therefore, tries to understand how closely it is possible to approximate optimal solutions to such problems in polynomial time. !! Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P NP conjecture. !! However, there are also many approximation algorithms that provide an additive guarantee on the quality of the returned solution. !! Guttmann-Beck, N. ; Hassin, R. (1999), ""Approximation algorithms for minimum k-cut"" (PDF), Algorithmica, pp."
machine learning algorithms,"While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. !! Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. !! Deep learning is a class of machine learning algorithms that:199200 uses multiple layers to progressively extract higher-level features from the raw input. !! In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms."
manifold alignment,"Manifold alignment assumes that disparate data sets produced by similar generating processes will share a similar underlying manifold representation. !! Perform linear manifold alignment on the embedded data, holding the first data set fixed, mapping each additional data set onto the first's manifold. !! Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. !! Manifold alignment can be used to find linear (feature-level) projections, or nonlinear (instance-level) embeddings. !! Most manifold alignment techniques consider only two data sets, but the concept extends to arbitrarily many initial data sets."
embedded data,"Perform linear manifold alignment on the embedded data, holding the first data set fixed, mapping each additional data set onto the first's manifold."
fuzzy classification,"A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function. !! A fuzzy classification corresponds to a membership function that indicates whether an individual is a member of a class, given its fuzzy classification predicate ~. !! Accordingly, fuzzy classification is the process of grouping individuals having the same characteristics into a fuzzy set. !! Fuzzy classification is the process of grouping elements into a fuzzy set whose membership function is defined by the truth value of a fuzzy propositional function. !! The fuzzy classification predicate ~ corresponds to a fuzzy restriction ""i is R"" of U, where R is a fuzzy set defined by a truth function."
fuzzy propositional function,Fuzzy classification is the process of grouping elements into a fuzzy set whose membership function is defined by the truth value of a fuzzy propositional function. !! A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function.
truth value,"In a temporal logic, a statement can have a truth value that varies in timein contrast with an atemporal logic, which applies only to statements whose truth values are constant in time. !! Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. !! Fuzzy classification is the process of grouping elements into a fuzzy set whose membership function is defined by the truth value of a fuzzy propositional function."
fuzzy classification predicate,"The fuzzy classification predicate ~ corresponds to a fuzzy restriction ""i is R"" of U, where R is a fuzzy set defined by a truth function. !! A fuzzy classification corresponds to a membership function that indicates whether an individual is a member of a class, given its fuzzy classification predicate ~. !! A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function."
fuzzy class,A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function.
fuzzy set,"Accordingly, fuzzy classification is the process of grouping individuals having the same characteristics into a fuzzy set. !! A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function."
membership function,"A fuzzy classification corresponds to a membership function that indicates whether an individual is a member of a class, given its fuzzy classification predicate ~."
mobile ad hoc networks,The Wireless Routing Protocol (WRP) is a proactive unicast routing protocol for mobile ad hoc networks (MANETs). !! The Temporally Ordered Routing Algorithm (TORA) is an algorithm for routing data across Wireless Mesh Networks or Mobile ad hoc networks.
wireless routing protocol,The Wireless Routing Protocol (WRP) is a proactive unicast routing protocol for mobile ad hoc networks (MANETs).
immersive virtual environment,"An immersive virtual musical instrument, or immersive virtual environment for music and sound, represents sound processes and their parameters as 3D entities of a virtual reality so that they can be perceived not only through auditory feedback but also visually in 3D and possibly through tactile as well as haptic feedback, using 3D interface metaphors consisting of interaction techniques such as navigation, selection and manipulation (NSM)."
human-centered computing,"Human-centered computing is closely related to human-computer interaction and information science. !! Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! Human-centered computing (HCC) studies the design, development, and deployment of mixed-initiative human-computer systems. !! Human-centered systems (HCS) are systems designed for human-centered computing. !! Human-centered computing is usually concerned with systems and practices of technology use while human-computer interaction is more focused on ergonomics and the usability of computing artifacts and information science is focused on practices surrounding the collection, manipulation, and use of information."
computer systems,"In computer science, an access control matrix or access matrix is an abstract, formal security model of protection state in computer systems, that characterizes the rights of each subject with respect to every object in the system. !! Digital identity is now often used in ways that require data about persons stored in computer systems to be linked to their civil, or national, identities. !! The term ""digital identity"" also denotes certain aspects of civil and personal identity that have resulted from the widespread use of identity information to represent people in an acceptable and trusted digital format in computer systems. !! Lall, Ashwin; Sekar, Vyas; Ogihara, Mitsunori; Xu, Jun; Zhang, Hui (2006), ""Data streaming algorithms for estimating entropy of network traffic"", Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS 2006) (PDF), p. 145, !! Human-centered computing (HCC) studies the design, development, and deployment of mixed-initiative human-computer systems. !! ACM Transactions on Computer Systems is a quarterly peer-reviewed scientific journal published by the Association for Computing Machinery. !! In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. !! A digital identity is information on an entity used by computer systems to represent an external agent. !! By the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision."
information science,"Document classification or document categorization is a problem in library science, information science and computer science. !! In philosophy, the term formal ontology is used to refer to an ontology defined by axioms in a formal language with the goal to provide an unbiased (domain- and application-independent) view on reality, which can help the modeler of domain- or application-specific ontologies (information science) to avoid possibly erroneous ontological assumptions encountered in modeling large-scale ontologies. !! However, data science is different from computer science and information science. !! Human-centered computing is closely related to human-computer interaction and information science. !! Human-centered computing is usually concerned with systems and practices of technology use while human-computer interaction is more focused on ergonomics and the usability of computing artifacts and information science is focused on practices surrounding the collection, manipulation, and use of information."
computer interaction,"The nature of interactive computing as well as its impact on users, are studied extensively in the field of computer interaction. !! Human-centered computing is closely related to human-computer interaction and information science. !! Most self-identified persuasive technology research focuses on interactive, computational technologies, including desktop computers, Internet services, video games, and mobile devices, but this incorporates and builds on the results, theories, and methods of experimental psychology, rhetoric, and human-computer interaction. !! User experience design draws from design approaches like human-computer interaction and user-centered design, and includes elements from similar disciplines like interaction design, visual design, information architecture, user research, and others. !! Augmented cognition is an interdisciplinary area of psychology and engineering, attracting researchers from the more traditional fields of human-computer interaction, psychology, ergonomics and neuroscience. !! Graphically embodied agents aim to unite gesture, facial expression and speech to enable face-to-face communication with users, providing a powerful means of human-computer interaction. !! In computing, scratch input is an acoustic-based method of Human-Computer Interaction (HCI) that takes advantage of the characteristic sound produced when a finger nail or other object is dragged over a surface, such as a table or wall. !! In human-computer interaction, low-key feedback is a type of output that takes a background role by being very subtle, sometimes nearly imperceptible. !! In computer science, particularly in human-computer interaction, presentation semantics specify how a particular piece of a formal language is represented in a distinguished manner accessible to human senses, usually human vision. !! Human-centered computing is usually concerned with systems and practices of technology use while human-computer interaction is more focused on ergonomics and the usability of computing artifacts and information science is focused on practices surrounding the collection, manipulation, and use of information."
graphic design,"Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! One definition is that it's information visualization when the spatial representation (e. g. , the page layout of a graphic design) is chosen, whereas it's scientific visualization when the spatial representation is given. !! Since the graphic design of the mapping can adversely affect the readability of a chart, mapping is a core competency of Data visualization."
industrial design,"Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design."
computer network,"Server Message Block (SMB) enables file sharing, printer sharing, network browsing, and inter-process communication (through named pipes) over a computer network. !! Network architecture is the design of a computer network. !! Computer network programming involves writing computer programs that enable processes to communicate with each other across a computer network. !! The nodes of a computer network may include personal computers, servers, networking hardware, or other specialised or general-purpose hosts. !! Quality of service (QoS) is the description or measurement of the overall performance of a service, such as a telephony or computer network or a cloud computing service, particularly the performance seen by the users of the network. !! Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications. !! In telecommunications, a protocol data unit (PDU) is a single unit of information transmitted among peer entities of a computer network. !! In communication networks, cognitive network (CN) is a new type of data network that makes use of cutting edge technology from several research areas (i. e. machine learning, knowledge representation, computer network, network management) to solve some problems current networks are faced with. !! Computer networks may be classified by many criteria, including the transmission medium used to carry signals, bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanism, and organizational intent. !! A computer network is a set of computers sharing resources located on or provided by network nodes. !! A grid network is not the same as a grid computer or a computational grid, although the nodes in a grid network are usually computers, and grid computing requires some kind of computer network or ""universal coding"" to interconnect the computers. !! Software incompatibility is a characteristic of software components or systems which cannot operate satisfactorily together on the same computer, or on different computers linked by a computer network. !! For example, a notification system can send an e-mail announcing when a computer network will be down for a scheduled maintenance. !! A broadcast storm or broadcast radiation is the accumulation of broadcast and multicast traffic on a computer network. !! Computer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines. !! A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer."
network architecture,"Network architecture is the design of a computer network. !! In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network. !! In telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated. !! The Open Systems Interconnection model (OSI model) defines and codifies the concept of layered network architecture. !! The network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links."
communications network,"In telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated."
internet protocol suite,"In the Internet Protocol Suite (TCP/IP), the data link layer functionality is contained within the link layer, the lowest layer of the descriptive model, which is assumed to be independent of physical infrastructure. !! The Internet Protocol (IP) is the network layer communications protocol in the Internet protocol suite for relaying datagrams across network boundaries. !! The Internet protocol suite is therefore often referred to as TCP/IP. !! The Internet Control Message Protocol (ICMP) is a supporting protocol in the Internet protocol suite. !! The Common Open Policy Service (COPS) Protocol is part of the internet protocol suite as defined by the RFC 2748. !! The network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links."
hardware links,"The network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links."
layered network architecture,The Open Systems Interconnection model (OSI model) defines and codifies the concept of layered network architecture.
osi model,"Cognitive network is different from cognitive radio (CR) as it covers all the layers of the OSI model (not only layers 1 and 2 as with CR ). !! The Open Systems Interconnection model (OSI model) defines and codifies the concept of layered network architecture. !! The application firewall can control communications up to the application layer of the OSI model, which is the highest operating layer, and where it gets its name."
distributed computing,"Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. !! In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network. !! In distributed computing, a remote procedure call (RPC) is when a computer program causes a procedure (subroutine) to execute in a different address space (commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction."
distributed application architecture,"In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network."
distributed application,"In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network."
lossless compression,"Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data. !! Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. !! By operation of the pigeonhole principle, no lossless compression algorithm can efficiently compress all possible data. !! Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data. !! Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods."
original data,"Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases. !! Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. !! Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data."
compressed data,Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data.
lossless compression algorithm,"Some of the most common lossless compression algorithms are listed below. !! While, in principle, any general-purpose lossless compression algorithm (general-purpose meaning that they can accept any bitstring) can be used on any type of data, many are unable to achieve significant compression on data that are not of the form for which they were designed to compress. !! However, many ordinary lossless compression algorithms produce headers, wrappers, tables, or other predictable output that might instead make cryptanalysis easier. !! By operation of the pigeonhole principle, no lossless compression algorithm can efficiently compress all possible data. !! No lossless compression algorithm can efficiently compress all possible data (see the section Limitations below for details)."
decompressed data,"Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable."
lossy methods,"Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods."
image file formats,"Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods."
bit sequences,"Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data."
statistical model,"Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a ""likelihood function"" derived from a statistical model for the observed data."
input data,"Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. !! Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data. !! Here is a simple competitive learning algorithm to find three clusters within some input data. !! This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm."
state machine,"Finite-state machines are of two typesdeterministic finite-state machines and non-deterministic finite-state machines. !! The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. !! Finite-state machine-based programming is generally the same, but, formally speaking, does not cover all possible variants, as FSM stands for finite-state machine, and automata-based programming does not necessarily employ FSMs in the strict sense. !! A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. !! Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite-state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). !! The finite-state machine has less computational power than some other models of computation such as the Turing machine. !! A deterministic finite-state machine can be constructed equivalent to any non-deterministic one."
automata-based programming,"Automata-based programming indeed closely matches the programming needs found in the field of automation. !! Another reason for using the notion of automata-based programming is that the programmer's style of thinking about the program in this technique is very similar to the style of thinking used to solve mathematical tasks using Turing machines, Markov algorithms, etc. !! Finite-state machine-based programming is generally the same, but, formally speaking, does not cover all possible variants, as FSM stands for finite-state machine, and automata-based programming does not necessarily employ FSMs in the strict sense. !! Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite-state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). !! Automata-based programming is widely used in lexical and syntactic analyses."
programming paradigm,"Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite-state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). !! In computer programming, symbolic programming is a programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data. !! In computer programming, feature-oriented programming (FOP) or feature-oriented software development (FOSD) is a programming paradigm for program generation in software product lines (SPLs) and for incremental development of programs. !! Logic programming is a programming paradigm which is largely based on formal logic."
markov algorithms,"Another reason for using the notion of automata-based programming is that the programmer's style of thinking about the program in this technique is very similar to the style of thinking used to solve mathematical tasks using Turing machines, Markov algorithms, etc. !! Refal is a programming language based on Markov algorithms. !! Markov algorithms have been shown to be Turing-complete, which means that they are suitable as a general model of computation and can represent any mathematical expression from its simple notation."
web design,"Lazy loading (also known as asynchronous loading) is a design pattern commonly used in computer programming and mostly in web design and development to defer initialization of an object until the point at which it is needed. !! User experience evaluation has become common practice in web design, especially within organizations implementing user-centered design practices. !! Responsive web design (RWD) or responsive design is an approach to web design that aims to make web pages render well on a variety of devices and window or screen sizes from minimum to maximum display size to ensure usability and satisfaction."
statistical language model,A cache language model is a type of statistical language model. !! A statistical language model is a probability distribution over sequences of words.
language model,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! A statistical language model is a probability distribution over sequences of words. !! Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model. !! The language model provides context to distinguish between words and phrases that sound phonetically similar. !! Data sparsity is a major problem in building language models."
probability distribution,"A statistical language model is a probability distribution over sequences of words. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! Monte Carlo methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution. !! Then, the photonic implementation of the boson sampling task consists of generating a sample from the probability distribution of single-photon measurements at the output of the circuit. !! Maximal entropy random walk (MERW) is a popular type of biased random walk on a graph, in which transition probabilities are chosen accordingly to the principle of maximum entropy, which says that the probability distribution which best represents the current state of knowledge is the one with largest entropy. !! In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. !! Since Bayesian statistics treats probability as a degree of belief, Bayes' theorem can directly assign a probability distribution that quantifies the belief to the parameter or set of parameters."
data sparsity,Data sparsity is a major problem in building language models.
optical character recognition,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications."
handwriting recognition,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Graph edit distance finds applications in handwriting recognition, fingerprint recognition and cheminformatics."
machine translation,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications."
speech recognition,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. !! It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). !! Applications of graphical models include causal inference, information extraction, speech recognition, computer vision, decoding of low-density parity-check codes, modeling of gene regulatory networks, gene finding and diagnosis of diseases, and graphical models for protein structure. !! Speech recognition applications include voice user interfaces such as voice dialing (e. g. ""call home""), call routing (e. g. ""I would like to make a collect call""), domotic appliance control, search key words (e. g. find a podcast where particular words were spoken), simple data entry (e. g. , entering a credit card number), preparation of structured documents (e. g. a radiology report), determining speaker characteristics, speech-to-text processing (e. g. , word processors or emails), and aircraft (usually termed direct voice input). !! From the technology perspective, speech recognition has a long history with several waves of major innovations. !! Some speech recognition systems require ""training"" (also called ""enrollment"") where an individual speaker reads text or isolated vocabulary into the system. !! Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. !! The term voice recognition can refer to speaker recognition or speech recognition. !! Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition. !! Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics. !! Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics."
information retrieval,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Reverse image search is a content-based image retrieval (CBIR) query technique that involves providing the CBIR system with a sample image that it will then base its search upon; in terms of information retrieval, the sample image is what formulates a search query. !! Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents. !! Similarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems. !! Exploratory search is a topic that has grown from the fields of information retrieval and information seeking but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a Google-like keyword search)."
language modeling,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers."
pronunciation model,Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model.
acoustic model,Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model.
source code,"Code stylometry (also known as program authorship attribution or source code authorship analysis) is the application of stylometry to computer code to attribute authorship to anonymous binary or source code. !! Some programming languages allow a program to operate differently or even have a different control flow than the source code, as long as it exhibits the same user-visible side effects, if undefined behavior never happens during program execution. !! If Istatic is source code designed to run inside that interpreter, then partial evaluation of the interpreter with respect to this data/program produces prog*, a version of the interpreter that only runs that source code, is written in the implementation language of the interpreter, does not require the source code to be resupplied, and runs faster than the original combination of the interpreter and the source. !! In some areas of computer programming, dead code is a section in the source code of a program which is executed but whose result is never used in any other computation. !! Inline expansion is similar to macro expansion, but occurs during compilation, without changing the source code (the text), while macro expansion occurs prior to compilation, and results in different text that is then processed by the compiler. !! In computing, file comparison is the calculation and display of the differences and similarities between data objects, typically text files such as source code. !! A compilation error message often helps programmers debugging the source code. !! In computer programming, a code smell is any characteristic in the source code of a program that possibly indicates a deeper problem. !! Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields. !! Time travel debugging or time traveling debugging is the process of stepping back in time through source code to understand what is happening during execution of a computer program. !! A software code audit is a comprehensive analysis of source code in a programming project with the intent of discovering bugs, security breaches or violations of programming conventions. !! In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. !! Loop optimization can be viewed as the application of a sequence of specific loop transformations (listed below or in Compiler transformations for high-performance computing) to the source code or intermediate representation, with each transformation having an associated test for legality. !! Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code."
data objects,"In computing, file comparison is the calculation and display of the differences and similarities between data objects, typically text files such as source code."
prototype-based supervised classification algorithm,"In computer science, learning vector quantization (LVQ) is a prototype-based supervised classification algorithm."
learning vector quantization,"See the 'Bibliography on the Self-Organizing Map (SOM) and Learning Vector Quantization (LVQ)'. !! In computer science, learning vector quantization (LVQ) is a prototype-based supervised classification algorithm. !! Self-Organizing Maps and Learning Vector Quantization for Feature Sequences, Somervuo and Kohonen."
reduction systems,"Such methods may be achieved by rewriting systems (also known as rewrite systems, rewrite engines, or reduction systems)."
compute arithmetic operations,Term rewriting systems can be employed to compute arithmetic operations on natural numbers.
functional architecture,"Traceabilities also support change management as part of requirements management in understanding the impacts of changes through requirements or other related elements (e. g. , functional impacts through relations to functional architecture), and facilitating introducing these changes."
language specification,"In computer programming, undefined behavior (UB) is the result of executing a program whose behavior is prescribed to be unpredictable, in the language specification to which the computer code adheres. !! In computing, compiler correctness is the branch of computer science that deals with trying to show that a compiler behaves according to its language specification."
compiler correctness,"In computing, compiler correctness is the branch of computer science that deals with trying to show that a compiler behaves according to its language specification."
relational semantics,An accessibility relation is a relation which plays a key role in assigning truth values to sentences in the relational semantics for modal logic.
mathematical study,"Queueing theory is the mathematical study of waiting lines, or queues. !! The mathematical study of how validities are tied to conditions on accessibility relations is known as modal correspondence theory. !! Switching circuit theory is the mathematical study of the properties of networks of idealized switches. !! Bifurcation theory is the mathematical study of changes in the qualitative or topological structure of a given family of curves, such as the integral curves of a family of vector fields, and the solutions of a family of differential equations."
double-ended priority queue,"One example application of the double-ended priority queue is external sorting. !! Double-ended priority queues can be built from balanced binary search trees (where the minimum and maximum elements are the leftmost and rightmost leaves, respectively), or using specialized data structures like min-max heap and pairing heap. !! In computer science, a double-ended priority queue (DEPQ) or double-ended heap is a data structure similar to a priority queue or heap, but allows for efficient removal of both the maximum and minimum, according to some ordering on the keys (items) stored in the structure."
priority queue,"In computer science, a binomial heap is a data structure that acts as a priority queue but also allows pairs of heaps to be merged. !! The time complexity of Prim's algorithm depends on the data structures used for the graph and for ordering the edges by weight, which can be done using a priority queue. !! In computer science, a double-ended priority queue (DEPQ) or double-ended heap is a data structure similar to a priority queue or heap, but allows for efficient removal of both the maximum and minimum, according to some ordering on the keys (items) stored in the structure."
balanced binary search trees,"Double-ended priority queues can be built from balanced binary search trees (where the minimum and maximum elements are the leftmost and rightmost leaves, respectively), or using specialized data structures like min-max heap and pairing heap."
connected wireless device,Mobile virtualization is hardware virtualization on a mobile phone or connected wireless device.
arnoldi iteration,"The idea of the Arnoldi iteration as an eigenvalue algorithm is to compute the eigenvalues in the Krylov subspace. !! The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! The Arnoldi iteration was invented by W. E. Arnoldi in 1951. !! This happens when the minimal polynomial of A is of degree k. In most applications of the Arnoldi iteration, including the eigenvalue algorithm below and GMRES, the algorithm has converged at this point."
eigenvalue algorithm,"In numerical linear algebra, the QR algorithm or QR iteration is an eigenvalue algorithm: that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. !! The idea of the Arnoldi iteration as an eigenvalue algorithm is to compute the eigenvalues in the Krylov subspace. !! The eigenvalue algorithm can then be applied to the restricted matrix. !! If an eigenvalue algorithm does not produce eigenvectors, a common practice is to use an inverse iteration based algorithm with set to a close approximation to the eigenvalue. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! Thus eigenvalue algorithms that work by finding the roots of the characteristic polynomial can be ill-conditioned even when the problem is not. !! Hessenberg and tridiagonal matrices are the starting points for many eigenvalue algorithms because the zero entries reduce the complexity of the problem. !! Rayleigh quotient iteration is an eigenvalue algorithm which extends the idea of the inverse iteration by using the Rayleigh quotient to obtain increasingly accurate eigenvalue estimates. !! These eigenvalue algorithms may also find eigenvectors. !! This happens when the minimal polynomial of A is of degree k. In most applications of the Arnoldi iteration, including the eigenvalue algorithm below and GMRES, the algorithm has converged at this point."
iterative method,"In mathematics, the generalized minimal residual method (GMRES) is an iterative method for the numerical solution of an indefinite nonsymmetric system of linear equations. !! Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e. g. differentiable or subdifferentiable). !! Rayleigh quotient iteration is an iterative method, that is, it delivers a sequence of approximate solutions that converges to a true solution in the limit. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. !! In mathematics, the folded spectrum method (FSM) is an iterative method for solving large eigenvalue problems."
numerical linear algebra,"Because many properties of matrices and vectors also apply to functions and operators, numerical linear algebra can also be viewed as a type of functional analysis which has a particular emphasis on practical algorithms. !! In numerical linear algebra, the QR algorithm or QR iteration is an eigenvalue algorithm: that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. !! In the mathematical discipline of numerical linear algebra, a matrix splitting is an expression which represents a given matrix as a sum or difference of matrices. !! Householder transformations are widely used in numerical linear algebra, for example, to annihilate the entries below the main diagonal of a matrix, to perform QR decompositions and in the first step of the QR algorithm. !! In numerical linear algebra, the biconjugate gradient stabilized method, often abbreviated as BiCGSTAB, is an iterative method developed by H. A. van der Vorst for the numerical solution of nonsymmetric linear systems. !! In numerical linear algebra, a convergent matrix is a matrix that converges to the zero matrix under matrix exponentiation. !! Numerical linear algebra aims to solve problems of continuous mathematics using finite precision computers, so its applications to the natural and social sciences are as vast as the applications of continuous mathematics. !! Numerical linear algebra uses properties of vectors and matrices to develop computer algorithms that minimize the error introduced by the computer, and is also concerned with ensuring that the algorithm is as efficient as possible. !! Noting the broad applications of numerical linear algebra, Lloyd N. Trefethen and David Bau, III argue that it is ""as fundamental to the mathematical sciences as calculus and differential equations"",:x even though it is a comparatively small field. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! In numerical linear algebra, a Givens rotation is a rotation in the plane spanned by two coordinates axes. !! The main use of Givens rotations in numerical linear algebra is to introduce zeros in vectors or matrices. !! In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations. !! In numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the GaussSeidel method for solving a linear system of equations, resulting in faster convergence. !! Numerical linear algebra, sometimes called applied linear algebra, is the study of how matrix operations can be used to create computer algorithms which efficiently and accurately provide approximate answers to questions in continuous mathematics."
optimality problem,The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.
characteristic polynomial,Thus eigenvalue algorithms that work by finding the roots of the characteristic polynomial can be ill-conditioned even when the problem is not. !! The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.
monic polynomials,The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.
krylov subspace,"The idea of the Arnoldi iteration as an eigenvalue algorithm is to compute the eigenvalues in the Krylov subspace. !! Many linear dynamical system tests in control theory, especially those related to controllability and observability, involve checking the rank of the Krylov subspace. !! These tests are equivalent to finding the span of the Grammians associated with the system/output maps so the uncontrollable and unobservable subspaces are simply the orthogonal complement to the Krylov subspace. !! denotes the maximal dimension of a Krylov subspace. !! Krylov subspaces are used in algorithms for finding approximate solutions to high-dimensional linear algebra problems. !! can be decomposed as the direct sum of Krylov subspaces."
decision boundary,"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. !! A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. !! In the case of backpropagation based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has. !! In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. !! The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. !! If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary."
margin classifier,"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. !! This extends the geometric interpretation of SVMfor linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier. !! One theoretical motivation behind margin classifiers is that their generalization error may be bound by parameters of the algorithm and a margin term."
linear equations,"In the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation. !! In mathematics, the generalized minimal residual method (GMRES) is an iterative method for the numerical solution of an indefinite nonsymmetric system of linear equations. !! In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. !! The conjugate residual method is an iterative numeric method used for solving systems of linear equations. !! One way to find the LU decomposition of this simple matrix would be to simply solve the linear equations by inspection. !! In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is positive-definite. !! There are several algorithms for solving a system of linear equations. !! When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations. !! The simplest method for solving a system of linear equations is to repeatedly eliminate variables. !! Iterative refinement is an iterative method proposed by James H. Wilkinson to improve the accuracy of numerical solutions to systems of linear equations. !! In the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods. !! This is an example of equivalence in a system of linear equations."
system matrix,"The conjugate residual method differs from the closely related conjugate gradient method primarily in that it involves more numerical operations and requires more storage, but the system matrix is only required to be Hermitian, not symmetric positive definite."
numerical operations,"The conjugate residual method differs from the closely related conjugate gradient method primarily in that it involves more numerical operations and requires more storage, but the system matrix is only required to be Hermitian, not symmetric positive definite."
representation learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning."
feature learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms. !! In supervised feature learning, features are learned using labeled input data. !! Feature learning can be either supervised or unsupervised. !! In unsupervised feature learning, features are learned with unlabeled input data. !! Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process."
feature detection,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data."
raw data,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! As well, raw data have not been subject to any other manipulation by a software program or a human researcher, analyst or technician. !! A cyclic redundancy check (CRC) is an error-detecting code commonly used in digital networks and storage devices to detect accidental changes to raw data. !! In the context of examinations, the raw data might be described as a raw score (after test scores). !! Raw data have not been subjected to processing, ""cleaning"" by researchers to remove outliers, obvious instrument reading errors or data entry errors, or any analysis (e. g. , determining central tendency aspects such as the average or median result). !! Raw data, also known as primary data, are data (e. g. , numbers, instrument readings, figures, etc. ) !! If a scientist sets up a computerized thermometer which records the temperature of a chemical mixture in a test tube every minute, the list of temperature readings for every minute, as printed out on a spreadsheet or viewed on a computer screen are ""raw data""."
machine learning tasks,Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process.
supervised feature learning,"In supervised feature learning, features are learned using labeled input data."
unlabeled input data,"In unsupervised feature learning, features are learned with unlabeled input data."
unsupervised feature learning,"In unsupervised feature learning, features are learned with unlabeled input data."
tree decomposition,"The concept of tree decompositions was originally introduced by Rudolf Halin (1976). !! Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition. !! Thus, given a graph G = (V, E), a tree decomposition is a pair (X, T), where X = {X1, . !! Intuitively, a tree decomposition represents the vertices of a given graph G as subtrees of a tree, in such a way that vertices in the given graph are adjacent only when the corresponding subtrees intersect. !! In graph theory, a tree decomposition is a mapping of a graph into a tree that can be used to define the treewidth of the graph and speed up solving certain computational problems on the graph."
graph theory,"a depth-first search starting at the node A, assuming that the left edges in the shown graph are chosen before right edges, and assuming the search remembers previously visited nodes and will not repeat them (since this is a small graph), will visit the nodes in the following order: A, B, D, F, E, C, G. The edges traversed in this search form a Trmaux tree, a structure with important applications in graph theory. !! In the mathematical field of graph theory, a spanning tree T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G. In general, a graph may have several spanning trees, but a graph that is not connected will not contain a spanning tree (see spanning forests below). !! Definitions in graph theory vary. !! In mathematics, and more specifically in graph theory, a directed graph (or digraph) is a graph that is made up of a set of vertices connected by directed edges often called arcs. !! Graph theory plays an important role in electrical modeling of electrical networks, here, weights are associated with resistance of the wire segments to obtain electrical properties of network structures. !! In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! Chemical graph theory uses the molecular graph as a means to model molecules. !! In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. !! The power of trace theory stems from the fact that the algebra of dependency graphs (such as Petri nets) is isomorphic to that of trace monoids, and thus, one can apply both algebraic formal language tools, as well as tools from graph theory. !! Structural induction is a proof method that is used in mathematical logic (e. g. , in the proof of o' theorem), computer science, graph theory, and some other mathematical fields. !! In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! Graph theory is also used to study molecules in chemistry and physics. !! In computing and graph theory, a dynamic connectivity structure is a data structure that dynamically maintains information about the connected components of a graph. !! In graph theory and theoretical computer science, the longest path problem is the problem of finding a simple path of maximum length in a given graph. !! In graph theory, a tree decomposition is a mapping of a graph into a tree that can be used to define the treewidth of the graph and speed up solving certain computational problems on the graph. !! In graph theory, the metric k-center or metric facility location problem is a combinatorial optimization problem studied in theoretical computer science."
tree decompositions,"The concept of tree decompositions was originally introduced by Rudolf Halin (1976). !! Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition. !! Many algorithmic problems that are NP-complete for arbitrary graphs may be solved efficiently for partial k-trees by dynamic programming, using the tree decompositions of these graphs."
join trees,"Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition."
clique trees,"Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition."
matrix decomposition,"Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition."
query optimization,"Query optimization is a feature of many relational database management systems and other databases such as graph databases. !! Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition. !! The purpose of query optimization, which is an automated process, is to find the way to process a given query in minimum time. !! Thus query optimization typically tries to approximate the optimum by comparing several common-sense alternatives to provide in a reasonable time a ""good enough"" plan which typically does not deviate much from the best possible result. !! The large possible variance in time justifies performing query optimization, though finding the exact optimal query plan, among all possibilities, is typically very complex, time-consuming by itself, may be too costly, and often practically impossible. !! Query plans for nested SQL queries can also be chosen using the same dynamic programming algorithm as used for join ordering, but this can lead to an enormous escalation in query optimization time."
binary file,"An object code optimizer, sometimes also known as a post pass optimizer or, for small sections of code, peephole optimizer, takes the output from a source language compile step - the object code or binary file - and tries to replace identifiable sections of the code with replacement code that is more algorithmically efficient (usually improved speed)."
object code,"An object code optimizer, sometimes also known as a post pass optimizer or, for small sections of code, peephole optimizer, takes the output from a source language compile step - the object code or binary file - and tries to replace identifiable sections of the code with replacement code that is more algorithmically efficient (usually improved speed)."
object code optimizer,"An object code optimizer, sometimes also known as a post pass optimizer or, for small sections of code, peephole optimizer, takes the output from a source language compile step - the object code or binary file - and tries to replace identifiable sections of the code with replacement code that is more algorithmically efficient (usually improved speed)."
mathematical logic,"In mathematical logic, a universal quantification is a type of quantifier, a logical constant which is interpreted as ""given any"" or ""for all"". !! Combinatory logic is a notation to eliminate the need for quantified variables in mathematical logic. !! In mathematical logic, a formula of first-order logic is in Skolem normal form if it is in prenex normal form with only universal first-order quantifiers. !! In mathematical logic, Skolem arithmetic is the first-order theory of the natural numbers with multiplication, named in honor of Thoralf Skolem. !! In computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set). !! Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects. !! The OR gate is a digital logic gate that implements logical disjunction () from mathematical logic it behaves according to the truth table above. !! Lambda calculus (also written as -calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. !! In universal algebra and mathematical logic, a term algebra is a freely generated algebraic structure over a given signature. !! Structural induction is a proof method that is used in mathematical logic (e. g. , in the proof of o' theorem), computer science, graph theory, and some other mathematical fields. !! Categorical logic is the branch of mathematics in which tools and concepts from category theory are applied to the study of mathematical logic. !! In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. !! The AND gate is a basic digital logic gate that implements logical conjunction () from mathematical logic AND gate behaves according to the truth table above. !! Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. !! In mathematical logic and theoretical computer science, an abstract rewriting system (also (abstract) reduction system or abstract rewrite system; abbreviated ARS) is a formalism that captures the quintessential notion and properties of rewriting systems."
theoretical computer science,"Martin Davis, Ron Sigal, Elaine J. Weyuker, Computability, complexity, and languages: fundamentals of theoretical computer science, 2nd ed. !! The term compressed data structure arises in the computer science subfields of algorithms, data structures, and theoretical computer science. !! In theoretical computer science, anonymous recursion is important, as it shows that one can implement recursion without requiring named functions. !! In theoretical computer science, a Markov algorithm is a string rewriting system that uses grammar-like rules to operate on strings of symbols. !! It is a theory in theoretical computer science, under discrete mathematics (a section of mathematics and also of computer science). !! Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. !! The Annual ACM Symposium on Theory of Computing (STOC) is an academic conference in the field of theoretical computer science. !! In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e. g. , approximate solutions versus precise ones). !! Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory. !! Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P NP conjecture. !! In mathematical logic and theoretical computer science, an abstract rewriting system (also (abstract) reduction system or abstract rewrite system; abbreviated ARS) is a formalism that captures the quintessential notion and properties of rewriting systems. !! These developments have led to the modern study of logic and computability, and indeed the field of theoretical computer science as a whole. !! Algorithmic information theory (AIT) is a branch of theoretical computer science that concerns itself with the relationship between computation and information of computably generated objects (as opposed to stochastically generated), such as strings or any other data structure. !! In discrete mathematics and theoretical computer science, the rotation distance between two binary trees with the same number of nodes is the minimum number of tree rotations needed to reconfigure one tree into another. !! A counter machine is an abstract machine used in a formal logic and theoretical computer science to model computation. !! In theoretical computer science, communication complexity studies the amount of communication required to solve a problem when the input to the problem is distributed among two or more parties. !! In graph theory and theoretical computer science, the longest path problem is the problem of finding a simple path of maximum length in a given graph. !! In graph theory, the metric k-center or metric facility location problem is a combinatorial optimization problem studied in theoretical computer science."
uniform binary search,"Uniform binary search may be faster on systems where it is inefficient to calculate the midpoint, such as on decimal computers. !! The uniform binary search was developed by A. K. Chandra of Stanford University in 1971. !! Uniform binary search would store the value of 3 as both indices differ from 6 by this same amount. !! Uniform binary search stores, instead of the lower and upper bounds, the difference in the index of the middle element from the current iteration to the next iteration."
decimal computers,"Uniform binary search may be faster on systems where it is inefficient to calculate the midpoint, such as on decimal computers."
relational database,"A relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970. !! The term ""relational database"" was invented by E. F. Codd at IBM in 1970. !! Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database. !! A system used to maintain relational databases is a relational database management system (RDBMS). !! Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. !! One well-known definition of what constitutes a relational database system is composed of Codd's 12 rules. !! A Data Mapper is a Data Access Layer that performs bidirectional transfer of data between a persistent data store (often a relational database) and an in-memory data representation (the domain layer). !! Armstrong's axioms are a set of references (or, more precisely, inference rules) used to infer all the functional dependencies on a relational database. !! Dimensional modeling does not necessarily involve a relational database. !! A spatial database is a general-purpose database (usually a relational database) that has been enhanced to include spatial data that represents objects defined in a geometric space, along with tools for querying and analyzing such data. !! A database organized in terms of the relational model is a relational database. !! First normal form (1NF) is a property of a relation in a relational database."
relational databases,"Object databases are different from relational databases which are table-oriented. !! Third normal form (3NF) is a database schema design approach for relational databases which uses normalizing principles to reduce the duplication of data, avoid data anomalies, ensure referential integrity, and simplify data management. !! SQL-92 does not support creating or using table-valued columns, which means that using only the ""traditional relational database features"" (excluding extensions even if they were later standardized) most relational databases will be in first normal form by necessity. !! The problem of database repair is a question about relational databases which has been studied in database theory, and which is a particular kind of data cleansing."
sql systems,Database systems which do not require first normal form are often called no sql systems.
database systems,"Database systems which do not require first normal form are often called no sql systems. !! In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems. !! In computer science, shadow paging is a technique for providing atomicity and durability (two of the ACID properties) in database systems. !! The main research conferences in the area are the ACM Symposium on Principles of Database Systems (PODS) and the International Conference on Database Theory (ICDT)."
artificial neural network,"A residual neural network (ResNet) is an artificial neural network (ANN). !! An artificial neural network consists of a collection of simulated neurons. !! The Helmholtz machine (named after Hermann von Helmholtz and his concept of Helmholtz free energy) is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. !! In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of Artificial Neural Network(ANN), most commonly applied to analyze visual imagery. !! A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. !! Connectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e. g. , fuzzy-multi layer perceptron where linguistic and natural form of inputs are used. !! In 1958, psychologist Frank Rosenblatt invented the perceptron, the first artificial neural network, funded by the United States Office of Naval Research. !! A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. !! Stochastic neural networks originating from SherringtonKirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights. !! Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. !! Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems inspired by the biological neural networks that constitute animal brains. !! This provided more processing power for the development of practical artificial neural networks in the 1980s."
artificial neural networks,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models. !! Artificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks. !! Self-organizing maps, like most artificial neural networks, operate in two modes: training and mapping. !! Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks. !! This provided more processing power for the development of practical artificial neural networks in the 1980s. !! Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. !! The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. !! This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. !! In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. !! Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. !! The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e. g. weights of connections between neurons in artificial neural networks) of the model. !! With respect to other advanced machine learning approaches, such as artificial neural networks, random forests, or genetic programming, learning classifier systems are particularly well suited to problems that require interpretable solutions. !! Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. !! Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. !! Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems inspired by the biological neural networks that constitute animal brains. !! In some fields, most notably in the context of artificial neural networks, the term ""sigmoid function"" is used as an alias for the logistic function. !! Using Artificial neural networks requires an understanding of their characteristics. !! A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. !! Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. !! Biological neural networks have inspired the design of artificial neural networks, but artificial neural networks are usually not strict copies of their biological counterparts. !! Neurocomputational speech processing is speech processing by artificial neural networks."
processing power,"Computational archaeology may include the use of geographical information systems (GIS), especially when applied to spatial analyses such as viewshed analysis and least-cost path analysis as these approaches are sufficiently computationally complex that they are extremely difficult if not impossible to implement without the processing power of a computer. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! An ASIC based IPS may detect and block denial-of-service attacks because they have the processing power and the granularity to analyze the attacks and act like a circuit breaker in an automated way. !! Single core processors used to be widespread in desktop computers, but as applications demanded more processing power, the slower speed of single core systems became a detriment to performance. !! This provided more processing power for the development of practical artificial neural networks in the 1980s."
simulated neurons,An artificial neural network consists of a collection of simulated neurons.
artificial neurons stochastic transfer functions,"Stochastic neural networks originating from SherringtonKirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights."
stochastic weights,"Stochastic neural networks originating from SherringtonKirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights."
fuzzy sets,"In mathematics, vague sets are an extension of fuzzy sets."
vague set,"proposed the notion of vague sets, where each object is characterized by two different membership functions: a true membership function and a false membership function. !! In mathematics, vague sets are an extension of fuzzy sets. !! Vague Sets"
logic programming,"Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic. !! In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs. !! Logic programming is a programming paradigm which is largely based on formal logic. !! Computational logic has also come to be associated with logic programming, because much of the early work in logic programming in the early 1970s also took place in the Department of Computational Logic in Edinburgh. !! The stable model semantics, which is used to give a semantics to logic programming with negation as failure, can be seen as a simplified form of autoepistemic logic. !! Backward chaining is implemented in logic programming by SLD resolution. !! Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. !! Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain."
logic programming language,"Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain."
logic programming languages,"Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic. !! The first host languages used were logic programming languages, so the field was initially called constraint logic programming. !! In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs."
classification problem,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. !! In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. !! Empirical risk minimization for a classification problem with a 0-1 loss function is known to be an NP-hard problem even for such a relatively simple class of functions as linear classifiers."
problem space,A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.
output label,A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.
hidden layers,"A convolutional neural network consists of an input layer, hidden layers and an output layer. !! In the case of backpropagation based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has."
backpropagation based artificial neural networks,"In the case of backpropagation based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has."
universal approximation theorem,"If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary."
continuous function,"If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary."
computational learning theory,"While its primary goal is to understand learning abstractly, computational learning theory has led to the development of practical algorithms. !! In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms. !! In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning. !! computational learning theory, a computation is considered feasible if it can be done in polynomial time. !! Computational learning theory: Survey and selected bibliography. !! In computational learning theory, induction of regular languages refers to the task of learning a formal description (e. g. grammar) of a regular language from a given set of example strings."
learning theory,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms."
time complexity,"In both cases, the time complexity is generally expressed as a function of the size of the input. !! In fact, for showing that a computable function is primitive recursive, it suffices to show that its time complexity is bounded above by a primitive recursive function of the input size. !! In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. !! The time complexity of Prim's algorithm depends on the data structures used for the graph and for ordering the edges by weight, which can be done using a priority queue. !! In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning. !! Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. !! An algorithm is said to take linear time, or O(n) time, if its time complexity is O(n). !! The time complexity of operations on the binary search tree is directly proportional to the height of the tree."
system performance,"Performance tuning is the improvement of system performance. !! Cognitive ergonomics studies cognition in work and operational settings, in order to optimize human well-being and system performance."
linear combination,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. !! In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials. !! In the logistic model, the log-odds (the logarithm of the odds) for the value labeled ""1"" is a linear combination of one or more independent variables (""predictors""); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). !! In the above polynomial interpolations using a linear combination of the given values, the coefficients were determined using the Lagrange method. !! Standardized Coefficients: Each predictor's weight in the linear combination that is the discriminant function. !! In many scenarios, an efficient and convenient polynomial interpolation is a linear combination of the given values, using previously known coefficients."
numerical analysis,"In numerical analysis, inverse iteration (also known as the inverse power method) is an iterative eigenvalue algorithm. !! Techniques for series acceleration are often applied in numerical analysis, where they are used to improve the speed of numerical integration. !! George, J. Alan (1973), ""Nested dissection of a regular finite element mesh"", SIAM Journal on Numerical Analysis, 10 (2): 345363, !! In numerical analysis, polynomial interpolation is the interpolation of a given data set by the polynomial of lowest possible degree that passes through the points of the dataset. !! In numerical analysis, an incomplete Cholesky factorization of a symmetric positive definite matrix is a sparse approximation of the Cholesky factorization. !! Numerical analysis finds application in all fields of engineering and the physical sciences, and in the 21st century also the life and social sciences, medicine, business and even the arts. !! Current growth in computing power has enabled the use of more complex numerical analysis, providing detailed and realistic mathematical models in science and engineering. !! In numerical analysis, the Shanks transformation is a non-linear series acceleration method to increase the rate of convergence of a sequence. !! Numerical analysis continues this long tradition: rather than giving exact symbolic answers translated into digits and applicable only to real-world measurements, approximate solutions within specified error bounds are used. !! Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set. !! In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. !! In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials. !! In numerical analysis the diffuse element method (DEM) or simply diffuse approximation is a meshfree method. !! In numerical analysis, interpolative decomposition (ID) factors a matrix as the product of two matrices, one of which contains selected columns from the original matrix, and the other of which has a subset of columns consisting of the identity matrix and all its values are no greater than 2 in absolute value. !! First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc. !! Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). !! In numerical analysis the minimum degree algorithm is an algorithm used to permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, to reduce the number of non-zeros in the Cholesky factor. !! For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated scientific problems; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. !! In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations. !! In numerical analysis, nested dissection is a divide and conquer heuristic for the solution of sparse symmetric systems of linear equations based on graph partitioning. !! In numerical analysis, finite differences are widely used for approximating derivatives, and the term ""finite difference"" is often used as an abbreviation of ""finite difference approximation of derivatives"". !! Rational functions are used in numerical analysis for interpolation and approximation of functions, for example the Pad approximations introduced by Henri Pad. !! Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology."
bernstein basis polynomials,"In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials."
bernstein polynomial,"With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bzier curves. !! In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials. !! Bernstein polynomials can be generalized to k dimensions the resulting polynomials have the form Bi1(x1) Bi2(x2) . !! A generalized parametric PR-QMF design technique based on Bernstein polynomial approximation. !! In the simplest case only products of the unit interval [0,1] are considered; but, using affine transformations of the line, Bernstein polynomials can also be defined for products [a1, b1] [a2, b2] ."
bernstein polynomials,"With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bzier curves. !! Bernstein polynomials can be generalized to k dimensions the resulting polynomials have the form Bi1(x1) Bi2(x2) . !! In the simplest case only products of the unit interval [0,1] are considered; but, using affine transformations of the line, Bernstein polynomials can also be defined for products [a1, b1] [a2, b2] ."
confidence score,"Pool-Based Sampling: In this scenario, instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner understands the data."
pool-based sampling,"Pool-Based Sampling: In this scenario, instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner understands the data."
propositional logic,"Unlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. !! While propositional logic can only express facts, autoepistemic logic can express knowledge and lack of knowledge about facts. !! It is also called propositional logic, statement logic, sentential calculus, sentential logic, or sometimes zeroth-order logic. !! The semantics of autoepistemic logic is based on the expansions of a theory, which have a role similar to models in propositional logic. !! This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic. !! Propositional logic may be studied through a formal system in which formulas of a formal language may be interpreted to represent propositions. !! In this sense, propositional logic is the foundation of first-order logic and higher-order logic. !! In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference. !! However, all the machinery of propositional logic is included in first-order logic and higher-order logics."
numerical computing programs,"UserLAnd Technologies is a free and open-source ad-free compatibility layer mobile app that allows Linux distributions, computer programs, computer games and numerical computing programs to run on mobile devices without requiring a root account."
computer games,"UserLAnd Technologies is a free and open-source ad-free compatibility layer mobile app that allows Linux distributions, computer programs, computer games and numerical computing programs to run on mobile devices without requiring a root account. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics."
computer programs,"Mathematical induction is an inference rule used in formal proofs, and is the foundation of most correctness proofs for computer programs. !! An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs. !! Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. !! UserLAnd Technologies is a free and open-source ad-free compatibility layer mobile app that allows Linux distributions, computer programs, computer games and numerical computing programs to run on mobile devices without requiring a root account. !! Trace scheduling is an optimization technique developed by Josh Fisher used in compilers for computer programs. !! In software engineering, version control (also known as revision control, source control, or source code management) is a class of systems responsible for managing changes to computer programs, documents, large web sites, or other collections of information. !! In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices. !! In software engineering, structured analysis (SA) and structured design (SD) are methods for analyzing business requirements and developing specifications for converting practices into computer programs, hardware configurations, and related manual procedures."
huffman coding,Adaptive Huffman coding (also called Dynamic Huffman coding) is an adaptive coding technique based on Huffman coding.
adaptive huffman coding,"This article incorporates public domain material from the NIST document: Black, Paul E. ""adaptive Huffman coding"". !! Adaptive Huffman coding (also called Dynamic Huffman coding) is an adaptive coding technique based on Huffman coding."
bitwise operation,"On simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition. !! Most bitwise operations are presented as two-operand instructions where the result replaces one of the input operands. !! While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer instruction pipelines and other architectural design choices, bitwise operations do commonly use less power because of the reduced use of resources. !! In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits. !! The bit shifts are sometimes considered bitwise operations, because they treat a value as a series of bits rather than as a numerical quantity."
bit array,"In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits."
bit string,"In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits."
bitwise operations,"On simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition. !! Most bitwise operations are presented as two-operand instructions where the result replaces one of the input operands. !! Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields. !! In computing, an arithmetic logic unit (ALU) is a combinational digital circuit that performs arithmetic and bitwise operations on integer binary numbers. !! While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer instruction pipelines and other architectural design choices, bitwise operations do commonly use less power because of the reduced use of resources."
input operands,Most bitwise operations are presented as two-operand instructions where the result replaces one of the input operands.
gradient descent,"The basic intuition behind gradient descent can be illustrated by a hypothetical scenario. !! The model (e. g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. !! They can use the method of gradient descent, which involves looking at the steepness of the hill at their current position, then proceeding in the direction with the steepest descent (i. e. , downhill). !! Moreover, not requiring the computation or approximation of function derivatives makes successive parabolic interpolation a popular alternative to other methods that do require them (such as gradient descent and Newton's method). !! Gradient descent is generally attributed to Cauchy, who first suggested it in 1847. !! In mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. !! is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution."
local minimum,In mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
differentiable function,In mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
local minima,"is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution."
software engineering,"Behavior trees are a formal, graphical modelling language used primarily in systems and software engineering. !! In software engineering, the mediator pattern defines an object that encapsulates how a set of objects interact. !! In contrast to manifold approaches and techniques in software engineering, software diagnosis does not depend on programming languages, modeling techniques, software development processes or the specific techniques used in the various stages of the software development process. !! In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. !! Single and composite or integrated behavior tree forms are both important in the application of behavior trees in systems and software engineering. !! Architecture description languages (ADLs) are used in several disciplines: system engineering, software engineering, and enterprise modelling and engineering. !! In software engineering, dependency injection is a technique in which an object receives other objects that it depends on, called dependencies. !! In software engineering, version control (also known as revision control, source control, or source code management) is a class of systems responsible for managing changes to computer programs, documents, large web sites, or other collections of information. !! In software engineering, the data mapper pattern is an architectural pattern. !! Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. !! In software engineering, the composite pattern is a partitioning design pattern. !! In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements. !! Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! In software engineering, structured analysis (SA) and structured design (SD) are methods for analyzing business requirements and developing specifications for converting practices into computer programs, hardware configurations, and related manual procedures. !! In software engineering, the active record pattern is considered an architectural pattern by some people and as an anti-pattern by some others recently. !! Beginning in the 1960s, software engineering was seen as its own type of engineering. !! They show how the results obtained with a triangulation of SIM and CEM point at new research avenues not only for semiotic engineering and HCI but also for other areas of computer science such as software engineering and programming. !! A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. !! Often one of the biggest problems in software engineering is that the requirements change quickly and the internet-speed development method was created to adapt to this situation. !! A model transformation language in systems and software engineering is a language intended specifically for model transformation. !! Software analysis patterns or analysis patterns in software engineering are conceptual models, which capture an abstraction of a situation that can often be encountered in modelling. !! In software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to one ""single"" instance. !! Software engineering is an engineering approach on a software development of systematics application. !! In 1968 NATO held the first Software Engineering conference where issues related to software were addressed: guidelines and best practices for the development of software were established. !! In software engineering, software durability means the solution ability of serviceability of software and to meet user's needs for a relatively long time. !! Additionally, the development of software engineering was seen as a struggle. !! Software diagnosis supports all branches of software engineering, in particular project management, quality management, risk management as well as implementation and test. !! Meta-process modeling is a type of metamodeling used in software engineering and systems engineering for the analysis and construction of models applicable and useful to some predefined problems. !! In software engineering, a circular dependency is a relation between two or more modules which either directly or indirectly depend on each other to function properly."
software project,Requirements analysis is critical to the success or failure of a systems or software project. !! Coding conventions are only applicable to the human maintainers and peer reviewers of a software project.
functional analysis,"Because many properties of matrices and vectors also apply to functions and operators, numerical linear algebra can also be viewed as a type of functional analysis which has a particular emphasis on practical algorithms. !! Functional requirements analysis will be used as the toplevel functions for functional analysis. !! Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. !! In functional analysis and quantum measurement theory, a positive-operator-valued measure (POVM) is a measure whose values are positive semi-definite operators on a Hilbert space."
big data,"Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data. !! The voice and style of the contentThe use of content intelligence is therefore connected to the science of big data and artificial intelligence. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! Data science is related to data mining, machine learning and big data. !! Approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy, when exact learning and inference are computationally intractable."
computation time,"Approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy, when exact learning and inference are computationally intractable. !! The simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined."
computationally intractable,"Approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy, when exact learning and inference are computationally intractable. !! The optimal computing budget allocation problem is formulated as a Bayesian Markov decision process(MDP) and is solved by using the dynamic programming (DP) algorithm where the Optimistic knowledge gradient policy is used to solve the computationally intractable of the dynamic programming (DP) algorithm."
deep reinforcement learning,"Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. !! Along with rising interest in neural networks beginning in the mid 1980s, interest grew in deep reinforcement learning, where a neural network is used in reinforcement learning to represent policies or value functions. !! Deep reinforcement learning has also been applied to many domains beyond games. !! Deep reinforcement learning reached another milestone in 2015 when AlphaGo, a computer program trained with deep RL to play Go, became the first computer Go program to beat a human professional Go player without handicap on a full-sized 1919 board. !! Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning."
deep learning,"Deep learning is a class of machine learning algorithms that:199200 uses multiple layers to progressively extract higher-level features from the raw input. !! In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of Artificial Neural Network(ANN), most commonly applied to analyze visual imagery. !! Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. !! The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. !! With the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks. !! Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. !! Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. !! Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. !! AI Bridging Cloud Infrastructure (ABCI) is a planned supercomputer being built at the University of Tokyo for use in artificial intelligence, machine learning, and deep learning. !! Since about 2016, deep learning has emerged as the dominant method for performing accurate articulated body pose estimation. !! In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the ""structured"" part."
computer vision,"Object detection has applications in many areas of computer vision, including image retrieval and video surveillance. !! Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy. !! Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. !! Applications of graphical models include causal inference, information extraction, speech recognition, computer vision, decoding of low-density parity-check codes, modeling of gene regulatory networks, gene finding and diagnosis of diseases, and graphical models for protein structure. !! Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. !! Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. !! Automated sign language translationGesture recognition can be conducted with techniques from computer vision and image processing. !! Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics. !! Articulated body pose estimation in computer vision is the study of algorithms and systems that recover the pose of an articulated body, which consists of joints and rigid parts using image-based observations. !! Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. !! Geometric feature learning is a technique combining machine learning and computer vision to solve visual tasks. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing. !! In computer vision or natural language processing, document layout analysis is the process of identifying and categorizing the regions of interest in the scanned image of a text document. !! Object recognition technology in the field of computer vision for finding and identifying objects in an image or video sequence."
natural language processing,"Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. !! Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. !! There has since been a large body of work centered around data streaming algorithms that spans a diverse spectrum of computer science fields such as theory, databases, networking, and natural language processing. !! 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted ""blocks worlds"" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. !! Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. !! Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. !! The principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to semantic parsing, natural language understanding, example-based translation, morpheme analysis, and place name derivations. !! Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. !! Natural language processing has its roots in the 1950s. !! In computer vision or natural language processing, document layout analysis is the process of identifying and categorizing the regions of interest in the scanned image of a text document. !! Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. !! Lexical analysis is also an important early stage in natural language processing, where text or sound waves are segmented into words and other units. !! Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing. !! Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. !! In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning."
reinforcement learning,"The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible. !! In the older paper from 1992, the action model learning was studied as an extension of reinforcement learning. !! Interest in learning classifier systems was reinvigorated in the mid 1990s largely due to two events; the development of the Q-Learning algorithm for reinforcement learning, and the introduction of significantly simplified Michigan-style LCS architectures by Stewart Wilson. !! Along with rising interest in neural networks beginning in the mid 1980s, interest grew in deep reinforcement learning, where a neural network is used in reinforcement learning to represent policies or value functions. !! The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. !! Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. !! Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e. g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). !! With respect to the field of reinforcement learning, learning automata are characterized as policy iterators. !! Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. !! Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. !! Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning."
neural network,"The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. !! Along with rising interest in neural networks beginning in the mid 1980s, interest grew in deep reinforcement learning, where a neural network is used in reinforcement learning to represent policies or value functions. !! Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. !! In 1981, a report was given on the application of transfer learning in training a neural network on a dataset of images representing letters of computer terminals. !! A Hopfield network (or Ising model of a neural network or IsingLenzLittle model) is a form of recurrent artificial neural network and a type of spin glass system popularised by John Hopfield in 1982 as described earlier by Little in 1974 based on Ernst Ising's work with Wilhelm Lenz on the Ising model. !! The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e. g. the number of hidden unitslayers and layer widthsin a neural network). !! Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. !! A biological neural network is composed of a groups of chemically connected or functionally associated neurons."
compilation error,"Although the definitions of compilation and interpretation can be vague, generally compilation errors only refer to static compilation and not dynamic compilation. !! Compilation error refers to a state when a compiler fails to compile a piece of computer program source code, either due to errors in the code, or, more unusually, due to errors in the compiler itself. !! A compilation error message often helps programmers debugging the source code. !! Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time. !! However, dynamic compilation can still technically have compilation errors, although many programmers and sources may identify them as run-time errors."
computer program source code,"Compilation error refers to a state when a compiler fails to compile a piece of computer program source code, either due to errors in the code, or, more unusually, due to errors in the compiler itself."
dynamic compilation,"However, dynamic compilation can still technically have compilation errors, although many programmers and sources may identify them as run-time errors. !! Although the definitions of compilation and interpretation can be vague, generally compilation errors only refer to static compilation and not dynamic compilation."
static compilation,"Although the definitions of compilation and interpretation can be vague, generally compilation errors only refer to static compilation and not dynamic compilation."
compilation errors,"Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time. !! However, dynamic compilation can still technically have compilation errors, although many programmers and sources may identify them as run-time errors."
run time,"Therefore, polymorphism is given by subtyping polymorphism as in other languages, and it is also extended in functionality by ad hoc polymorphism at run time. !! The use of optimization software requires that the function f is defined in a suitable programming language and connected at compile or run time to the optimization software. !! In computer science, big O notation is used to classify algorithms according to how their run time or space requirements grow as the input size grows. !! Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time."
javascript v8 engine,"Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time."
program text,"In computer programming, a free-form language is a programming language in which the positioning of characters on the page in program text is insignificant. !! Program transformations may be specified as automated procedures that modify compiler data structures (e. g. abstract syntax trees) representing the program text, or may be specified more conveniently using patterns or templates representing parameterized source code fragments."
pattern calculus,Pattern calculus bases all computation on pattern matching of a very general kind. !! concurrent pattern calculus
pattern matching,"Parsing algorithms often rely on pattern matching to transform strings into syntax trees. !! Early programming languages with pattern matching constructs include COMIT (1957), SNOBOL (1962), Refal (1968) with tree-based pattern matching, Prolog (1972), SASL (1976), NPL (1977), and KRC (1981). !! In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. !! Important examples of compressed data structures include the compressed suffix array and the FM-index, both of which can represent an arbitrary text of characters T for pattern matching. !! Pattern calculus bases all computation on pattern matching of a very general kind. !! Pattern matching sometimes includes support for guards. !! Uses of pattern matching include outputting the locations (if any) of a pattern within a token sequence, to output some component of the matched pattern, and to substitute the matching pattern with some other token sequence (i. e. , search and replace)."
concurrent pattern calculus,concurrent pattern calculus
scientific visualization,"One definition is that it's information visualization when the spatial representation (e. g. , the page layout of a graphic design) is chosen, whereas it's scientific visualization when the spatial representation is given. !! In scientific visualization, line integral convolution (LIC) is a technique proposed by Brian Cabral and Leith Leedom to visualize a vector field, such as fluid motion."
vector field,"In scientific visualization, line integral convolution (LIC) is a technique proposed by Brian Cabral and Leith Leedom to visualize a vector field, such as fluid motion."
web caching,"In computer science, consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as filesystems, databases, optimistic replication systems or web caching)."
distributed data stores,"In computer science, consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as filesystems, databases, optimistic replication systems or web caching)."
distributed information processing,"Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control."
scientific computing,"In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations. !! Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. !! Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols. !! Computational science, also known as scientific computing or scientific computation (SC), is a rapidly growing field that uses advanced computing capabilities to understand and solve complex problems. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform."
distributed algorithms,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation. !! One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. !! Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. !! MIT Open Courseware - Distributed Algorithms !! Distributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing."
distributed search,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation."
resource allocation,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation. !! Analysis of Resource Allocation of Stochastic Diffusion Search."
spanning tree generation,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation."
parallel algorithm,"Distributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. !! The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption."
semantic predicate,A syntactic predicate specifies the syntactic validity of applying a production in a formal grammar and is analogous to a semantic predicate that specifies the semantic validity of applying a production.
semantic predicates,The term syntactic predicate was coined by Parr & Quong and differentiates this form of predicate from semantic predicates (also discussed).
computing machinery,ACM Transactions on Computer Systems is a quarterly peer-reviewed scientific journal published by the Association for Computing Machinery.
probability distributions,"In statistics and information theory, a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions. !! In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions."
maximum entropy probability distribution,"Every probability distribution is trivially a maximum entropy probability distribution under the constraint that the distribution has its own entropy. !! In statistics and information theory, a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions."
signal processing,"In signal processing, a digital filter is a system that performs mathematical operations on a sampled, discrete-time signal to reduce or enhance certain aspects of that signal. !! k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. !! Digital signal processing and analog signal processing are subfields of signal processing. !! While multidimensional signal processing is a subset of signal processing, it is unique in the sense that it deals specifically with data that can only be adequately detailed using more than one dimension. !! Cognitive computing (CC) refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. !! Mathematical Morphology and its Application to Signal Processing, J. Serra and Ph. !! Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics. !! Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. !! Analog signal processing is a type of signal processing conducted on continuous analog signals by some analog means (as opposed to the discrete digital signal processing where the signal processing is carried out by a digital process). !! In signal processing, multidimensional signal processing covers all signal processing done using multidimensional signals and systems."
digital filter,"In signal processing, a digital filter is a system that performs mathematical operations on a sampled, discrete-time signal to reduce or enhance certain aspects of that signal. !! A digital filter system usually consists of an analog-to-digital converter (ADC) to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc. !! Program Instructions (software) running on the microprocessor implement the digital filter by performing the necessary mathematical operations on the numbers received from the ADC. !! Digital filters may be more expensive than an equivalent analog filter due to their increased complexity, but they make practical many designs that are impractical or impossible as analog filters. !! Digital filters can often be made very high order, and are often finite impulse response filters, which allows for linear phase response."
input signal,"The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. !! A digital filter system usually consists of an analog-to-digital converter (ADC) to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc."
analog filters,"Digital filters may be more expensive than an equivalent analog filter due to their increased complexity, but they make practical many designs that are impractical or impossible as analog filters."
digital filters,"Digital filters can often be made very high order, and are often finite impulse response filters, which allows for linear phase response."
computational complexity theory,"More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. !! In computability theory and computational complexity theory, an undecidable problem is a decision problem for which it is proved to be impossible to construct an algorithm that always leads to a correct yes-or-no answer. !! In computational complexity theory, the linear search problem is an optimal search problem introduced by Richard E. Bellman and independently considered by Anatole Beck. !! In computational complexity theory, a pseudo-polynomial transformation is a function which maps instances of one strongly NP-complete problem into another and is computable in pseudo-polynomial time. !! This application of abstract machines is related to the subject of computational complexity theory. !! In computational complexity theory and game complexity, a parsimonious reduction is a transformation from one problem to another (a reduction) that preserves the number of solutions. !! In computational complexity theory, a problem refers to the abstract question to be solved. !! One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. !! In computational complexity theory, the maximum satisfiability problem (MAX-SAT) is the problem of determining the maximum number of clauses, of a given Boolean formula in conjunctive normal form, that can be made true by an assignment of truth values to the variables of the formula. !! In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem. !! In computer science, parameterized complexity is a branch of computational complexity theory that focuses on classifying computational problems according to their inherent difficulty with respect to multiple parameters of the input or output. !! In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems. !! In computer science, a first-order reduction is a very strong type of reduction between two computational problems in computational complexity theory. !! A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. !! An algorithm is said to be of polynomial time if its running time is upper bounded by a polynomial expression in the size of the input for the algorithm, that is, T(n) = O(nk) for some positive constant k. Problems for which a deterministic polynomial time algorithm exists belong to the complexity class P, which is central in the field of computational complexity theory. !! Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. !! Combinatorial optimization is related to operations research, algorithm theory, and computational complexity theory. !! The set of primitive recursive functions is known as PR in computational complexity theory. !! Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory. !! In the computational complexity theory of counting problems, a polynomial-time counting reduction is a type of reduction (a transformation from one problem to another) used to define the notion of completeness for the complexity class P. !! In computational complexity theory, a polynomial-time reduction is a method for solving one problem using another. !! Note that, unlike in computational complexity theory, communication complexity is not concerned with the amount of computation performed by Alice or Bob, or the size of the memory used, as we generally assume nothing about the computational power of either Alice or Bob. !! In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is a polynomial in the numeric value of the input (the largest integer present in the input)but not necessarily in the length of the input (the number of bits required to represent it), which is the case for polynomial time algorithms."
computational models,"In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems."
computational resource,"Computational resources are useful because we can study which problems can be computed in a certain amount of each computational resource. !! The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory. !! The simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined. !! As the inputs get bigger, the amount of computational resources needed to solve a problem will increase. !! In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems."
computational problems,"In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems. !! Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. !! The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory."
memory space,"The methodology of run-time analysis can also be utilized for predicting other growth rates, such as consumption of memory space. !! The simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined."
computational resources,"Graph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models. !! Computational resources are useful because we can study which problems can be computed in a certain amount of each computational resource."
complexity theory,"The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. !! Polynomial-time reductions are frequently used in complexity theory for defining both complexity classes and complete problems for those classes. !! The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory."
nested loop,"In compiler theory, loop interchange is the process of exchanging the order of two iteration variables used by a nested loop."
compiler theory,"In compiler theory, partial redundancy elimination (PRE) is a compiler optimization that eliminates expressions that are redundant on some but not necessarily all paths through a program. !! In compiler theory, loop optimization is the process of increasing execution speed and reducing the overheads associated with loops. !! In compiler theory, loop interchange is the process of exchanging the order of two iteration variables used by a nested loop. !! In compiler theory, dependence analysis produces execution-order constraints between statements/instructions."
cpu cache,"However, the TLB cache is part of the memory management unit (MMU) and not directly related to the CPU caches. !! In computing, data-oriented design is a program optimization approach motivated by efficient usage of the CPU cache, used in video game development. !! Memory part 2: CPU caches an article on lwn. !! The major purpose of loop interchange is to take advantage of the CPU cache when accessing array elements. !! In computer engineering, a tag RAM is used to specify which of the possible memory locations is currently stored in a CPU cache. !! A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. !! A victim cache is a cache used to hold blocks evicted from a CPU cache upon replacement."
compiler optimization,"In compiler theory, partial redundancy elimination (PRE) is a compiler optimization that eliminates expressions that are redundant on some but not necessarily all paths through a program. !! Like any compiler optimization, loop interchange may lead to worse performance because cache performance is only part of the story. !! In compiler optimization, register allocation is the process of assigning local automatic variables and expression results to a limited number of processor registers. !! In computing, inline expansion, or inlining, is a manual or compiler optimization that replaces a function call site with the body of the called function. !! In computer science, loop inversion is a compiler optimization and loop transformation in which a while loop is replaced by an if block containing a do."
random indexing,"Random indexing is a dimensionality reduction method and computational framework for distributional semantics, based on the insight that very-high-dimensional vector space model implementations are impractical, that models need not grow in dimensionality when new items (e. g. new terminology) are encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately. !! Many random indexing methods primarily generate similarity from co-occurrence of items in a corpus. !! The TopSig technique extends the random indexing model to produce bit vectors for comparison with the Hamming distance similarity function. !! Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on sparse distributed memory, and can be described as an incremental formulation of a random projection. !! It can be also verified that random indexing is a random projection technique for the construction of Euclidean spacesi."
computational framework,"Random indexing is a dimensionality reduction method and computational framework for distributional semantics, based on the insight that very-high-dimensional vector space model implementations are impractical, that models need not grow in dimensionality when new items (e. g. new terminology) are encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately."
sparse distributed memory,"Sparse distributed memory (SDM) is a mathematical model of human long-term memory introduced by Pentti Kanerva in 1988 while he was at NASA Ames Research Center. !! Sparse distributed memory is a mathematical representation of human memory, and uses high-dimensional space to help model the large amounts of memory that mimics that of the human neural network. !! The sparse distributed memory system distributes each pattern into approximately one hundredth of the locations, so interference can have detrimental results. !! Genetic memory uses genetic algorithm and sparse distributed memory as a pseudo artificial neural network. !! At the University of Memphis, Uma Ramamurthy, Sidney K. D'Mello, and Stan Franklin created a modified version of the sparse distributed memory system that represents ""realizing forgetting. "" !! Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on sparse distributed memory, and can be described as an incremental formulation of a random projection."
digital data,"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of digital data."
coding theory,"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! In telecommunication, information theory, and coding theory, forward error correction (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels."
programming languages,"In contrast to manifold approaches and techniques in software engineering, software diagnosis does not depend on programming languages, modeling techniques, software development processes or the specific techniques used in the various stages of the software development process. !! repeat the cycle until a suitable level of validation is obtained: the computational scientist trusts that the simulation generates adequately realistic results for the system under the studied conditionsSubstantial effort in computational sciences has been devoted to developing algorithms, efficient implementation in programming languages, and validating computational results. !! In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. !! Phase Distinction is a property of programming languages that observe a strict division between types and terms. !! The most common use of code signing is to provide security when deploying; in some programming languages, it can also be used to help prevent namespace conflicts. !! In the classification of programming languages, an applicative programming language is built out of functions applied to arguments. !! Most programming languages consist of instructions for computers. !! Programming languages are one kind of computer language, and are used in computer programming to implement algorithms. !! In computer programming, run-time type information or run-time type identification (RTTI) is a feature of some programming languages (such as C++, Object Pascal, and Ada) that exposes information about an object's data type at runtime. !! Cognitive dimensions or cognitive dimensions of notations are design principles for notations, user interfaces and programming languages, described by researcher Thomas R. G. Green and furthered researched with Marian Petre. !! A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output. !! Thousands of different programming languages have been created, and more are being created every year. !! Since hardware description languages are not considered to be programming languages by most hardware engineers, hardware refactoring is to be considered a separate field from traditional code refactoring. !! Parsing algorithms for natural language cannot rely on the grammar having 'nice' properties as with manually designed grammars for programming languages. !! In programming languages, ad hoc polymorphism is a kind of polymorphism in which polymorphic functions can be applied to arguments of different types, because a polymorphic function can denote a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied. !! Prior to the release of Deep Learning Studio in January 2017, proficiency in Python, among other programming languages, was essential in developing effective deep learning models. !! He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats. !! Most programming languages have some form of runtime system that provides an environment in which programs run. !! Although the logic has also been studied for its own sake, more broadly, ideas from linear logic have been influential in fields such as programming languages, game semantics, and quantum physics (because linear logic can be seen as the logic of quantum information theory), as well as linguistics, particularly because of its emphasis on resource-boundedness, duality, and interaction. !! There are programmable machines that use a set of specific instructions, rather than general programming languages. !! Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input. !! Short-circuit evaluation, minimal evaluation, or McCarthy evaluation (after John McCarthy) is the semantics of some Boolean operators in some programming languages in which the second argument is executed or evaluated only if the first argument does not suffice to determine the value of the expression: when the first argument of the AND function evaluates to false, the overall value must be false; and when the first argument of the OR function evaluates to true, the overall value must be true. !! Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages."
natural language,"Knowledge representation and reasoning (KRR, KR&R, KR) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. !! Sequence alignments are also used for non-biological sequences, such as calculating the distance cost between strings in a natural language or in financial data. !! Context-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose."
data set,"In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. !! In Ensemble of Classifier Chains (ECC) several CC classifiers can be trained with random order of chains (i. e. random order of labels) on a random subset of data set."
random subset,In Ensemble of Classifier Chains (ECC) several CC classifiers can be trained with random order of chains (i. e. random order of labels) on a random subset of data set.
binary tree,"In computer science, a ternary search tree is a type of trie (sometimes called a prefix tree) where nodes are arranged in a manner similar to a binary search tree, but with up to three children rather than the binary tree's limit of two. !! A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set containing the root. !! A skew heap (or self-adjusting heap) is a heap data structure implemented as a binary tree."
empty set,"A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set containing the root. !! The only non-singleton set with this property is the empty set. !! In mathematics, the power set (or powerset) of a set S is the set of all subsets of S, including the empty set and S itself."
binary trees,"if T1 and T2 are extended binary trees, then denote by T1 T2 the extended binary tree obtained by adding a root r connected to the left to T1 and to the right to T2 by adding edges when these sub-trees are non-empty. !! Because of a combinatorial equivalence between binary trees and triangulations of convex polygons, rotation distance is equivalent to the flip distance for triangulations of convex polygons. !! A more informal way of making the distinction is to say, quoting the Encyclopedia of Mathematics, that ""every node has a left child, a right child, neither, or both"" and to specify that these ""are all different"" binary trees. !! The everyday division of documents into chapters, sections, paragraphs, and so on is an analogous example with n-ary rather than binary trees. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! Binary trees labelled this way are used to implement binary search trees and binary heaps, and are used for efficient searching and sorting. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set containing the root."
computer language,"Programming languages are one kind of computer language, and are used in computer programming to implement algorithms. !! The software engineering community uses an architecture description language as a computer language to create a description of a software architecture. !! A general-purpose language is a computer language that is broadly applicable across application domains, and lacks specialized features for a particular domain."
general-purpose language,"A general-purpose language is a computer language that is broadly applicable across application domains, and lacks specialized features for a particular domain."
semi-structured model,"The semi-structured model is a database model where there is no separation between the data and the schema, and the amount of structure used depends on the purpose."
database model,"The most popular example of a database model is the relational model, which uses a table-based format. !! A relational database contains multiple tables, each similar to the one in the ""flat"" database model. !! Three key terms are used extensively in relational database models: relations, attributes, and domains. !! A database model is a type of data model that determines the logical structure of a database. !! The semi-structured model is a database model where there is no separation between the data and the schema, and the amount of structure used depends on the purpose. !! Hierarchical database modelIt is the oldest form of data base model. !! Database design is the organization of data according to a database model."
numerical method,"The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations). !! In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. !! The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm."
semantic analysis,"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. !! Thus, a newer alternative is probabilistic latent semantic analysis, based on a multinomial model, which is reported to give better results than standard LSA. !! ""Introduction to Latent Semantic Analysis"" (PDF). !! The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. !! The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches."
latent semantic analysis,"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. !! The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. !! ""Introduction to Latent Semantic Analysis"" (PDF)."
memory search,"The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search."
multinomial model,"Thus, a newer alternative is probabilistic latent semantic analysis, based on a multinomial model, which is reported to give better results than standard LSA."
user queries,"The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches."
pattern recognition,"It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. !! Pattern recognition systems are commonly trained from labeled ""training"" data. !! Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. !! Pattern recognition is the automated recognition of patterns and regularities in data. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. !! Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration."
structural pattern recognition,"Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features."
statistical pattern recognition,Syntactic pattern recognition can be used instead of statistical pattern recognition if there is clear structure in the patterns.
character sets,"Multiple coded character sets may share the same repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different code points. !! Many of the changes were subtle, such as collatable character sets within certain numeric ranges. !! Coded Character Sets, History and Development. !! The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)"
statistical modeling,"Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis."
least-squares support-vector machine,"Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis."
k nearest neighbors,"An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). !! This value is the average of the values of k nearest neighbors."
single nearest neighbor,"If k = 1, then the object is simply assigned to the class of that single nearest neighbor."
classification accuracy,"Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis."
distance metric,"Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis."
Object Oriented Programming,Object Oriented Programming puts the Nouns first and foremost. !! Object oriented programming with ANSI-C. Hanser.
object oriented programming,Object oriented programming with ANSI-C. Hanser.
supervised learning,"A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. !! Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). !! Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. !! A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. !! Transductive support-vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. !! In such situations, semi-supervised learning can be of great practical value. !! Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. !! Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. !! From the perspective of statistical learning theory, supervised learning is best understood. !! In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes. !! Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning. !! In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). !! Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. !! Some supervised learning algorithms require the user to determine certain control parameters. !! Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning."
3d core graphics system,The 3D Core Graphics System (a. k. a. !! The 3D Core Graphics System (or Core) was the first graphical standard to be developed.
logical calculi,"For more, see Other logical calculi below. !! With the tools of first-order logic it is possible to formulate a number of theories, either with explicit axioms or by rules of inference, that can themselves be treated as logical calculi."
relational database theory,"Sixth normal form (6NF) is a term in relational database theory, used in two different ways. !! Kent, W. (1983) A Simple Guide to Five Normal Forms in Relational Database Theory, Communications of the ACM, vol."
relational algebra,"Date and others have defined sixth normal form as a normal form, based on an extension of the relational algebra."
optimal solution,"More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. !! The equality in the max-flow min-cut theorem follows from the strong duality theorem in linear programming, which states that if the primal program has an optimal solution, x*, then the dual program also has an optimal solution, y*, such that the optimal values formed by the two solutions are equal. !! Stack search is not guaranteed to find the optimal solution to the search problem. !! In computer science, a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems."
data mining,"Decision tree learning or induction of decision trees is one of the predictive modelling approaches used in statistics, data mining and machine learning. !! In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length. !! Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e. g. behavior modeling, classification, data mining, regression, function approximation, or game strategy). !! Decision tree learning is a method commonly used in data mining. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Data mining in general and rule induction in detail are trying to create algorithms without human programming but with analyzing existing data structures. !! Data mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. !! Agent mining is an interdisciplinary area that synergizes multiagent systems with data mining and machine learning. !! Data science is related to data mining, machine learning and big data. !! Data stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery. !! Frequent pattern discovery (or FP discovery, FP mining, or Frequent itemset mining) is part of knowledge discovery in databases, Massive Online Analysis, and data mining; it describes the task of finding the most frequent and relevant patterns in large datasets."
string kernel,"String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be. !! In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length. !! (i. e. data are elements of a vector space), using a string kernel allows the extension of these methods to handle sequence data. !! String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis. !! Using string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors."
kernel function,"In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length."
string kernels,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis. !! String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be."
kernelized learning algorithms,"Using string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors."
text mining,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis."
gene analysis,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis."
sequence data,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis."
classification algorithms,"Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms have been developed. !! More recently, receiver operating characteristic (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms."
data sets,"Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms have been developed. !! In statistics, combinatorial data analysis (CDA) is the study of data sets where the order in which objects are arranged is important. !! Phase correlation is an approach to estimate the relative translative offset between two similar images (digital image correlation) or other data sets."
semantic resolution tree,A semantic resolution tree is a tree used for the definition of the semantics of a programming language.
software design,"One of the main components of software design is the software requirements analysis (SRA). !! A software design description (a. k. a. software design document or SDD; just design document; also Software Design Specification) is a representation of a software design that is to be used for recording design information, addressing various design concerns, and communicating that information to the designs stakeholders. !! In computer programming and software design, code refactoring is the process of restructuring existing computer codechanging the factoringwithout changing its external behavior. !! The hexagonal architecture, or ports and adapters architecture, is an architectural pattern used in software design. !! The principle of least astonishment (POLA), aka principle of least surprise (alternatively a law or rule), applies to user interface and software design. !! In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. !! Software design may refer to either ""all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems"" or ""the activity following requirements specification and before programming, as . !! Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. !! Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints. !! In order to account for the unanticipated gaps in the software design, during software construction some design modifications must be made on a smaller or larger scale to flesh out details of the software design. !! ""Software design usually involves problem-solving and planning a software solution. !! Software design is the process of envisioning and defining software solutions to one or more sets of problems."
user interface,"The benefit of low-key feedback is that it can provide always available indication without cluttering the user interface with explicit indicators such as text labels or indicator lights. !! In the industrial design field of humancomputer interaction, a user interface (UI) is the space where interactions between humans and machines occur. !! The Office Assistant is a discontinued ""intelligent"" user interface for Microsoft Office that assisted users by way of an interactive animated character which interfaced with the Office help content. !! User interfaces are composed of one or more layers, including a human-machine interface (HMI) that interfaces machines with physical input hardware such as keyboards, mice, or game pads, and output hardware such as computer monitors, speakers, and printers. !! Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc. !! The principle of least astonishment (POLA), aka principle of least surprise (alternatively a law or rule), applies to user interface and software design. !! The design considerations applicable when creating user interfaces are related to, or involve such disciplines as, ergonomics and psychology. !! Generally, the goal of user interface design is to produce a user interface that makes it easy, efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result (i. e. maximum usability). !! Don Norman cited principles of cognitive engineering in his 1981 article, ""The truth about Unix: The user interface is horrid. "" !! In humancomputer interaction, an organic user interface (OUI) is defined as a user interface with a non-flat display. !! The hexagonal architecture divides a system into several loosely-coupled interchangeable components, such as the application core, the database, the user interface, test scripts and interfaces with other systems. !! Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls."
operating systems,"Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers. !! ""Bad command or file name"" is a common and ambiguous error message in MS-DOS and some other operating systems. !! Security-focused operating systems also exist. !! Other specialized classes of operating systems (special-purpose operating systems), such as embedded and real-time systems, exist for many applications. !! Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources. !! The cylinder count of 306 is not conveniently close to any power of 1024; operating systems and programs using the customary binary prefixes show this as 9. !! 2 percent, while other operating systems amount to just 0."
optimization theory,"Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). !! In computer science and optimization theory, the max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut, i. e. , the smallest total weight of the edges which if removed would disconnect the source from the sink. !! Robust optimization is a field of optimization theory that deals with optimization problems in which a certain measure of robustness is sought against uncertainty that can be represented as deterministic variability in the value of the parameters of the problem itself and/or its solution. !! In optimization theory, semi-infinite programming (SIP) is an optimization problem with a finite number of variables and an infinite number of constraints, or an infinite number of variables and a finite number of constraints."
max-flow min-cut theorem,"In computer science and optimization theory, the max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut, i. e. , the smallest total weight of the edges which if removed would disconnect the source from the sink. !! The equality in the max-flow min-cut theorem follows from the strong duality theorem in linear programming, which states that if the primal program has an optimal solution, x*, then the dual program also has an optimal solution, y*, such that the optimal values formed by the two solutions are equal. !! Max-flow min-cut theorem. !! The other half of the max-flow min-cut theorem refers to a different aspect of a network: the collection of cuts. !! In this new definition, the generalized max-flow min-cut theorem states that the maximum value of an s-t flow is equal to the minimum capacity of an s-t cut in the new sense."
linear programming,"In mathematical optimization, the criss-cross algorithm is any of a family of algorithms for linear programming. !! Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. !! Linear programming can be applied to various fields of study. !! More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. !! A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists. !! Linear programming is a special case of mathematical programming (also known as mathematical optimization). !! Like the simplex algorithm of George B. Dantzig, the criss-cross algorithm is not a polynomial-time algorithm for linear programming. !! The equality in the max-flow min-cut theorem follows from the strong duality theorem in linear programming, which states that if the primal program has an optimal solution, x*, then the dual program also has an optimal solution, y*, such that the optimal values formed by the two solutions are equal."
graph algorithms,"In the study of graph algorithms, Courcelle's theorem is the statement that every graph property definable in the monadic second-order logic of graphs can be decided in linear time on graphs of bounded treewidth."
vector optimization,"A multi-objective optimization problem is a special case of a vector optimization problem: The objective space is the finite dimensional Euclidean space partially ordered by the component-wise ""less than or equal to"" ordering. !! Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously. !! Benson's algorithm for linear vector optimization problems. !! Thus the minimizer of this vector optimization problem are the Pareto efficient points. !! Vector optimization is a subarea of mathematical optimization where optimization problems with a vector-valued objective functions are optimized with respect to a given partial ordering and subject to certain constraints."
data warehouse,"The process of dimensional modeling builds on a 4-step design method that helps to ensure the usability of the dimensional model and the use of the data warehouse. !! The earliest installations using anchor modeling were made in Sweden with the first dating back to 2004, when a data warehouse for an insurance company was built using the technique."
cross-correlation matrix,The cross-correlation matrix is used in various digital signal processing algorithms. !! The cross-correlation matrix of two random vectors is a matrix containing as elements the cross-correlations of all pairs of elements of the random vectors.
correlation matrix,The cross-correlation matrix is used in various digital signal processing algorithms. !! The cross-correlation matrix of two random vectors is a matrix containing as elements the cross-correlations of all pairs of elements of the random vectors.
random vectors,"In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! The cross-correlation matrix of two random vectors is a matrix containing as elements the cross-correlations of all pairs of elements of the random vectors."
binary splitting,"In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. !! To take full advantage of the scheme, fast multiplication algorithms such as ToomCook and SchnhageStrassen must be used; with ordinary O(n2) multiplication, binary splitting may render no speedup at all or be slower. !! Additionally, whereas the most naive evaluation scheme for a rational series uses a full-precision division for each term in the series, binary splitting requires only one final division at the target precision; this is not only faster, but conveniently eliminates rounding errors. !! Binary splitting requires more memory than direct term-by-term summation, but is asymptotically faster since the sizes of all occurring subproducts are reduced. !! Since all subdivisions of the series can be computed independently of each other, binary splitting lends well to parallelization and checkpointing."
optimization mechanism,"determines the type of the network built by the optimization mechanism. !! In network science, the optimization mechanism is a network growth algorithm, which randomly places new nodes in the system, and connects them to the existing nodes based on a cost-benefit analysis. !! Optimization mechanism is thought to be the underlying mechanism in several real networks, such as transportation networks, power grid, router networks, the network of highways, etc. !! Depending on the parameters used in the optimization mechanism, the algorithm can build three types of networks: a star network, a random network, and a scale-free network. !! The optimization mechanism is a model with growth, in which preferential attachment is valid under certain assumptions."
star network,"Depending on the parameters used in the optimization mechanism, the algorithm can build three types of networks: a star network, a random network, and a scale-free network."
router networks,"Optimization mechanism is thought to be the underlying mechanism in several real networks, such as transportation networks, power grid, router networks, the network of highways, etc."
probabilistic tests,Many popular primality tests are probabilistic tests.
sample space,"These tests use, apart from the tested number n, some other numbers a which are chosen at random from some sample space; the usual randomized primality tests never report a prime number as composite, but it is possible for a composite number to be reported as prime."
unsupervised learning,"Unsupervised learning is a type of algorithm that learns patterns from untagged data. !! Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). !! The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. !! Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. !! Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e. g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). !! For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups. !! In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. !! Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods. !! Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. !! Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning."
neural networks,"The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. !! Subsymbolic artificial intelligence is the set of alternative approaches which do not use explicit high level symbols, such as mathematical optimization, statistical classifiers and neural networks. !! It's been generally proven that using methods based on neural networks, vector support machines, statistics, and the nearest neighbors are a great way to do this traffic classification, but in some specific cases some methods are better than others, for example: neural networks work better when the whole observation set is taken into account. !! Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods. !! The Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers of perceptrons, and is implemented as a feedforward neural network. !! Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! Competitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as competitive layer. !! Some researchers have achieved ""near-human performance"" on the MNIST database, using a committee of neural networks; in the same paper, the authors achieve performance double that of humans on other recognition tasks."
probabilistic methods,Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods.
maximum likelihood,"In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. !! is called the maximum likelihood estimate. !! so defined is measurable, then it is called the maximum likelihood estimator. !! In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. !! The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. !! The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate."
supervised method,"In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
boltzmann learning rule,"In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
hopfield learning rule,"In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
gibbs sampling,"But the reversible-jump variant is useful when doing Markov chain Monte Carlo or Gibbs sampling over nonparametric Bayesian models such as those involving the Dirichlet process or Chinese restaurant process, where the number of mixing components/clusters/etc. !! In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
bayesian network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! Efficient algorithms can perform inference and learning in Bayesian networks. !! For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. !! Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. !! Bayesian networks that model sequences of variables (e. g. speech signals or protein sequences) are called dynamic Bayesian networks. !! This type of graphical model is known as a directed graphical model, Bayesian network, or belief network. !! Probabilistic relational model a Probabilistic Relational Model (PRM) is the counterpart of a Bayesian network in statistical relational learning."
probabilistic relational model,Probabilistic relational model a Probabilistic Relational Model (PRM) is the counterpart of a Bayesian network in statistical relational learning.
hardware technologies,Software and hardware technologies are used for human presence detection.
sql statements,A database dump (also: SQL dump) contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements.
table structure,A database dump (also: SQL dump) contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements.
sql dump,A database dump (also: SQL dump) contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements.
database dumps,"Database dumps are often published by free content projects, to allow reuse or forking, as well as local searching of the database using tools such as grep."
optimization problems,"Robust optimization is a field of optimization theory that deals with optimization problems in which a certain measure of robustness is sought against uncertainty that can be represented as deterministic variability in the value of the parameters of the problem itself and/or its solution. !! Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). !! In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. !! In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids and give constant-factor approximations to optimization problems with the submodular structure. !! Vector optimization is a subarea of mathematical optimization where optimization problems with a vector-valued objective functions are optimized with respect to a given partial ordering and subject to certain constraints."
multimedia communication,Universal communication format is a communication protocol developed by the IEEE for multimedia communication.
flux architecture,"To support React's concept of unidirectional data flow (which might be contrasted with AngularJS's bidirectional flow), the Flux architecture was developed as an alternative to the popular modelviewcontroller architecture."
hebbian learning,"A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network."
hidden layer,Competitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as competitive layer.
database columns,A database relation (e. g. a database table) is said to meet third normal form standards if all the attributes (e. g. database columns) are functionally dependent on solely the primary key.
database relation,A database relation (e. g. a database table) is said to meet third normal form standards if all the attributes (e. g. database columns) are functionally dependent on solely the primary key.
database table,A database relation (e. g. a database table) is said to meet third normal form standards if all the attributes (e. g. database columns) are functionally dependent on solely the primary key. !! A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.
input error,"The negative outcome of such a design is that a doctor's number will be duplicated in the database if they have multiple patients, thus increasing both the chance of input error and the cost and risk of updating that number should it change (compared to a third normal form-compliant data model that only stores a doctor's number once on a doctor table)."
database normalization,"Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. !! An Introduction to Database Normalization by Mike Hillyer. !! The third normal form (3NF) is a normal form used in database normalization. !! Database Normalization Basics by Mike Chapple (About. !! The process is progressive, and a higher level of database normalization cannot be achieved unless the previous levels have been satisfied."
supervised machine learning,Similarity learning is an area of supervised machine learning in artificial intelligence.
similarity learning,"For further information on this topic, see the surveys on metric and similarity learning by Bellet et al. !! For this reason, ranking-based similarity learning is easier to apply in real large-scale applications. !! Similarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems. !! Similarity learning is an area of supervised machine learning in artificial intelligence. !! Similarity learning is closely related to distance metric learning."
distance metric learning,Similarity learning is closely related to distance metric learning.
recommendation systems,"Similarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems."
multivariate analysis,"Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. !! Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis."
linear algebra,"In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. !! In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm for matrix multiplication. !! In linear algebra, the Schmidt decomposition (named after its originator Erhard Schmidt) refers to a particular way of expressing a vector in the tensor product of two inner product spaces. !! In mathematics, particularly in linear algebra and applications, matrix analysis is the study of matrices and their algebraic properties. !! In linear algebra, a Householder transformation (also known as a Householder reflection or elementary reflector) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. !! :911 The stochastic matrix was first developed by Andrey Markov at the beginning of the 20th century, and has found use throughout a wide variety of scientific fields, including probability theory, statistics, mathematical finance and linear algebra, as well as computer science and population genetics. !! In linear algebra, a QR decomposition, also known as a QR factorization or QU factorization, is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. QR decomposition is often used to solve the linear least squares problem and is the basis for a particular eigenvalue algorithm, the QR algorithm. !! M. T. Chu, R. E. Funderlic, R. J. Plemmons, Structured low-rank approximation, Linear Algebra and its Applications, Volume 366, 1 June 2003, Pages 157172 !! In mathematics, particularly in linear algebra, matrix multiplication is a binary operation that produces a matrix from two matrices. !! Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering. !! Historically, matrix multiplication has been introduced for facilitating and clarifying computations in linear algebra. !! In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. !! Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. !! In linear algebra, the Cholesky decomposition or Cholesky factorization (pronounced sh-LES-kee) is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful for efficient numerical solutions, e. g. , Monte Carlo simulations. !! In linear algebra, a circulant matrix is a square matrix in which all row vectors are composed of the same elements and each row vector is rotated one element to the right relative to the preceding row vector."
non-negative matrix factorization,"There are different types of non-negative matrix factorizations. !! Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the 1990s under the name positive matrix factorization. !! In chemometrics non-negative matrix factorization has a long history under the name ""self modeling curve resolution"". !! In Learning the parts of objects by non-negative matrix factorization Lee and Seung proposed NMF mainly for parts-based decomposition of images. !! Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements."
linear system,"The inverse iteration algorithm requires solving a linear system or calculation of the inverse matrix. !! The Kaczmarz method is applicable to any linear system of equations, but its computational advantage relative to other methods depends on the system being sparse. !! In numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the GaussSeidel method for solving a linear system of equations, resulting in faster convergence. !! In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. !! Stationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a ""correction equation"" for which this process is repeated."
inverse matrix,The inverse iteration algorithm requires solving a linear system or calculation of the inverse matrix.
artificial neurons,"A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes."
computer security,"In cryptanalysis and computer security, a dictionary attack is an attack using a restricted subset of a keyspace to defeat a cipher or authentication mechanism by trying to determine its decryption key or passphrase, sometimes trying thousands or millions of likely possibilities often obtained from lists of past security breaches. !! A DMA attack is a type of side channel attack in computer security, in which an attacker can penetrate a computer or other device, by exploiting the presence of high-speed expansion ports that permit direct memory access (DMA). !! In computer security, a sandbox is a security mechanism for separating running programs, usually in an effort to mitigate system failures and/or software vulnerabilities from spreading. !! In computer security, digital certificates are verified using a chain of trust. !! In computer security, a chain of trust is established by validating each component of hardware and software from the end entity up to the root certificate."
authentication mechanism,"In cryptanalysis and computer security, a dictionary attack is an attack using a restricted subset of a keyspace to defeat a cipher or authentication mechanism by trying to determine its decryption key or passphrase, sometimes trying thousands or millions of likely possibilities often obtained from lists of past security breaches. !! The efficacy of code signing as an authentication mechanism for software depends on the security of underpinning signing keys."
binary operation,"In mathematics, the Frobenius inner product is a binary operation that takes two matrices and returns a number. !! In mathematics, particularly in linear algebra, matrix multiplication is a binary operation that produces a matrix from two matrices."
tensor product,"Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product !! In linear algebra, the Schmidt decomposition (named after its originator Erhard Schmidt) refers to a particular way of expressing a vector in the tensor product of two inner product spaces."
complex vector spaces,Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product
vector spaces,Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product
hilbert spaces,Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product
parent class,"In class-based programming, inheritance is done by defining new classes as extensions of existing classes: the existing class is the parent class and the new class is the child class. !! Composition over inheritance (or composite reuse principle) in object-oriented programming (OOP) is the principle that classes should achieve polymorphic behavior and code reuse by their composition (by containing instances of other classes that implement the desired functionality) rather than inheritance from a base or parent class."
compiler design,"In compiler design, static single assignment form (often abbreviated as SSA form or simply SSA) is a property of an intermediate representation (IR), which requires that each variable be assigned exactly once, and every variable be defined before it is used."
recursive relation,"The master theorem allows many recurrence relations of this form to be converted to -notation directly, without doing an expansion of the recursive relation."
regularization methods,"Linear Regularization Methods""."
linear regularization methods,"Linear Regularization Methods""."
register allocation,"Register allocation consists therefore in choosing where to store the variables at runtime, i. e. inside or outside registers. !! Register allocation can happen over a basic block (local register allocation), over a whole function/procedure (global register allocation), or across function boundaries traversed via call-graph (interprocedural register allocation). !! In compiler optimization, register allocation is the process of assigning local automatic variables and expression results to a limited number of processor registers. !! Register allocation raises several problems that can be tackled (or avoided) by different register allocation approaches. !! Many register allocation approaches optimize for one or more specific categories of actions."
computational system,"The computational theory of mind holds that the mind is a computational system that is realized (i. e. physically implemented) by neural activity in the brain. !! When used in this manner, the counter machine is used to model the discrete time-steps of a computational system in relation to memory accesses. !! The SKI combinator calculus is a combinatory logic system and a computational system."
formal knowledge representation languages,Description logics (DL) are a family of formal knowledge representation languages.
description logics,"There are many varieties of description logics and there is an informal naming convention, roughly describing the operators allowed. !! Description logics (DL) are a family of formal knowledge representation languages."
mathematical constructors,"There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors."
fuzzy description logics,"There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors."
randomization function,"So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external ""random"" data such as the program's startup time. !! In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm. !! In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed. !! The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time)."
randomizing function,"In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm."
randomized algorithm,"In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator. !! In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm. !! A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. !! Computational complexity theory models randomized algorithms as probabilistic Turing machines. !! Randomized algorithms are particularly useful when faced with a malicious ""adversary"" or attacker who deliberately tries to feed a bad input to the algorithm (see worst-case complexity and competitive analysis (online algorithm)) such as in the Prisoner's dilemma. !! The most basic randomized complexity class is RP, which is the class of decision problems for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2."
binary dependent variable,"Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! In addition, linear regression may make nonsensical predictions for a binary dependent variable."
logistic function,"Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! In some fields, most notably in the context of artificial neural networks, the term ""sigmoid function"" is used as an alias for the logistic function."
logistic regression,"In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). !! Outputs with more than two values are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model). !! The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! In a binary logistic regression model, the dependent variable has two levels (categorical)."
logistic model,"In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). !! Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. !! In the logistic model, the log-odds (the logarithm of the odds) for the value labeled ""1"" is a linear combination of one or more independent variables (""predictors""); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). !! Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled ""0"" and ""1"". !! In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc."
independent variable,"Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio."
learning automaton,"A learning automaton is an adaptive decision-making unit situated in a random environment that learns the optimal action through repeated interactions with its environment. !! A learning automaton is one type of machine learning algorithm studied since 1970s. !! However, the term learning automaton was not used until Narendra and Thathachar introduced it in a survey paper in 1974. !! A visualised demo / Art Work of a single Learning Automaton had been developed by Systems (microSystems) Research Group at Newcastle University."
autonomic networking,"Generic Autonomic Networking Architecture (GANA) EFIPSANS Project http://www. !! Instead of a layering approach, autonomic networking targets a more flexible structure termed compartmentalization. !! Autonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. !! Autonomic networking at the core of enterprise Wan governance blog !! A fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking."
autonomic computing,"Autonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001."
control theory,"Many linear dynamical system tests in control theory, especially those related to controllability and observability, involve checking the rank of the Krylov subspace. !! Fuzzy logic has been applied to many fields, from control theory to artificial intelligence. !! Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. !! A fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking."
computer architecture,"In computer science, a tagged architecture is a particular type of computer architecture where every word of memory constitutes a tagged union, being divided into a number of bits of data, and a tag section that describes the type of the data: how it is to be interpreted, and, if it is a reference, the type of the object that it points to. !! In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation. !! In computer architecture, Amdahl's law (or Amdahl's argument) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved. !! In computer architecture, the memory hierarchy separates computer storage into a hierarchy based on response time. !! Dataflow architecture is a computer architecture that directly contrasts the traditional von Neumann architecture or control flow architecture. !! In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. !! The earliest computer architectures were designed on paper and then directly built into the final hardware form. !! Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints. !! In computer architecture, cache coherence is the uniformity of shared resource data that ends up stored in multiple local caches. !! The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine."
data structures,"Algorithmic information theory principally studies complexity measures on strings (or other data structures). !! The term compressed data structure arises in the computer science subfields of algorithms, data structures, and theoretical computer science. !! Data structures serve as the basis for abstract data types (ADT). !! The predecessor problem is a simple case of the nearest neighbor problem, and data structures that solve it have applications in problems like integer sorting. !! The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra. !! When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. !! The range searching problem and the data structures that solve it are a fundamental topic of computational geometry. !! Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. !! Usually, efficient data structures are key to designing efficient algorithms. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Algorithms and data structures are central to computer science. !! Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. !! Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of keyvalue pairs and looking up the value associated with a given key. !! WADS, the Algorithms and Data Structures Symposium, is an international academic conference in the field of computer science, focusing on algorithms and data structures. !! The term ""generic programming"" was originally coined by David Musser and Alexander Stepanov in a more specific sense than the above, to describe a programming paradigm whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above. !! Forward declaration is used in languages that require declaration before use; it is necessary for mutual recursion in such languages, as it is impossible to define such functions (or data structures) without a forward reference in one definition: one of the functions (respectively, data structures) must be defined first."
objective function,"An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc. !! More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. !! Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. !! Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e. g. differentiable or subdifferentiable)."
quadratic function,"More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution."
conformational analysis,Nonlinear optimization methods are widely used in conformational analysis.
statistical mechanics,"Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! In statistical mechanics, the two-dimensional square lattice Ising model is a simple lattice model of interacting magnetic spins. !! In physics, maximum entropy thermodynamics (colloquially, MaxEnt thermodynamics) views equilibrium thermodynamics and statistical mechanics as inference processes. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."
physical memory,"Address Windowing Extensions (AWE) is a Microsoft Windows application programming interface that allows a 32-bit software application to access more physical memory than it has virtual address space, even in excess of the 4 GB limit."
virtual address space,"Address Windowing Extensions (AWE) is a Microsoft Windows application programming interface that allows a 32-bit software application to access more physical memory than it has virtual address space, even in excess of the 4 GB limit."
microsoft windows application programming interface,"Address Windowing Extensions (AWE) is a Microsoft Windows application programming interface that allows a 32-bit software application to access more physical memory than it has virtual address space, even in excess of the 4 GB limit."
encryption keys,"An article published in Dr. Dobb's Journal in 2004 noted that memory allocated using Address Windowing Extensions will not be written to the pagefile, and suggested that AWE regions could therefore be used as a way of protecting sensitive application data such as encryption keys."
multimodal architecture,"The Multimodal Architecture and Interfaces recommendation introduces a generic structure and a communication protocol to allow the modules in a multimodal system to communicate with each other. !! Multimodal Architecture and Interfaces is an open standard developed by the World Wide Web Consortium since 2005. !! Papers presented to W3C's Multimodal Architecture and Interfaces Workshop, 1920 July 2004. !! The Multimodal Architecture and Interfaces specification is based on the MVC design pattern, that proposes to organize the user interface structure in three parts: the Model, the View and the Controller. !! Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need."
multimodal system,The Multimodal Architecture and Interfaces recommendation introduces a generic structure and a communication protocol to allow the modules in a multimodal system to communicate with each other. !! Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need.
runtime framework,Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need.
main functions,Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need.
user interface structure,"The Multimodal Architecture and Interfaces specification is based on the MVC design pattern, that proposes to organize the user interface structure in three parts: the Model, the View and the Controller."
mvc design pattern,"The Multimodal Architecture and Interfaces specification is based on the MVC design pattern, that proposes to organize the user interface structure in three parts: the Model, the View and the Controller."
orthogonal decomposition,"The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations). !! Weiss, Julien: A Tutorial on the Proper Orthogonal Decomposition. !! Applications of the Proper Orthogonal Decomposition Method http://www. !! As its name hints, it's operating an Orthogonal Decomposition along with the Principal Components of the field."
computational fluid dynamics,"The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations). !! In some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e. g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used."
structural analysis,"Structural analysis is thus a key part of the engineering design of structures. !! Advanced structural analysis may examine dynamic response, stability and non-linear behavior. !! Structural analysis is the determination of the effects of loads on physical structures and their components. !! Structural analysis employs the fields of applied mechanics, materials science and applied mathematics to compute a structure's deformations, internal forces, stresses, support reactions, accelerations, and stability. !! It is common practice to use approximate solutions of differential equations as the basis for structural analysis. !! The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations)."
scalar processor,"The Cortex-M7, like many consumer CPUs today, is a superscalar processor. !! A scalar processor is classified as a single instruction, single data (SISD) processor in Flynn's taxonomy. !! Scalar processors are a class of computer processors that process only one data item at a time. !! A superscalar processor (such as the Intel P5) may execute more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to redundant functional units on the processor. !! The Intel 486 is an example of a scalar processor."
scalar processors,Scalar processors are a class of computer processors that process only one data item at a time.
computer processors,Scalar processors are a class of computer processors that process only one data item at a time. !! In computer processors the carry flag (usually indicated as the C flag) is a single bit in a system status register/flag register used to indicate when an arithmetic carry or borrow has been generated out of the most significant arithmetic logic unit (ALU) bit position.
superscalar processor,"A superscalar processor (such as the Intel P5) may execute more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to redundant functional units on the processor. !! The Cortex-M7, like many consumer CPUs today, is a superscalar processor."
clock cycle,A superscalar processor (such as the Intel P5) may execute more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to redundant functional units on the processor.
singleton pattern,"In software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to one ""single"" instance."
Singleton pattern,"In essence, the singleton pattern forces it to be responsible for ensuring that it is only instantiated once. !! In software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to one ""single"" instance. !! How to navigate the deceptively simple Singleton pattern."
source programs,Semantic dictionary encoding (SDE) preserves the full semantic context of source programs while adding further information that can be used for accelerating the speed of code generation.
linear function,Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
multilevel sampling system,Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
adaptive predictive coding,Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
telecommunications network,Active networking is a communication pattern that allows packets flowing through a telecommunications network to dynamically modify the operation of the network.
active networking,"Active networking is a communication pattern that allows packets flowing through a telecommunications network to dynamically modify the operation of the network. !! Active networking relates to other networking paradigms primarily based upon how computing and communication are partitioned in the architecture. !! The use of real-time genetic algorithms within the network to compose network services is also enabled by active networking. !! Active networking allows the possibility of highly tailored and rapid ""real-time"" changes to the underlying network operation. !! Network processors are one means of implementing active networking concepts."
network processors,Network processors are one means of implementing active networking concepts.
medical imaging,"This makes computational electromagnetics (CEM) important to the design, and modeling of antenna, radar, satellite and other communication systems, nanophotonic devices and high speed silicon electronics, medical imaging, cell-phone antenna design, among other applications. !! Medical imaging seeks to reveal internal structures hidden by the skin and bones, as well as to diagnose and treat disease. !! Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. !! Medical imaging also establishes a database of normal anatomy and physiology to make it possible to identify abnormalities. !! Lossless JPEG has some popularity in medical imaging, and is used in DNG and some digital cameras to compress raw images, but otherwise was never widely adopted. !! Although imaging of removed organs and tissues can be performed for medical reasons, such procedures are usually considered part of pathology instead of medical imaging. !! Medical imaging is the technique and process of imaging the interior of a body for clinical analysis and medical intervention, as well as visual representation of the function of some organs or tissues (physiology). !! In a limited comparison, these technologies can be considered forms of medical imaging in another discipline."
interpolative decomposition,"In numerical analysis, interpolative decomposition (ID) factors a matrix as the product of two matrices, one of which contains selected columns from the original matrix, and the other of which has a subset of columns consisting of the identity matrix and all its values are no greater than 2 in absolute value."
identity matrix,"In numerical analysis, interpolative decomposition (ID) factors a matrix as the product of two matrices, one of which contains selected columns from the original matrix, and the other of which has a subset of columns consisting of the identity matrix and all its values are no greater than 2 in absolute value."
multimedia data,"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc."
data stream clustering,"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. !! Data stream clustering has recently attracted attention for emerging applications that involve large amounts of streaming data. !! Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time."
streaming algorithm,"In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). !! There has since been a large body of work centered around data streaming algorithms that spans a diverse spectrum of computer science fields such as theory, databases, networking, and natural language processing. !! In computing, a one-pass algorithm or single-pass algorithm is a streaming algorithm which reads its input exactly once. !! Lall, Ashwin; Sekar, Vyas; Ogihara, Mitsunori; Xu, Jun; Zhang, Hui (2006), ""Data streaming algorithms for estimating entropy of network traffic"", Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS 2006) (PDF), p. 145, !! Though streaming algorithms had already been studied by Munro and Paterson as early as 1978, as well as Philippe Flajolet and G. Nigel Martin in 1982/83, the field of streaming algorithms was first formalized and popularized in a 1996 paper by Noga Alon, Yossi Matias, and Mario Szegedy. !! Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time. !! For this paper, the authors later won the Gdel Prize in 2005 ""for their foundational contribution to streaming algorithms. """
streaming data,Data stream clustering has recently attracted attention for emerging applications that involve large amounts of streaming data.
processor time,"Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources."
cost allocation,"Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources."
web servers,"Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers. !! Key aspects of service-oriented infrastructure include industrialisation and virtualisation, providing IT infrastructure services via a pool of resources (web servers, application servers, database servers, servers, storage instances) instead of through discrete instances."
binary search trees,"Binary search trees are also efficacious in sorting algorithms and search algorithms. !! Binary search trees are used in implementing priority queues, using the element or node's key as priorities. !! The geometry of binary search trees has been used to provide an algorithm which is dynamically optimal if any binary search tree algorithm is dynamically optimal."
binary search tree algorithm,"The geometry of binary search trees has been used to provide an algorithm which is dynamically optimal if any binary search tree algorithm is dynamically optimal. !! The binary search tree algorithm was discovered independently by several researchers, including P. F. Windley, Andrew Donald Booth, Andrew Colin, Thomas N. Hibbard, and attributed to Conway Berners-Lee and David Wheeler, in 1960 for storing labeled data in magnetic tapes."
data modeling,"Data modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations. !! Data modeling defines not just data elements, but also their structures and the relationships between them. !! Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. !! The data-flow diagram is a tool that is part of structured analysis and data modeling. !! The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details."
information system,"Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. !! A data-flow diagram is a way of representing a flow of data through a process or a system (usually an information system)."
data model,"A hierarchical database model is a data model in which the data are organized into a tree-like structure. !! Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! A database model is a type of data model that determines the logical structure of a database. !! A data model (or datamodel) is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. !! A data model explicitly determines the structure of data. !! The term data model can refer to two distinct but closely related concepts. !! So the ""data model"" of a banking application may be defined using the entity-relationship ""data model"". !! For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner."
logical data model,"The logical data model of vector graphics is based on the mathematics of coordinate geometry, in which shapes are defined as a set of points in a two- or three-dimensional cartesian coordinate system, as p = (x, y) or p = (x, y, z). !! The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details."
data elements,"Data Structure Diagram is a diagram type that is used to depict the structure of data elements in the data dictionary. !! Data modeling defines not just data elements, but also their structures and the relationships between them."
graph embedding,"This article deals only with the strict definition of graph embedding. !! Some authors define a weaker version of the definition of ""graph embedding"" by omitting the non-intersection condition for edges. !! In such contexts the stricter definition is described as ""non-crossing graph embedding""."
application programming interfaces,"Software frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system. !! :815Both types of automated personal assistant technology are enabled by the combination of mobile computing devices, application programming interfaces (APIs), and the proliferation of mobile apps."
code libraries,"Software frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system."
software frameworks,"The designers of software frameworks aim to facilitate software developments by allowing designers and programmers to devote their time to meeting software requirements rather than dealing with the more standard low-level details of providing a working system, thereby reducing overall development time. !! According to Pree, software frameworks consist of frozen spots and hot spots. !! Software frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system. !! The elegance issue is why relatively few software frameworks have stood the test of time: the best frameworks have been able to evolve gracefully as the underlying technology on which they were built advanced. !! Software frameworks rely on the Hollywood Principle: ""Don't call us, we'll call you. """
mutable array,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
resizable array,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
dynamic table,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
random access,"Elements in a sorted array can be looked up by their index (random access) at O(1) time, an operation taking O(log n) or O(n) time for more complex data structures. !! In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
array list,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
dynamic array,"The elements of the dynamic array are stored contiguously at the start of the underlying array, and the remaining positions towards the end of the underlying array are reserved, or unused. !! In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed. !! Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation. !! A simple dynamic array can be constructed by allocating an array of fixed-size, typically larger than the number of elements immediately required. !! A dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end."
static arrays,"Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation."
back end,"A dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end."
dynamically allocated array,"A dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end."
swarm intelligence algorithms,This is a chronologically ordered list of metaphor-based metaheuristics and swarm intelligence algorithms. !! Recent work has involved merging the global search properties of SDS with other swarm intelligence algorithms.
bcjr algorithm,"The BCJR algorithm is an algorithm for maximum a posteriori decoding of error correcting codes defined on trellises (principally convolutional codes). !! The online textbook: Information Theory, Inference, and Learning Algorithms, by David J. C. MacKay, discusses the BCJR algorithm in chapter 25. !! Susa framework implements BCJR algorithm for forward error correction codes and channel equalization in C++. !! The implementation of BCJR algorithm in Susa signal processing framework"
recurrent neural network,"Recurrent neural networks were based on David Rumelhart's work in 1986. !! The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. !! A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. !! Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. !! The echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). !! Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. !! The Echo State Network (ESN) belongs to the Recurrent Neural Network (RNN) family and provide their architecture and supervised learning principle."
numpy bindings,"Some publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network, (iii) ReservoirComputing."
matlab code,"Some publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network, (iii) ReservoirComputing."
autonomous operation,"Another feature of the ESN is the autonomous operation in prediction: if the Echo State Network is trained with an input that is a backshifted version of the output, then it can be used for signal generation/prediction by using the previous output as input."
signal generation,"Another feature of the ESN is the autonomous operation in prediction: if the Echo State Network is trained with an input that is a backshifted version of the output, then it can be used for signal generation/prediction by using the previous output as input."
recurrent neural networks,"Recurrent neural networks were based on David Rumelhart's work in 1986. !! Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. !! ""Long Short-Term Memory in Recurrent Neural Networks"" (PDF). !! Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. !! Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. !! Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014."
fully recurrent neural networks,Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.
gated recurrent units,"Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. !! Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014."
discrete time recurrent neural networks,"Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations."
differential equations,"Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. !! Bifurcation theory is the mathematical study of changes in the qualitative or topological structure of a given family of curves, such as the integral curves of a family of vector fields, and the solutions of a family of differential equations. !! In continuous simulation, continuously changing state variables of a system are modeled by differential equations. !! Continuous Simulation refers to simulation approaches where a system is modeled with the help of variables that change continuously according to a set of differential equations. !! The approximation of derivatives by finite differences plays a central role in finite difference methods for the numerical solution of differential equations, especially boundary value problems. !! It is common practice to use approximate solutions of differential equations as the basis for structural analysis. !! In both differential equations in continuous time and difference equations in discrete time, initial conditions affect the value of the dynamic variables (state variables) at any future time."
shannon sampling theorem,"Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations."
leaf node,"The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. !! Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes. !! Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only to visit the root last (i. e. , they first access the children of the root, but only access the value of the root last). !! of the latter case form the relation (L) (<H) which is a partial map that assigns each non-leaf node its first child node. !! Similarly, (L+) (>H) assigns each non-leaf node with finitely many children its last child node."
external node,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
terminal node,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
outer node,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
child nodes,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
leaf nodes,"The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. !! Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only to visit the root last (i. e. , they first access the children of the root, but only access the value of the root last)."
root node,"The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. !! Possibly the easiest operation for the randomized meldable heap, FindMin() simply returns the element currently stored in the heap's root node. !! In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. The node at the ""top"" of the heap (with no parents) is called the root node."
cognitive architecture,"The research on cognitive architectures as software instantiation of cognitive theories was initiated by Allen Newell in 1990. !! He included more aspects of his research on long-term memory and thinking processes into this research and eventually designed a cognitive architecture he eventually called ACT. !! The Institute for Creative Technologies defines cognitive architecture as: ""hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together in conjunction with knowledge and skills embodied within the architecture to yield intelligent behavior in a diversity of complex environments. "" !! Successful cognitive architectures include ACT-R (Adaptive Control of Thought - Rational) and SOAR. !! A cognitive architecture refers to both a theory about the structure of the human mind and to a computational instantiation of such a theory used in the fields of artificial intelligence (AI) and computational cognitive science."
cognitive architectures,"Cognitive architectures form a subset of general agent architectures. !! The research on cognitive architectures as software instantiation of cognitive theories was initiated by Allen Newell in 1990. !! Cognitive architectures can be symbolic, connectionist, or hybrid. !! Successful cognitive architectures include ACT-R (Adaptive Control of Thought - Rational) and SOAR. !! The software used to implement the cognitive architectures were also ""cognitive architectures""."
cognitive theories,The research on cognitive architectures as software instantiation of cognitive theories was initiated by Allen Newell in 1990.
artificial systems,"The Institute for Creative Technologies defines cognitive architecture as: ""hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together in conjunction with knowledge and skills embodied within the architecture to yield intelligent behavior in a diversity of complex environments. """
rayleigh quotient,Rayleigh quotient iteration is an eigenvalue algorithm which extends the idea of the inverse iteration by using the Rayleigh quotient to obtain increasingly accurate eigenvalue estimates.
computational neural network models,Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics.
quantum neural networks,"Most Quantum neural networks are developed as feed-forward networks. !! Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. !! Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior. !! Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics."
quantum mechanics,"""Interpretations of Quantum Mechanics"" by Peter J. Lewis. !! A uniqueness theorem for interpretations of quantum mechanics. !! Others, like Nico van Kampen and Willis Lamb, have openly criticized non-orthodox interpretations of quantum mechanics. !! Relational quantum mechanics (RQM) is an interpretation of quantum mechanics which treats the state of a quantum system as being observer-dependent, that is, the state is the relation between the observer and the system. !! Relational quantum mechanics arose from a comparison of the quandaries posed by the interpretations of quantum mechanics with those resulting from Lorentz transformations prior to the development of special relativity. !! Stochastic quantum mechanics (or the stochastic interpretation) is an interpretation of quantum mechanics. !! In quantum mechanics, and especially quantum information and the study of open quantum systems, the trace distance T is a metric on the space of density matrices and gives a measure of the distinguishability between two states. !! Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. !! Time-symmetric interpretations of quantum mechanics were first suggested by Walter Schottky in 1921. !! In quantum mechanics, a density matrix is a matrix that describes the quantum state of a physical system. !! Modal interpretations of quantum mechanics were first conceived of in 1972 by Bas van Fraassen, in his paper ""A formal approach to the philosophy of science. """
quantum information,"The term quantum information theory is also used, but it fails to encompass experimental research, and can be confused with a subfield of quantum information science that addresses the processing of quantum information. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms."
classical computer,"Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data."
quantum computer,"Superconducting quantum computing is an implementation of a quantum computer in superconducting electronic circuits. !! Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data. !! A quantum sort is any sorting algorithm that runs on a quantum computer. !! Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. !! Quantum programming is the process of assembling sequences of instructions, called quantum programs, that are capable of running on a quantum computer."
quantum data,"Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data."
algorithmic design,"Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior."
classical backpropagation rule,"Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior."
training set,"Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior. !! Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. !! The point distribution model is a model for representing the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes."
model predictive control,High-level controllers such as model predictive control (MPC) or real-time optimization (RTO) employ mathematical optimization.
axiomatic set theory,"In axiomatic set theory (as developed, for example, in the ZFC axioms), the existence of the power set of any set is postulated by the axiom of power set."
constraint programming,"Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. !! In constraint programming, users declaratively state the constraints on the feasible solutions for a set of decision variables. !! Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. !! Constraint programming is an embedding of constraints in a host language. !! The constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time."
logic program,"Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program."
constraint logic programming,"Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. !! Today most Prolog implementations include one or more libraries for constraint logic programming. !! The first host languages used were logic programming languages, so the field was initially called constraint logic programming. !! The first implementations of constraint logic programming were Prolog III, CLP(R), and CHIP."
stochastic control,"Robert Merton used stochastic control to study optimal portfolios of safe and risky assets. !! Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. !! Stochastic control aims to design the time path of the controlled variables that performs the desired control task with minimum cost, somehow defined, despite the presence of this noise. !! An extremely well-studied formulation in stochastic control is that of linear quadratic Gaussian control. !! The field of stochastic control has developed greatly since the 1970s, particularly in its applications to finance."
stochastic optimal control,Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system.
linear quadratic gaussian control,An extremely well-studied formulation in stochastic control is that of linear quadratic Gaussian control.
convolutional neural networks,"Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. !! Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling. !! On the other hand, neural techniques are able to do end-to-end object detection without specifically defining features, and are typically based on convolutional neural networks (CNN). !! The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. !! Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition. !! The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources."
spatial relations,"Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling."
computing resources,"The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources."
feedback connections,The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections.
neural abstraction pyramid,The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections.
generative model,The Helmholtz machine (named after Hermann von Helmholtz and his concept of Helmholtz free energy) is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data.
helmholtz machine,"Helmholtz machines may also be used in applications requiring a supervised learning algorithm (e. g. character recognition, or position-invariant recognition of an object within a field). !! The Helmholtz machine (named after Hermann von Helmholtz and his concept of Helmholtz free energy) is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. !! A Helmholtz machine contains two networks, a bottom-up recognition network that takes the data as input and produces a distribution over hidden variables, and a top-down ""generative"" network that generates values of the hidden variables and the data itself. !! Helmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm. !! At the time, Helmholtz machines were one of a handful of learning architectures that used feedback as well as feedforward to ensure quality of learned models."
helmholtz machines,"Helmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm. !! At the time, Helmholtz machines were one of a handful of learning architectures that used feedback as well as feedforward to ensure quality of learned models."
unsupervised learning algorithm,"Helmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm."
character recognition,"Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. !! Helmholtz machines may also be used in applications requiring a supervised learning algorithm (e. g. character recognition, or position-invariant recognition of an object within a field)."
supervised learning algorithm,"A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. !! Helmholtz machines may also be used in applications requiring a supervised learning algorithm (e. g. character recognition, or position-invariant recognition of an object within a field). !! This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm. !! In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. !! Some supervised learning algorithms require the user to determine certain control parameters. !! A wide range of supervised learning algorithms are available, each with its strengths and weaknesses."
electronic signature,"While an electronic signature can be as simple as a name entered in an electronic document, digital signatures are increasingly used in e-commerce and in regulatory filings to implement electronic signatures in a cryptographically protected way. !! An electronic signature is intended to provide a secure and accurate identification method for the signatory to provide a seamless transaction. !! Definitions of electronic signatures vary depending on the applicable jurisdiction. !! Electronic signatures are a legal concept distinct from digital signatures, a cryptographic mechanism often used to implement electronic signatures. !! An electronic signature, or e-signature, is data that is logically associated with other data and which is used by the signatory to sign the associated data."
electronic signatures,"Electronic signatures are a legal concept distinct from digital signatures, a cryptographic mechanism often used to implement electronic signatures."
digital signatures,"Electronic signatures are a legal concept distinct from digital signatures, a cryptographic mechanism often used to implement electronic signatures. !! a small change to a message should change the hash value so extensively that a new hash value appears uncorrelated with the old hash value (avalanche effect)Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. !! While an electronic signature can be as simple as a name entered in an electronic document, digital signatures are increasingly used in e-commerce and in regulatory filings to implement electronic signatures in a cryptographically protected way."
implicit euler method,"In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations."
backward euler method,"In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations. !! The backward Euler method has order one. !! and the formula for the backward Euler method follows. !! The region of absolute stability for the backward Euler method is the complement in the complex plane of the disk with radius 1 centered at 1, depicted in the figure. !! The backward Euler method has error of order one in time."
computational statistics,"), ""Special Section: Teaching Computational Statistics"", The American Statistician, 58: 1, !! The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models. !! This marks the beginning of the era of mechanized computational statistics and semiautomatic data processing systems. !! Algorithms for calculating variance play a major role in computational statistics. !! Though computational statistics is widely used today, it actually has a relatively short history of acceptance in the statistics community. !! Computational statistics, or statistical computing, is the bond between statistics and computer science. !! Computational Statistics & Data Analysis is a monthly peer-reviewed scientific journal covering research on and applications of computational statistics and data analysis."
statistical computing,"Computational statistics, or statistical computing, is the bond between statistics and computer science."
generalized additive models,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models."
kernel density estimation,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models."
local regression,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models."
semiautomatic data processing systems,This marks the beginning of the era of mechanized computational statistics and semiautomatic data processing systems.
single core cpu,A computer using a single core CPU is generally slower than a multi-core system.
desktop computers,"Single core processors used to be widespread in desktop computers, but as applications demanded more processing power, the slower speed of single core systems became a detriment to performance."
single core processors,Single core processors are still in use in some niche circumstances.
raspberry pi,Single core processors also used in hobbyist computers like the Raspberry Pi and Single-board microcontrollers.
nm matrix,"In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM."
computer memory,"Similarly, since objects in computer memory are not inherently sequential, and may include links to other objects (including self-referential links), XML data binding mappings often have difficulty preserving all the information about an object when it is marshalled to XML. !! A sorted array is an array data structure in which each element is sorted in numerical, alphabetical, or some other order, and placed at equally spaced addresses in computer memory. !! In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM. !! Random-access memory (RAM; ) is a form of computer memory that can be read and changed in any order, typically used to store working data and machine code. !! Memory management is a form of resource management applied to computer memory. !! XML data binding refers to a means of representing information in an XML document as a business object in computer memory."
in-place matrix transposition,"The following briefly summarizes the published algorithms to perform in-place matrix transposition. !! In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM."
situ matrix transposition,"In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM."
parity function,"Parity function: their value is 1 if the input vector has odd number of onesThe n-ary versions of AND, OR, XOR, NAND, NOR and XNOR are also symmetric Boolean functions."
distributed artificial intelligence,"The objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of artificial intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents). !! Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition. !! In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents[2]. !! Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. !! Distributed Artificial Intelligence (DAI) is an approach to solving complex learning, planning, and decision making problems."
decision making problems,"Distributed Artificial Intelligence (DAI) is an approach to solving complex learning, planning, and decision making problems."
autonomous processing nodes,"The objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of artificial intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents)."
distributed artificial intelligence systems,"Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition."
network elements,"Streaming media is multimedia that is delivered and consumed in a continuous manner from a source, with little or no intermediate storage in network elements."
streaming media,"Streaming media is multimedia that is delivered and consumed in a continuous manner from a source, with little or no intermediate storage in network elements. !! The term ""streaming media"" can apply to media other than video and audio, such as live closed captioning, ticker tape, and real-time text, which are all considered ""streaming text"". !! Other early companies that created streaming media technology include RealNetworks (originally known as Progressive Networks) and Protocomm both prior to widespread World Wide Web usage. !! Practical streaming media was only made possible with advances in data compression, due to the impractically high bandwidth requirements of uncompressed media. !! Microsoft developed a media player known as ActiveMovie in 1995 that supported streaming media and included a proprietary streaming format, which was the precursor to the streaming feature later in Windows Media Player 6."
streaming text,"The term ""streaming media"" can apply to media other than video and audio, such as live closed captioning, ticker tape, and real-time text, which are all considered ""streaming text""."
progressive networks,Other early companies that created streaming media technology include RealNetworks (originally known as Progressive Networks) and Protocomm both prior to widespread World Wide Web usage.
computer science fields,"There has since been a large body of work centered around data streaming algorithms that spans a diverse spectrum of computer science fields such as theory, databases, networking, and natural language processing."
data streaming algorithms,"Lall, Ashwin; Sekar, Vyas; Ogihara, Mitsunori; Xu, Jun; Zhang, Hui (2006), ""Data streaming algorithms for estimating entropy of network traffic"", Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS 2006) (PDF), p. 145,"
subject-oriented programming,"Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches. !! The introduction of aspect-oriented programming in 1997 raised questions about its relationship to subject-oriented programming, and about the difference between subjects and aspects. !! Subject-oriented programming advocates the organization of the classes that describe objects into ""subjects"", which may be composed to form larger subjects. !! In the presentation of subject-oriented programming, the join-points were deliberately restricted to field access and method call on the grounds that those were the points at which well-designed frameworks were designed to admit functional extension. !! In computing, subject-oriented programming is an object-oriented software paradigm in which the state (fields) and behavior (methods) of objects are not seen as intrinsic to the objects themselves, but are provided by various subjective perceptions (""subjects"") of the objects."
method call,"In the presentation of subject-oriented programming, the join-points were deliberately restricted to field access and method call on the grounds that those were the points at which well-designed frameworks were designed to admit functional extension."
composition filters,"Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches."
feature oriented programming,"Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches."
boolean algebra,"Boolean algebra was introduced by George Boole in his first book The Mathematical Analysis of Logic (1847), and set forth more fully in his An Investigation of the Laws of Thought (1854). !! Instead of elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction (and) denoted as , the disjunction (or) denoted as , and the negation (not) denoted as . !! According to Huntington, the term ""Boolean algebra"" was first suggested by Sheffer in 1913, although Charles Sanders Peirce gave the title ""A Boolean Algebra with One Constant"" to the first chapter of his ""The Simplest Mathematics"" in 1880. !! In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. !! In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference. !! Boolean algebra has been fundamental in the development of digital electronics, and is provided for in all modern programming languages. !! A truth table is a mathematical table used in logicspecifically in connection with Boolean algebra, boolean functions, and propositional calculuswhich sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables."
de morgan's law,"De Morgan's laws are an example of a more general concept of mathematical duality. !! De Morgan's laws commonly apply to text searching using Boolean operators AND, OR, and NOT. !! In set notation, De Morgan's laws can be remembered using the mnemonic ""break the line, change the sign"". !! De Morgan's laws are normally shown in the compact form above, with the negation of the output on the left and negation of the inputs on the right. !! In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference."
transformation rules,"In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference."
mathematical duality,De Morgan's laws are an example of a more general concept of mathematical duality.
instance-based learning,"One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. !! In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory. !! Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks."
kernel machines,"Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks."
rbf networks,"Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks."
systems architecture,"Systems architecture depends heavily on practices and techniques which were developed over thousands of years in many other fields, perhaps the most important being civil architecture. !! A systems architecture makes use of elements of both software and hardware and is used to enable design of such a composite system. !! What is Systems Architecture !! Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. !! Systems Architecture: Canaxia Brings an Architect on Board, Article !! Journal of Systems Architecture"
reactive programming,"On the other hand, in reactive programming, the value of a is automatically updated whenever the values of b or c change, without the program having to explicit re-execute the statement a := b + c to determine the presently assigned value of a. !! Reactive programming has been proposed as a way to simplify the creation of interactive user interfaces and near-real-time system animation. !! Another example is a hardware description language such as Verilog, where reactive programming enables changes to be modeled as they propagate through circuits. !! In computing, reactive programming is a declarative programming paradigm concerned with data streams and the propagation of change. !! For example, in a modelviewcontroller (MVC) architecture, reactive programming can facilitate changes in an underlying model that are reflected automatically in an associated view."
hardware description language,"Another example is a hardware description language such as Verilog, where reactive programming enables changes to be modeled as they propagate through circuits."
doubly linked list,"In computer science, a doubly linked list is a linked data structure that consists of a set of sequentially linked records called nodes. !! In a 'doubly linked list', each node contains, besides the next-node link, a second link field pointing to the 'previous' node in the sequence. !! Any node of a doubly linked list, once obtained, can be used to begin a new traversal of the list, in either direction (towards beginning or end), from the given node. !! The link fields of a doubly linked list node are often called next and previous or forward and backward. !! While adding or removing a node in a doubly linked list requires changing more links than the same operations on a singly linked list, the operations are simpler and potentially more efficient (for nodes other than first nodes) because there is no need to keep track of the previous node during traversal or no need to traverse the list to find the previous node, so that its link can be modified. !! The first and last nodes of a doubly linked list are immediately accessible (i. e. , accessible without traversal, and usually called head and tail) and therefore allow traversal of the list from the beginning or end of the list, respectively: e. g. , traversing the list from beginning to end, or from end to beginning, in a search of the list for a node with specific data value."
Node Link,"In a 'doubly linked list', each node contains, besides the next-node link, a second link field pointing to the 'previous' node in the sequence. !! With this convention, an empty list consists of the sentinel node alone, pointing to itself via the next-node link."
synchronous optical networking,Remote error indication (REI) or formerly far end block error (FEBE) is an alarm signal used in synchronous optical networking (SONET).
matrix operations,"Numerical linear algebra, sometimes called applied linear algebra, is the study of how matrix operations can be used to create computer algorithms which efficiently and accurately provide approximate answers to questions in continuous mathematics."
bayes network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG)."
probabilistic graphical model,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables."
bayes net,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG)."
decision network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG)."
belief network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! This type of graphical model is known as a directed graphical model, Bayesian network, or belief network."
directed acyclic graph,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! If the graph is a directed acyclic graph (DAG), topological orderings are pre-topological orderings and vice versa."
bayesian networks,"Efficient algorithms can perform inference and learning in Bayesian networks. !! A generalization of the Viterbi algorithm, termed the max-sum algorithm (or max-product algorithm) can be used to find the most likely assignment of all or some subset of latent variables in a large number of graphical models, e. g. Bayesian networks, Markov random fields and conditional random fields. !! Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. !! Bayesian networks that model sequences of variables (e. g. speech signals or protein sequences) are called dynamic Bayesian networks. !! Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. !! Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
probabilistic relationships,"For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms."
speech signals,"This article is about Compressed sensing in speech signals. !! Bayesian networks that model sequences of variables (e. g. speech signals or protein sequences) are called dynamic Bayesian networks. !! The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. !! Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. !! Speech processing is the study of speech signals and the processing methods of signals."
proxy pattern,"Proxy pattern description from the Portland Pattern Repository !! In computer programming, the proxy pattern is a software design pattern. !! The lazy loading demonstrated in this example is not part of the proxy pattern, but is merely an advantage made possible by the use of the proxy. !! Using the proxy pattern, the code of the ProxyImage avoids multiple loading of the image, accessing it from the other system in a memory-saving manner."
artificial bee colony algorithm,"In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Dervi Karaboa (Erciyes University) in 2005."
sorting algorithm,"Among the authors of early sorting algorithms around 1951 was Betty Holberton, who worked on ENIAC and UNIVAC. !! For 20 years, merge-insertion sort was the sorting algorithm with the fewest comparisons known for all input lengths. !! For typical serial sorting algorithms, good behavior is O(n log n), with parallel sort in O(log2 n), and bad behavior is O(n2). !! In computer science, a sorting algorithm is an algorithm that puts elements of a list into an order. !! A quantum sort is any sorting algorithm that runs on a quantum computer. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted). !! Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key values (N) are approximately the same. !! A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a ""less than or equal to"" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list."
computer code,Code stylometry (also known as program authorship attribution or source code authorship analysis) is the application of stylometry to computer code to attribute authorship to anonymous binary or source code.
code stylometry,"Code stylometry (also known as program authorship attribution or source code authorship analysis) is the application of stylometry to computer code to attribute authorship to anonymous binary or source code. !! Unlike software forensics, code stylometry attributes authorship for purposes other than intellectual property infringement, including plagiarism detection, copyright investigation, and authorship verification."
copyright investigation,"Unlike software forensics, code stylometry attributes authorship for purposes other than intellectual property infringement, including plagiarism detection, copyright investigation, and authorship verification."
pseudorandom number generators,"However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators. !! Other higher-quality PRNGs, both in terms of computational and statistical performance, were developed before and after this date; these can be identified in the List of pseudorandom number generators. !! Although sequences that are closer to truly random can be generated using hardware random number generators, pseudorandom number generators are important in practice for their speed in number generation and their reproducibility. !! Random number are generated by Javascript pseudorandom number generators (PRNGs) algorithms !! Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling."
javascript pseudorandom number generators,Random number are generated by Javascript pseudorandom number generators (PRNGs) algorithms
hash tables,"There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. !! Double hashing is a computer programming technique used in conjunction with open addressing in hash tables to resolve hash collisions, by using a secondary hash of the key as an offset when a collision occurs. !! Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, allow significantly faster searching for all but short lists. !! Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of keyvalue pairs and looking up the value associated with a given key. !! As Thorup & Zhang (2012) write, ""Hash tables are the most commonly used nontrivial data structures, and the most popular implementation on standard hardware uses linear probing, which is both fast and simple. """
enhanced double hashing,"(a tetrahedral number), does solve the problem, a technique known as enhanced double hashing."
euclidean distance,"For qubits, the trace distance is equal to half the Euclidean distance in the Bloch representation."
abstract data type,"An abstract data type is defined by its behavior (semantics) from the point of view of a user, of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. !! The term abstract data type can also be regarded as a generalized approach of a number of algebraic structures, such as lattices, groups, and rings. !! The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development. !! In computer science, a mergeable heap (also called a meldable heap) is an abstract data type, which is a heap supporting a merge operation. !! In computer science, an abstract data type (ADT) is a mathematical model for data types. !! An abstract graphical data type (AGDT) is an extension of an abstract data type for computer graphics. !! Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages."
abstract graphical data type,An abstract graphical data type (AGDT) is an extension of an abstract data type for computer graphics.
program errors,Unanticipated arithmetic overflow is a fairly common cause of program errors.
maximum-margin hyperplane,"In the context of support-vector machines, the optimally separating hyperplane or maximum-margin hyperplane is a hyperplane which separates two convex hulls of points and is equidistant from the two."
distributed computing environment,"The Distributed Computing Environment is a component of the OSF offerings, along with Motif, OSF/1 and the Distributed Management Environment (DME). !! In a distributed computing environment, distributed object communication realizes communication between distributed objects. !! In computing, the Distributed Computing Environment (DCE) software system was developed in the early 1990s from the work of the Open Software Foundation (OSF), a consortium (founded in 1988) that included Apollo Computer (part of Hewlett-Packard from 1989), IBM, Digital Equipment Corporation, and others. !! By integrating security, RPC and other distributed services on a single ""official"" distributed computing environment, OSF could offer a major advantage over SVR4, allowing any DCE-supporting system (namely OSF/1) to interoperate in a larger network."
distributed object communication,"The server side object participating in distributed object communication is known as a skeleton (or stub; term avoided here). !! In a distributed computing environment, distributed object communication realizes communication between distributed objects. !! The client side object participating in distributed object communication is known as a stub or proxy, and is an example of a proxy object."
proxy object,"The client side object participating in distributed object communication is known as a stub or proxy, and is an example of a proxy object."
structural induction,"The structural induction proof is a proof that the proposition holds for all the minimal structures and that if it holds for the immediate substructures of a certain structure S, then it must hold for S also. !! Structural recursion is usually proved correct by structural induction; in particularly easy cases, the inductive step is often left out. !! A structural induction proof of some proposition P(L) then consists of two parts: A proof that P([]) is true and a proof that if P(L) is true for some list L, and if L is the tail of list M, then P(M) must also be true. !! Structural induction is a proof method that is used in mathematical logic (e. g. , in the proof of o' theorem), computer science, graph theory, and some other mathematical fields. !! Structural recursion is a recursion method bearing the same relationship to structural induction as ordinary recursion bears to ordinary mathematical induction."
structural recursion,"Structural recursion is usually proved correct by structural induction; in particularly easy cases, the inductive step is often left out. !! Structural recursion is a recursion method bearing the same relationship to structural induction as ordinary recursion bears to ordinary mathematical induction."
internet service providers,"An Internet Routing Registry (IRR) is a database of Internet route objects for determining, and sharing route and related information used for configuring routers, with a view to avoiding problematic issues between Internet service providers."
internet routing registry,"The Internet routing registry works by providing an interlinked hierarchy of objects designed to facilitate the organization of IP routing between organizations, and also to provide data in an appropriate format for automatic programming of routers. !! An Internet Routing Registry (IRR) is a database of Internet route objects for determining, and sharing route and related information used for configuring routers, with a view to avoiding problematic issues between Internet service providers."
internet route objects,"An Internet Routing Registry (IRR) is a database of Internet route objects for determining, and sharing route and related information used for configuring routers, with a view to avoiding problematic issues between Internet service providers."
ip routing,"The Internet routing registry works by providing an interlinked hierarchy of objects designed to facilitate the organization of IP routing between organizations, and also to provide data in an appropriate format for automatic programming of routers."
microsoft sql server,"In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems."
information dimension,"It is shown that information dimension and differential entropy are tightly connected. !! then we are doing exactly the same quantization as the definition of information dimension. !! The information dimension of a distribution gives a theoretical upper bound on the compression rate, if one wants to compress a variable coming from this distribution. !! In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! In 2010, Wu and Verd gave an operational characterization of Rnyi information dimension as the fundamental limit of almost lossless data compression for analog sources under various regularity constraints of the encoder/decoder."
euclidean space,"In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! In computational geometry, a sweep line algorithm or plane sweep algorithm is an algorithmic paradigm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space."
normalized entropy,"In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors."
distribution-based clustering,Distribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes.
meldable heap,"In computer science, a mergeable heap (also called a meldable heap) is an abstract data type, which is a heap supporting a merge operation."
computational linguistics,"Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. !! Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. !! The longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff utility, and has applications in computational linguistics and bioinformatics."
automatic speech recognition,"It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT)."
text processing,"In text processing, a proximity search looks for documents where two or more separately matching term occurrences are within a specified distance, where distance is the number of intermediate words or characters. !! Speech recognition applications include voice user interfaces such as voice dialing (e. g. ""call home""), call routing (e. g. ""I would like to make a collect call""), domotic appliance control, search key words (e. g. find a podcast where particular words were spoken), simple data entry (e. g. , entering a credit card number), preparation of structured documents (e. g. a radiology report), determining speaker characteristics, speech-to-text processing (e. g. , word processors or emails), and aircraft (usually termed direct voice input)."
robbinsmonro optimization algorithm,"Stochastic gradient Langevin dynamics (SGLD) is an optimization technique composed of characteristics from Stochastic gradient descent, a RobbinsMonro optimization algorithm, and Langevin dynamics, a mathematical extension of molecular dynamics models."
stochastic gradient descent,"Stochastic gradient Langevin dynamics (SGLD) is an optimization technique composed of characteristics from Stochastic gradient descent, a RobbinsMonro optimization algorithm, and Langevin dynamics, a mathematical extension of molecular dynamics models. !! This can perform significantly better than ""true"" stochastic gradient descent described, because the code can make use of vectorization libraries rather than computing each step separately as was first shown in where it was called ""the bunch-mode back-propagation algorithm"". !! To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. !! The model (e. g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. !! The convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation. !! While the basic idea behind stochastic approximation can be traced back to the RobbinsMonro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning. !! Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e. g. differentiable or subdifferentiable)."
stochastic gradient langevin dynamics,"Stochastic gradient Langevin dynamics (SGLD) is an optimization technique composed of characteristics from Stochastic gradient descent, a RobbinsMonro optimization algorithm, and Langevin dynamics, a mathematical extension of molecular dynamics models."
self-balancing binary search tree,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! These operations when designed for a self-balancing binary search tree, contain precautionary measures against boundlessly increasing tree height, so that these abstract data structures receive the attribute ""self-balancing"". !! In computer science, a self-balancing binary search tree (BST) is any node-based binary search tree that automatically keeps its height (maximal number of levels below the root) small in the face of arbitrary item insertions and deletions. !! This is the case for many binary search trees, such as the AVL trees and the redblack trees the latter was called symmetric binary B-tree and was renamed; it can, however, still be confused with the generic concept of self-balancing binary search tree because of the initials. !! Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues."
avl trees,"This is the case for many binary search trees, such as the AVL trees and the redblack trees the latter was called symmetric binary B-tree and was renamed; it can, however, still be confused with the generic concept of self-balancing binary search tree because of the initials. !! 720 times the worst-case height of RB trees, so AVL trees are more rigidly balanced. !! AVL trees can be colored redblack, thus are a subset of RB trees."
abstract data structures,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! :299-302 Binary search trees are also a fundamental data structure used in construction of abstract data structures such as sets, multisets, and associative arrays."
associative arrays,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! They can be used to implement several other common abstract data types, including lists, stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement those data structures directly without using a linked list as the basis. !! :299-302 Binary search trees are also a fundamental data structure used in construction of abstract data structures such as sets, multisets, and associative arrays."
priority queues,"From a computational-complexity standpoint, priority queues are congruent to sorting algorithms. !! To improve performance, priority queues are typically based on a heap, giving O(log n) performance for inserts and removals, and O(n) to build the heap initially from a set of n elements. !! Stacks and queues are different than priority queues. !! While coders often implement priority queues with heaps, they are conceptually distinct from heaps. !! The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues. !! This operation and its O(1) performance is crucial to many applications of priority queues."
mutable ordered lists,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets."
digital signal,"In electronics, a digital-to-analog converter (DAC, D/A, D2A, or D-to-A) is a system that converts a digital signal into an analog signal. !! Simple digital signals represent information in discrete bands of analog levels. !! In a digital signal, the physical quantity representing the information may be a variable electric current or voltage, the intensity, phase or polarization of an optical or other electromagnetic field, acoustic pressure, the magnetization of a magnetic storage media, etcetera. !! A digital signal is a signal that represents data as a sequence of discrete values; at any given time it can only take on, at most, one of a finite number of values. !! As a result, digital signals have noise immunity; electronic noise, provided it is not too great, will not affect digital circuits, whereas noise always degrades the operation of analog signals to some degree. !! Digital signals having more than two states are occasionally used; circuitry using such signals is called multivalued logic. !! In electronics, an analog-to-digital converter (ADC, A/D, or A-to-D) is a system that converts an analog signal, such as a sound picked up by a microphone or light entering a digital camera, into a digital signal."
behavioral software design pattern,The state pattern is a behavioral software design pattern that allows an object to alter its behavior when its internal state changes.
strategy pattern,"The state pattern can be interpreted as a strategy pattern, which is able to switch a strategy through invocations of methods defined in the pattern's interface."
polymorphic association,"Polymorphic association is a term used in discussions of Object-Relational Mapping with respect to the problem of representing in the relational database domain, a relationship from one class to multiple classes."
relational database domain,"Polymorphic association is a term used in discussions of Object-Relational Mapping with respect to the problem of representing in the relational database domain, a relationship from one class to multiple classes."
ordered sets,"In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices."
ram usage,"In computing, lightweight software also called lite program and lightweight application, is a computer program that is designed to have a small memory footprint (RAM usage) and low CPU usage, overall a low usage of system resources."
lightweight software,"In computing, lightweight software also called lite program and lightweight application, is a computer program that is designed to have a small memory footprint (RAM usage) and low CPU usage, overall a low usage of system resources."
weight-balanced tree,"A weight-balanced tree is a binary search tree that stores the sizes of subtrees in the nodes. !! Join: The function Join is on two weight-balanced trees t1 and t2 and a key k and will return a tree containing all elements in t1, t2 as well as k. It requires k to be greater than all keys in t1 and smaller than all keys in t2. !! Several set operations have been defined on weight-balanced trees: union, intersection and set difference. !! Weight-balanced trees are popular in the functional programming community and are used to implement sets and maps in MIT Scheme, SLIB and implementations of Haskell. !! With the new operations, the implementation of weight-balanced trees can be more efficient and highly-parallelizable."
binary search tree,"AA trees are a variation of the redblack tree, a form of binary search tree which supports efficient addition and deletion of entries. !! The performance of a binary search tree is dependent on the order of insertion of the nodes into the tree; several variations of the binary search tree can be built with guaranteed worst-case performance. !! A weight-balanced tree is a binary search tree that stores the sizes of subtrees in the nodes. !! In computer science, a ternary search tree is a type of trie (sometimes called a prefix tree) where nodes are arranged in a manner similar to a binary search tree, but with up to three children rather than the binary tree's limit of two. !! In computer science, an optimal binary search tree (Optimal BST), sometimes called a weight-balanced binary tree, is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities). !! In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree data structure whose internal nodes each store a key greater than all the keys in the node's left subtree and less than those in its right subtree. !! The binary search tree algorithm was discovered independently by several researchers, including P. F. Windley, Andrew Donald Booth, Andrew Colin, Thomas N. Hibbard, and attributed to Conway Berners-Lee and David Wheeler, in 1960 for storing labeled data in magnetic tapes. !! Tango trees work by partitioning a binary search tree into a set of preferred paths, which are themselves stored in auxiliary trees (so the tango tree is represented as a tree of trees). !! Binary search trees allow binary search for fast lookup, addition, and removal of data items, and can be used to implement dynamic sets and lookup tables. !! The time complexity of operations on the binary search tree is directly proportional to the height of the tree."
function join,"Join: The function Join is on two weight-balanced trees t1 and t2 and a key k and will return a tree containing all elements in t1, t2 as well as k. It requires k to be greater than all keys in t1 and smaller than all keys in t2."
meta learning,"Meta learning is originally described by Donald B. Maudsley (1979) as ""the process by which learners become aware of and increasingly in control of habits of perception, inquiry, learning, and growth that they have internalized"". !! Meta learning can be defined as an awareness and understanding of the phenomenon of learning itself as opposed to subject knowledge. !! Five principles were enunciated to facilitate meta learning. !! Meta learning is a branch of metacognition concerned with learning about one's own learning and learning processes. !! The idea of meta learning was later used by John Biggs (1985) to describe the state of ""being aware of and taking control of one's own learning""."
pooling layers,"The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. !! Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. !! Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. !! This is followed by other layers such as pooling layers, fully connected layers, and normalization layers. !! Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers."
fully connected layers,"This is followed by other layers such as pooling layers, fully connected layers, and normalization layers."
normalization layers,"This is followed by other layers such as pooling layers, fully connected layers, and normalization layers."
traditional convolutional layers,Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers.
neuron clusters,Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer.
feature maps,"Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value."
binary arithmetic,"Fractions in binary arithmetic terminate only if 2 is the only prime factor in the denominator. !! The full title of Leibniz's article is translated into English as the ""Explanation of Binary Arithmetic, which uses only the characters 1 and 0, with some remarks on its usefulness, and on the light it throws on the ancient Chinese figures of Fu Xi"". !! In 1937, Claude Shannon produced his master's thesis at MIT that implemented Boolean algebra and binary arithmetic using electronic relays and switches for the first time in history."
adjacency list,"An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighbouring vertices or edges. !! In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! The main operation performed by the adjacency list data structure is to report a list of the neighbors of a given vertex. !! Each unordered list within an adjacency list describes the set of neighbors of a particular vertex in the graph. !! This version of the adjacency list uses more memory than the version in which adjacent vertices are listed directly, but the existence of explicit edge objects allows it extra flexibility in storing additional information about edges."
finite graph,"In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph."
neighbouring vertices,An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighbouring vertices or edges.
adjacent vertices,"This version of the adjacency list uses more memory than the version in which adjacent vertices are listed directly, but the existence of explicit edge objects allows it extra flexibility in storing additional information about edges."
adjacency list data structure,The main operation performed by the adjacency list data structure is to report a list of the neighbors of a given vertex.
modern decision theory,The origins of robust optimization date back to the establishment of modern decision theory in the 1950s and the use of worst case analysis and Wald's maximin model as a tool for the treatment of severe uncertainty.
maximin model,It was shown that the stability radius model is an instance of Wald's maximin model. !! The origins of robust optimization date back to the establishment of modern decision theory in the 1950s and the use of worst case analysis and Wald's maximin model as a tool for the treatment of severe uncertainty.
classification criteria,There are a number of classification criteria for robust optimization problems/models.
maximin models,Modern robust optimization deals primarily with non-probabilistic models of robustness that are worst case oriented and as such usually deploy Wald's maximin models.
probabilistic models,Modern robust optimization deals primarily with non-probabilistic models of robustness that are worst case oriented and as such usually deploy Wald's maximin models.
software construction,"Software construction is a software engineering discipline. !! In order to account for the unanticipated gaps in the software design, during software construction some design modifications must be made on a smaller or larger scale to flesh out details of the software design. !! Linguistic notations which are distinguished in particular by the use of word-like strings of text to represent complex software constructions, and the combination of such word-like strings into patterns that have a sentence-like syntax."
chi-square automatic interaction detection,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing)."
bonferroni testing,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing)."
decision tree technique,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing)."
augmented reality,"Commercial augmented reality experiences were first introduced in entertainment and gaming businesses. !! Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real world are enhanced by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory. !! In this way, augmented reality alters one's ongoing perception of a real-world environment, whereas virtual reality completely replaces the user's real-world environment with a simulated one. !! The primary value of augmented reality is the manner in which components of the digital world blend into a person's perception of the real world, not as a simple display of data, but through the integration of immersive sensations, which are perceived as natural parts of an environment. !! Augmented reality is related to two largely synonymous terms: mixed reality and computer-mediated reality. !! (2002), ""Exploring Humanistic Intelligence Through Physiologically Mediated Reality"" (PDF), Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR'02) 0-7695-1781-1/02, IEEE"
compiler optimization technique,Loop splitting is a compiler optimization technique. !! Bounds-checking elimination is a compiler optimization technique that eliminates unneeded bounds checking.
loop splitting,More generalised loop splitting was added in GCC 7. !! Loop splitting is a compiler optimization technique. !! Loop peeling is a special case of loop splitting which splits any problematic first (or last) few iterations from the loop and performs them outside of the loop body.
loop peeling,Loop peeling is a special case of loop splitting which splits any problematic first (or last) few iterations from the loop and performs them outside of the loop body.
loop body,Loop peeling is a special case of loop splitting which splits any problematic first (or last) few iterations from the loop and performs them outside of the loop body.
generalised loop splitting,More generalised loop splitting was added in GCC 7.
application programming interface,"Interface-based programming defines the application as a collection of components, in which Application Programming Interface (API) calls between components may only be made through abstract interfaces, not concrete classes. !! Portable Distributed Objects (PDO) is an application programming interface (API) for creating object-oriented code that can be executed remotely on a network of computers."
portable distributed objects,Portable Distributed Objects (PDO) is an application programming interface (API) for creating object-oriented code that can be executed remotely on a network of computers.
category theory,"In category theory, a subobject classifier is a special object of a category such that, intuitively, the subobjects of any object X in the category correspond to the morphisms from X to . !! Categorical logic is the branch of mathematics in which tools and concepts from category theory are applied to the study of mathematical logic."
truth value object,"Therefore, a subobject classifier is also known as a ""truth value object"" and the concept is widely used in the categorical description of logic."
computer network architecture,Delay-tolerant networking (DTN) is an approach to computer network architecture that seeks to address the technical issues in heterogeneous networks that may lack continuous network connectivity.
heterogeneous networks,Delay-tolerant networking (DTN) is an approach to computer network architecture that seeks to address the technical issues in heterogeneous networks that may lack continuous network connectivity.
delay-tolerant networking,"The Delay-Tolerant Networking Research Group. !! This field saw many optimizations on classic ad hoc and delay-tolerant networking algorithms and began to examine factors such as security, reliability, verifiability, and other areas of research that are well understood in traditional computer networking. !! In 2002, Kevin Fall started to adapt some of the ideas in the IPN design to terrestrial networks and coined the term delay-tolerant networking and the DTN acronym. !! Delay-tolerant networking (DTN) is an approach to computer network architecture that seeks to address the technical issues in heterogeneous networks that may lack continuous network connectivity."
ipn design,"In 2002, Kevin Fall started to adapt some of the ideas in the IPN design to terrestrial networks and coined the term delay-tolerant networking and the DTN acronym."
terrestrial networks,"In 2002, Kevin Fall started to adapt some of the ideas in the IPN design to terrestrial networks and coined the term delay-tolerant networking and the DTN acronym."
traditional computer networking,"This field saw many optimizations on classic ad hoc and delay-tolerant networking algorithms and began to examine factors such as security, reliability, verifiability, and other areas of research that are well understood in traditional computer networking."
classic ad hoc,"This field saw many optimizations on classic ad hoc and delay-tolerant networking algorithms and began to examine factors such as security, reliability, verifiability, and other areas of research that are well understood in traditional computer networking."
hashing problem,Static Hashing is another form of the hashing problem which allows users to perform lookups on a finalized dictionary set (all objects in the dictionary are final and not changing).
static hashing,"Static Hashing is another form of the hashing problem which allows users to perform lookups on a finalized dictionary set (all objects in the dictionary are final and not changing). !! Since static hashing requires that the database, its objects and reference remain the same its applications are limited."
informatics techniques,"Biodiversity informatics is the application of informatics techniques to biodiversity information, such as taxonomy, biogeography or ecology."
biodiversity informatics,"Biodiversity informatics is the application of informatics techniques to biodiversity information, such as taxonomy, biogeography or ecology. !! Biodiversity informatics contrasts with ""bioinformatics"", which is often used synonymously with the computerized handling of data in the specialized area of molecular biology. !! Biodiversity informatics (different but linked to bioinformatics) is the application of information technology methods to the problems of organizing, accessing, visualizing and analyzing primary biodiversity data. !! Biodiversity informatics may also have to cope with managing information from unnamed taxa such as that produced by environmental sampling and sequencing of mixed-field samples. !! Biodiversity informatics is a term that was only coined around 1992 but with rapidly increasing data sets has become useful in numerous studies and applications, such as the construction of taxonomic databases or geographic information systems."
taxonomic databases,"Biodiversity informatics is a term that was only coined around 1992 but with rapidly increasing data sets has become useful in numerous studies and applications, such as the construction of taxonomic databases or geographic information systems."
processing unit,Measurement of the six degrees of freedom is accomplished today through both AC and DC magnetic or electromagnetic fields in sensors that transmit positional and angular data to a processing unit.
tree contraction,"Parallel tree contraction was introduced by Gary L. Miller and John H. Reif, and has subsequently been modified to improve efficiency by X. !! We now show the evaluation can be done with parallel tree contraction. !! In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms. !! Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
parallel solution,"In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms."
parallel graph algorithms,"In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms."
parallel tree contraction,"Parallel tree contraction was introduced by Gary L. Miller and John H. Reif, and has subsequently been modified to improve efficiency by X. !! We now show the evaluation can be done with parallel tree contraction. !! In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms. !! Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
maximal subtree isomorphism,"Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
graph isomorphism,"Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
tree isomorphism,"Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
moravec's paradox,"Explanation of the XKCD comic about Moravec's paradox !! Moravec's paradox is the observation by artificial intelligence and robotics researchers that, contrary to traditional assumptions, reasoning requires very little computation, but sensorimotor and perception skills require enormous computational resources."
basic function,This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition.
eigenmode expansion,"Eigenmode expansion (EME) is a computational electrodynamics modelling technique. !! Eigenmode expansion is a rigorous technique to simulate electromagnetic propagation which relies on the decomposition of the electromagnetic fields into a basis set of local eigenmodes that exists in the cross section of the device. !! Eigenmode expansion is a linear frequency-domain method. !! Unlike the beam propagation method, which is only valid under the slowly varying envelope approximation, eigenmode expansion provides a rigorous solution to Maxwell's equations."
data encryption standard,RFC4772 : Security Implications of Using the Data Encryption Standard (DES) !! The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of digital data. !! In 1973 NBS solicited private industry for a data encryption standard (DES).
continuous unimodal function,"Successive parabolic interpolation is a technique for finding the extremum (minimum or maximum) of a continuous unimodal function by successively fitting parabolas (polynomials of degree two) to a function of one variable at three unique points or, in general, a function of n variables at 1+n(n+3)/2 points, and at each iteration replacing the ""oldest"" point with the extremum of the fitted parabola."
successive parabolic interpolation,"Successive parabolic interpolation is a technique for finding the extremum (minimum or maximum) of a continuous unimodal function by successively fitting parabolas (polynomials of degree two) to a function of one variable at three unique points or, in general, a function of n variables at 1+n(n+3)/2 points, and at each iteration replacing the ""oldest"" point with the extremum of the fitted parabola. !! Moreover, not requiring the computation or approximation of function derivatives makes successive parabolic interpolation a popular alternative to other methods that do require them (such as gradient descent and Newton's method)."
binary inputs,"A logic gate is an idealized model of computation or a physical electronic device implementing a Boolean function, a logical operation performed on one or more binary inputs that produces a single binary output."
boolean function,"In Boolean logic, the majority function (also called the median operator) is the Boolean function that evaluates to false when half or more arguments are false and true otherwise, i. e. the value of the function equals the value of the majority of the inputs. !! A Boolean network consists of a discrete set of boolean variables each of which has a Boolean function (possibly different for each variable) assigned to it which takes inputs from a subset of those variables and output that determines the state of the variable it is assigned to. !! Minimizing (or, equivalently, maximizing) a pseudo-Boolean function is NP-hard. !! The degree of the pseudo-Boolean function is simply the degree of the polynomial in this representation. !! In the mathematical field of combinatorics, a bent function is a special type of Boolean function which is maximally non-linear; it is as different as possible from the set of all linear and affine functions when measured by Hamming distance between truth tables. !! This can easily be seen by formulating, for example, the maximum cut problem as maximizing a pseudo-Boolean function. !! In computer science, a binary decision diagram (BDD) or branching program is a data structure that is used to represent a Boolean function. !! A logic gate is an idealized model of computation or a physical electronic device implementing a Boolean function, a logical operation performed on one or more binary inputs that produces a single binary output."
logic gate,"Logic gates are primarily implemented using diodes or transistors acting as electronic switches, but can also be constructed using vacuum tubes, electromagnetic relays (relay logic), fluidic logic, pneumatic logic, optics, molecules, or even mechanical elements. !! Depending on the context, the term may refer to an ideal logic gate, one that has for instance zero rise time and unlimited fan-out, or it may refer to a non-ideal physical device (see Ideal and real op-amps for comparison). !! With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic. !! Compound logic gates AND-OR-Invert (AOI) and OR-AND-Invert (OAI) are often employed in circuit design because their construction using MOSFETs is simpler and more efficient than the sum of the individual gates. !! A logic gate is an idealized model of computation or a physical electronic device implementing a Boolean function, a logical operation performed on one or more binary inputs that produces a single binary output."
boolean functions,"With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic. !! This is an important class of pseudo-boolean functions, because they can be minimized in polynomial time. !! A truth table is a mathematical table used in logicspecifically in connection with Boolean algebra, boolean functions, and propositional calculuswhich sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables."
boolean logic,"In Boolean logic, the majority function (also called the median operator) is the Boolean function that evaluates to false when half or more arguments are false and true otherwise, i. e. the value of the function equals the value of the majority of the inputs. !! With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic."
compound logic gates,Compound logic gates AND-OR-Invert (AOI) and OR-AND-Invert (OAI) are often employed in circuit design because their construction using MOSFETs is simpler and more efficient than the sum of the individual gates.
performance analyzer,Performance Analyzer is available as part of Oracle Developer Studio. !! Performance Analyzer is a commercial utility software for software performance analysis for x86 or SPARC machines.
software performance analysis,Performance Analyzer is a commercial utility software for software performance analysis for x86 or SPARC machines.
oracle developer studio,Performance Analyzer is available as part of Oracle Developer Studio.
n log n comparisons,"Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted)."
input sequences,"Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted)."
comparison sorting algorithms,"Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted)."
sorting algorithms,"Binary search trees are also efficacious in sorting algorithms and search algorithms. !! From a computational-complexity standpoint, priority queues are congruent to sorting algorithms. !! Among the authors of early sorting algorithms around 1951 was Betty Holberton, who worked on ENIAC and UNIVAC. !! For typical serial sorting algorithms, good behavior is O(n log n), with parallel sort in O(log2 n), and bad behavior is O(n2). !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted). !! In particular, some sorting algorithms are ""in-place""."
randomized algorithms,"Reservoir sampling is a family of randomized algorithms for choosing a simple random sample, without replacement, of k items from a population of unknown size n in a single pass over the items. !! In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator. !! The study of randomized algorithms was spurred by the 1977 discovery of a randomized primality test (i. e. , determining the primality of a number) by Robert M. Solovay and Volker Strassen. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Computational complexity theory models randomized algorithms as probabilistic Turing machines. !! Randomized algorithms are particularly useful when faced with a malicious ""adversary"" or attacker who deliberately tries to feed a bad input to the algorithm (see worst-case complexity and competitive analysis (online algorithm)) such as in the Prisoner's dilemma. !! This class acts as the randomized equivalent of P, i. e. BPP represents the class of efficient randomized algorithms."
timespace tradeoffs,"Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds."
information security,"In the fields of physical security and information security, access control (AC) is the selective restriction of access to a place or other resource, while access management describes the process. !! Web application security is a branch of information security that deals specifically with the security of websites, web applications and web services."
significant digits,"The term floating point refers to the fact that a number's radix point (decimal point, or, more commonly in computers, binary point) can ""float""; that is, it can be placed anywhere relative to the significant digits of the number."
logarithm function,"The value distribution is similar to floating point, but the value-to-representation curve (i. e. , the graph of the logarithm function) is smooth (except at 0)."
binary floating point,"Some simple rational numbers (e. g. , 1/3 and 1/10) cannot be represented exactly in binary floating point, no matter what the precision is."
time delay neural network,"Matlab: The neural network toolbox has explicit functionality designed to produce a time delay neural network give the step size of time delays and an optional training function. !! The Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers of perceptrons, and is implemented as a feedforward neural network. !! Time delay neural network (TDNN) is a multilayer artificial neural network architecture whose purpose is to 1) classify patterns with shift-invariance, and 2) model context at each layer of the network."
feedforward neural network,"The Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers of perceptrons, and is implemented as a feedforward neural network. !! A probabilistic neural network (PNN) is a feedforward neural network, which is widely used in classification and pattern recognition problems."
neural network toolbox,Matlab: The neural network toolbox has explicit functionality designed to produce a time delay neural network give the step size of time delays and an optional training function.
time delays,Matlab: The neural network toolbox has explicit functionality designed to produce a time delay neural network give the step size of time delays and an optional training function.
automata theory,"Automata theory is closely related to formal language theory. !! Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. !! Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. !! Automata theory was initially considered a branch of mathematical systems theory, studying the behavior of discrete-parameter systems. !! With the publication of this volume, ""automata theory emerged as a relatively autonomous discipline"". !! Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. !! To convert a grammar to Chomsky normal form, a sequence of simple transformations is applied in a certain order; this is described in most textbooks on automata theory. !! Early work in automata theory differed from previous work on systems by using abstract algebra to describe information systems rather than differential calculus to describe material systems."
abstract machines,"Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. !! Abstract machines can also be used to model abstract data types, which can be specified in terms of their operational semantics on an abstract machine. !! In the theory of computation, abstract machines are often used in thought experiments regarding computability or to analyze the complexity of algorithms. !! Through the use of abstract machines, it is possible to compute the amount of resources (time, memory, etc. ) !! This application of abstract machines is related to the subject of computational complexity theory. !! Undecidable problems can be related to different topics, such as logic, abstract machines or topology. !! More complex definitions create abstract machines with full instruction sets, registers and models of memory."
formal language theory,"In formal language theory, deterministic context-free languages (DCFL) are a proper subset of context-free languages. !! Automata theory is closely related to formal language theory. !! In formal language theory, computer science and linguistics, the Chomsky hierarchy (also referred to as the ChomskySchtzenberger hierarchy) is a containment hierarchy of classes of formal grammars."
differential calculus,Early work in automata theory differed from previous work on systems by using abstract algebra to describe information systems rather than differential calculus to describe material systems.
sparse graph,"More precisely, it follows from a result of Nash-Williams (1964) that the graphs of arboricity at most a are exactly the (a,a)-sparse graphs. !! The opposite, a graph with only a few edges, is a sparse graph. !! Pseudoforests are exactly the (1,0)-sparse graphs, and the Laman graphs arising in rigidity theory are exactly the (2,3)-tight graphs. !! A Sparse graph code is a code which is represented by a sparse graph. !! However, not every (3,6)-sparse graph is planar. !! Thus trees are exactly the (1,1)-tight graphs, forests are exactly the (1,1)-sparse graphs, and graphs with arboricity k are exactly the (k,k)-sparse graphs."
thus trees,"Thus trees are exactly the (1,1)-tight graphs, forests are exactly the (1,1)-sparse graphs, and graphs with arboricity k are exactly the (k,k)-sparse graphs."
cognitive dimensions,"Cognitive dimensions or cognitive dimensions of notations are design principles for notations, user interfaces and programming languages, described by researcher Thomas R. G. Green and furthered researched with Marian Petre."
binary space partitioning,"Binary space partitioning is a generic process of recursively dividing a scene into two until the partitioning satisfies one or more requirements. !! A disadvantage of binary space partitioning is that generating a BSP tree can be time-consuming. !! Binary space partitioning was developed in the context of 3D computer graphics in 1969. !! Binary space partitioning arose from the computer graphics need to rapidly draw three-dimensional scenes composed of polygons. !! In computer science, binary space partitioning (BSP) is a method for recursively subdividing a space into two convex sets by using hyperplanes as partitions."
3d computer graphics,"Binary space partitioning was developed in the context of 3D computer graphics in 1969. !! In 3D computer graphics, ray tracing is a technique for modeling light transport for use in a wide variety of rendering algorithms for generating digital images."
bsp tree,A disadvantage of binary space partitioning is that generating a BSP tree can be time-consuming.
humancomputer interaction,"Interruption science is a branch of human factors psychology and emerged from humancomputer interaction and cognitive psychology. !! In the industrial design field of humancomputer interaction, a user interface (UI) is the space where interactions between humans and machines occur. !! Augmented cognition research generally focuses on tasks and environments where humancomputer interaction and interfaces already exist. !! The field of information visualization has emerged ""from research in humancomputer interaction, computer science, graphics, visual design, psychology, and business methods. !! Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! Implicit data collection is used in humancomputer interaction to gather data about the user in an implicit, non-invasive way. !! In humancomputer interaction, MPG stands for ""multi-touch, physics and gestures"", referencing a common method of interacting with computers and various electronic devices. !! Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search at SIGIR06 and Exploratory Search and HCI at CHI07 (in order to meet with the experts in humancomputer interaction). !! Common topics of interaction design include design, humancomputer interaction, and software development. !! In humancomputer interaction, an organic user interface (OUI) is defined as a user interface with a non-flat display. !! Mobile interaction is an aspect of humancomputer interaction that emerged when computers became small enough to enable mobile usage, around the 1990s. !! Proper design of error messages is an important topic in usability and other fields of humancomputer interaction. !! Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in humancomputer interaction and ergonomics."
predictive model,Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in humancomputer interaction and ergonomics.
fitts's law,"Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in humancomputer interaction and ergonomics. !! Fitts's law is used to model the act of pointing, either by physically touching an object with a hand or finger, or virtually, by pointing to an object on a computer monitor using a pointing device. !! Both parameters show the linear dependency in Fitts's law. !! Fitts's law has been shown to apply under a variety of conditions; with many different limbs (hands, feet, the lower lip, head-mounted sights), manipulanda (input devices), physical environments (including underwater), and user populations (young, old, special educational needs, and drugged participants). !! The first humancomputer interface application of Fitts's law was by Card, English, and Burr, who used the index of performance (IP), interpreted as 1b, to compare performance of different input devices, with the mouse coming out on top compared to the joystick or directional movement keys."
linear dependency,Both parameters show the linear dependency in Fitts's law.
sigmoid curve,"A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve."
mathematical function,"An unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. !! A minimax approximation algorithm (or L approximation or uniform approximation) is a method to find an approximation of a mathematical function that minimizes maximum error. !! A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve."
sigmoid function,"A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve. !! In some fields, most notably in the context of artificial neural networks, the term ""sigmoid function"" is used as an alias for the logistic function. !! Other standard sigmoid functions are given in the Examples section. !! Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing. !! Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams)."
standard sigmoid functions,Other standard sigmoid functions are given in the Examples section.
gompertz curve,Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams).
ogee curve,Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams).
sigmoid functions,"Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing."
real numbers,"Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. !! Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing."
analog-to-digital converter,"Since the values are added together, the dithering produces results that are more exact than the LSB of the analog-to-digital converter. !! People often produce music on computers using an analog recording and therefore need analog-to-digital converters to create the pulse-code modulation (PCM) data streams that go onto compact discs and digital music files. !! A Time-stretch analog-to-digital converter (TS-ADC) digitizes a very wide bandwidth analog signal, that cannot be digitized by a conventional electronic ADC, by time-stretching the signal prior to digitization. !! Analog-to-digital converters are integral to 2000s era music reproduction technology and digital audio workstation-based sound recording. !! A digital filter system usually consists of an analog-to-digital converter (ADC) to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc. !! In electronics, an analog-to-digital converter (ADC, A/D, or A-to-D) is a system that converts an analog signal, such as a sound picked up by a microphone or light entering a digital camera, into a digital signal."
analog signal,"In electronics, a digital-to-analog converter (DAC, D/A, D2A, or D-to-A) is a system that converts a digital signal into an analog signal. !! In electronics, an analog-to-digital converter (ADC, A/D, or A-to-D) is a system that converts an analog signal, such as a sound picked up by a microphone or light entering a digital camera, into a digital signal."
analog recording,People often produce music on computers using an analog recording and therefore need analog-to-digital converters to create the pulse-code modulation (PCM) data streams that go onto compact discs and digital music files.
subrecursive hierarchies,"How can noncomputable functions be classified into a hierarchy based on their level of noncomputabilityAlthough there is considerable overlap in terms of knowledge and methods, mathematical computability theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages."
memory debugger,"Many memory debuggers require applications to be recompiled with special dynamic memory allocation libraries, whose APIs are mostly compatible with conventional dynamic memory allocation libraries, or else use dynamic linking. !! Programs written in languages that have garbage collection, such as managed code, might also need memory debuggers, e. g. for memory leaks due to ""living"" references in collections. !! Memory debuggers work by monitoring memory access, allocations, and deallocation of memory. !! A memory debugger is a debugger for finding software memory problems such as memory leaks and buffer overflows. !! Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
memory leaks,"A memory debugger is a debugger for finding software memory problems such as memory leaks and buffer overflows. !! In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected. !! The memory management system must track outstanding allocations to ensure that they do not overlap and that no memory is ever ""lost"" (i. e. that there are no ""memory leaks"")."
buffer overflows,A memory debugger is a debugger for finding software memory problems such as memory leaks and buffer overflows.
managed code,"Programs written in languages that have garbage collection, such as managed code, might also need memory debuggers, e. g. for memory leaks due to ""living"" references in collections."
garbage collection,"Garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance. !! Resources other than memory, such as network sockets, database handles, user interaction windows, file and device descriptors, are not typically handled by garbage collection. !! Programs written in languages that have garbage collection, such as managed code, might also need memory debuggers, e. g. for memory leaks due to ""living"" references in collections. !! In some languages that do not have built in garbage collection, it can be added through a library, as with the Boehm garbage collector for C and C++. !! In computer science, garbage collection (GC) is a form of automatic memory management. !! Garbage collection was invented by American computer scientist John McCarthy around 1959 to simplify manual memory management in Lisp. !! Garbage collection relieves the programmer from performing manual memory management where the programmer specifies what objects to deallocate and return to the memory system and when to do so."
conventional dynamic memory allocation libraries,"Many memory debuggers require applications to be recompiled with special dynamic memory allocation libraries, whose APIs are mostly compatible with conventional dynamic memory allocation libraries, or else use dynamic linking."
virtual machine,"Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
memory debuggers,"Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
special memory allocation libraries,"Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
shift cipher,"In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques."
caesar cipher,"The Caesar cipher is named after Julius Caesar, who, according to Suetonius, used it with a shift of three (A becoming D when encrypting, and D becoming A when decrypting) to protect messages of military significance. !! In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques. !! The encryption step performed by a Caesar cipher is often incorporated as part of more complex schemes, such as the Vigenre cipher, and still has modern application in the ROT13 system. !! As with all single-alphabet substitution ciphers, the Caesar cipher is easily broken and in modern practice offers essentially no communications security. !! It is unknown how effective the Caesar cipher was at the time, but it is likely to have been reasonably secure, not least because most of Caesar's enemies would have been illiterate and others would have assumed that the messages were written in an unknown foreign language."
caesar shift,"In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques."
vigenre cipher,"The encryption step performed by a Caesar cipher is often incorporated as part of more complex schemes, such as the Vigenre cipher, and still has modern application in the ROT13 system."
computational materials science,"Computational materials science and engineering uses modeling, simulation, theory, and informatics to understand materials. !! Computational materials science is one sub-discipline of both computational science and computational engineering, containing significant overlap with computational chemistry and computational physics. !! The Gordon Research Conference on Computational Materials Science and Engineering began in 2020. !! One notable sub-field of computational materials science is integrated computational materials engineering (ICME), which seeks to use computational results and methods in conjunction with experiments, with a focus on industrial and commercial application. !! Those dedicated to the field include Computational Materials Science, Modelling and Simulation in Materials Science and Engineering, and npj Computational Materials."
integrated computational materials engineering,"One notable sub-field of computational materials science is integrated computational materials engineering (ICME), which seeks to use computational results and methods in conjunction with experiments, with a focus on industrial and commercial application."
computational engineering,"Computational materials science is one sub-discipline of both computational science and computational engineering, containing significant overlap with computational chemistry and computational physics."
computational physics,"Computational materials science is one sub-discipline of both computational science and computational engineering, containing significant overlap with computational chemistry and computational physics. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics. !! Lattice models are also ideal for study by the methods of computational physics, as the discretization of any continuum model automatically turns it into a lattice model."
machine consciousness,"Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics."
synthetic consciousness,"Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics."
artificial consciousness,"Artificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states. !! Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics. !! In his article ""Artificial Consciousness: Utopia or Real Possibility,"" Giorgio Buttazzo says that a common objection to artificial consciousness is that ""Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can no longer be reprogrammed, from rethinking), emotions, or free will. !! As there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. !! Artificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence. !! The aim of the theory of artificial consciousness is to ""Define that which would have to be synthesized were consciousness to be found in an engineered artifact"" (Aleksander 1995)."
cognitive robotics,"Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics."
boolean operators,"Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields. !! Short-circuit evaluation, minimal evaluation, or McCarthy evaluation (after John McCarthy) is the semantics of some Boolean operators in some programming languages in which the second argument is executed or evaluated only if the first argument does not suffice to determine the value of the expression: when the first argument of the AND function evaluates to false, the overall value must be false; and when the first argument of the OR function evaluates to true, the overall value must be true."
conditional expression,"In any programming language that implements short-circuit evaluation, the expression x and y is equivalent to the conditional expression if x then y else x, and the expression x or y is equivalent to if x then x else y."
ensemble learning method,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
random forests,"Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration. !! The idea of random subspace selection from Ho was also influential in the design of random forests. !! :587588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. !! With respect to other advanced machine learning approaches, such as artificial neural networks, random forests, or genetic programming, learning classifier systems are particularly well suited to problems that require interpretable solutions. !! An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc. ). !! Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
random decision forests,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
decision trees,"Decision tree learning or induction of decision trees is one of the predictive modelling approaches used in statistics, data mining and machine learning. !! Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. !! Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
training time,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
random forest,"Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration. !! :587588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. !! For classification tasks, the output of the random forest is the class selected by most trees. !! An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc. ). !! Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
classification tasks,"For classification tasks, the output of the random forest is the class selected by most trees. !! Gradient boosting is a machine learning technique used in regression and classification tasks, among others."
gradient boosted trees,":587588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. !! Another useful regularization techniques for gradient boosted trees is to penalize model complexity of the learned model."
automatic memory management,"In computer science, garbage collection (GC) is a form of automatic memory management. !! Memory management within an address space is generally categorized as either manual memory management or automatic memory management. !! The Garbage Collection Handbook: The Art of Automatic Memory Management."
normal discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events."
discriminant function analysis,"In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type. !! Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. !! Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership."
linear discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events."
comparison sort,"A comparison sort must have an average-case lower bound of (n log n) comparison operations, which is known as linearithmic time. !! Counting sort is not a comparison sort; it uses key values as indexes into an array and the (n log n) lower bound for comparison sorting will not apply. !! A metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. !! Non-comparison sorts (such as the examples discussed below) can achieve O(n) performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized). !! There are fundamental limits on the performance of comparison sorts. !! A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a ""less than or equal to"" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list."
comparison sorts,"Comparison sorts may run faster on some lists; many adaptive sorts such as insertion sort run in O(n) time on an already-sorted or nearly-sorted list. !! A metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. !! Non-comparison sorts (such as the examples discussed below) can achieve O(n) performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized). !! There are fundamental limits on the performance of comparison sorts. !! Despite these limitations, comparison sorts offer the notable practical advantage that control over the comparison function allows sorting of many different datatypes and fine control over how the list is sorted."
linearithmic time,"A comparison sort must have an average-case lower bound of (n log n) comparison operations, which is known as linearithmic time."
lookup table,"In computer science, a lookup table (LUT) is an array that replaces runtime computation with a simpler array indexing operation. !! This is often beneficial, since succinct data structures find their uses in large data sets, in which case cache misses become much more frequent and the chances of the lookup table being evicted from closer CPU caches becomes higher. !! Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions. !! Perfect hash functions may be used to implement a lookup table with constant worst-case access time. !! Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input. !! It made sense to reduce expensive read operations by a form of manual caching by creating either static lookup tables (embedded in the program) or dynamic prefetched arrays to contain only the most commonly occurring data items. !! FPGAs also make extensive use of reconfigurable, hardware-implemented, lookup tables to provide programmable hardware functionality."
lookup tables,"Binary search trees allow binary search for fast lookup, addition, and removal of data items, and can be used to implement dynamic sets and lookup tables. !! Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input. !! FPGAs also make extensive use of reconfigurable, hardware-implemented, lookup tables to provide programmable hardware functionality. !! Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions."
complex functions,"Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions."
statistical density functions,"Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions."
dynamic prefetched arrays,It made sense to reduce expensive read operations by a form of manual caching by creating either static lookup tables (embedded in the program) or dynamic prefetched arrays to contain only the most commonly occurring data items.
generic programming,"Generic programming centers around the idea of abstracting from concrete, efficient algorithms to obtain generic algorithms that can be combined with different data representations to produce a wide variety of useful software. !! The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra. !! Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. !! The term ""generic programming"" was originally coined by David Musser and Alexander Stepanov in a more specific sense than the above, to describe a programming paradigm whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above. !! However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type that can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e. g. a pair of iterators pointing to the beginning and end of the subsequence or range to process."
algebraic theories,"The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra."
abstract algebra,"The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra."
data visualization,"To communicate information clearly and efficiently, data visualization uses statistical graphics, plots, information graphics and other tools. !! Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e. g. , points, lines, or bars) contained in graphics. !! Data visualization (often abbreviated data viz) is an interdisciplinary field that deals with the graphic representation of data. !! Data visualization has its roots in the field of statistics and is therefore generally considered a branch of descriptive Statistics. !! Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses. !! Since the graphic design of the mapping can adversely affect the readability of a chart, mapping is a core competency of Data visualization."
graphic representation,Data visualization (often abbreviated data viz) is an interdisciplinary field that deals with the graphic representation of data.
descriptive statistics,"Data visualization has its roots in the field of statistics and is therefore generally considered a branch of descriptive Statistics. !! In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA)."
information graphics,"To communicate information clearly and efficiently, data visualization uses statistical graphics, plots, information graphics and other tools."
visual objects,"Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e. g. , points, lines, or bars) contained in graphics."
graph minor theory,"In this variation of graph minor theory, a graph is always simplified after any edge contraction to eliminate its self-loops and multiple edges."
edge contraction,"In this variation of graph minor theory, a graph is always simplified after any edge contraction to eliminate its self-loops and multiple edges."
rooted tree,"A rooted tree itself has been defined by some authors as a directed graph. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! A rooted forest may be directed, called a directed rooted forest, either making all its edges point away from the root in each rooted treein which case it is called a branching or out-forestor making all its edges point towards the root in each rooted treein which case it is called an anti-branching or in-forest. !! A rooted tree may be directed, called a directed rooted tree, either making all its edges point away from the rootin which case it is called an arborescence or out-treeor making all its edges point towards the rootin which case it is called an anti-arborescence or in-tree. !! A rooted forest is a disjoint union of rooted trees. !! An ordered tree (or plane tree) is a rooted tree in which an ordering is specified for the children of each vertex."
plane tree,An ordered tree (or plane tree) is a rooted tree in which an ordering is specified for the children of each vertex.
ordered tree,"Conversely, given an ordered tree, and conventionally drawing the root at the top, then the child vertices in an ordered tree can be drawn left-to-right, yielding an essentially unique planar embedding. !! An ordered tree (or plane tree) is a rooted tree in which an ordering is specified for the children of each vertex."
child vertices,"Conversely, given an ordered tree, and conventionally drawing the root at the top, then the child vertices in an ordered tree can be drawn left-to-right, yielding an essentially unique planar embedding."
serial manipulator,The main advantage of a serial manipulator is a large workspace with respect to the size of the robot and the floor space it occupies. !! For example when a serial manipulator is fully extended it is in what is known as the boundary singularity. !! A singularity is a configuration of a serial manipulator in which the joint parameters no longer completely define the position and orientation of the end-effector. !! Serial manipulators are the most common industrial robots and they are designed as a series of links connected by motor-actuated joints that extend from a base to an end-effector.
serial manipulators,Serial manipulators are the most common industrial robots and they are designed as a series of links connected by motor-actuated joints that extend from a base to an end-effector.
boundary singularity,For example when a serial manipulator is fully extended it is in what is known as the boundary singularity.
javascript computer programming,Lazy inheritance is a design pattern used in JavaScript computer programming.
server message block,"Server Message Block (SMB) enables file sharing, printer sharing, network browsing, and inter-process communication (through named pipes) over a computer network. !! Server Message Block (SMB) is a communication protocol that Microsoft created for providing shared access to files and printers across nodes on a network. !! [MS-SMB]: Server Message Block (SMB) Protocol. !! Specifies the Server Message Block (SMB) Protocol, which defines extensions to the existing Common Internet File System (CIFS) specification that have been implemented by Microsoft since the publication of the CIFS specification. !! [MS-SMB2]: Server Message Block (SMB) Protocol Versions 2 and 3."
network browsing,"Server Message Block (SMB) enables file sharing, printer sharing, network browsing, and inter-process communication (through named pipes) over a computer network."
decision boundaries,"In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes."
smoothness assumption,"This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms. !! In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes."
clustering algorithms,"This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms. !! As listed above, clustering algorithms can be categorized based on their cluster model. !! Basic mean shift clustering algorithms maintain a set of data points the same size as the input data set. !! ELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms. !! The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms."
generative adversarial networks,"Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy. !! Synthetic media as a field has grown rapidly since the creation of generative adversarial networks, primarily through the rise of deepfakes as well as music synthesis, text generation, human image synthesis, speech synthesis, and more. !! A Style-Based Generator Architecture for Generative Adversarial Networks. !! Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs."
generative models,"They don't necessarily perform better than generative models at classification and regression tasks. !! With the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks. !! Recently, there has been a trend to build very large deep generative models. !! Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs."
regression tasks,They don't necessarily perform better than generative models at classification and regression tasks.
deep neural networks,"With the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks."
Gradient boosting,"The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. !! Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner in an iterative fashion. !! Gradient boosting is a machine learning technique used in regression and classification tasks, among others. !! Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. !! (This section follows the exposition of gradient boosting by Li. )"
gradient boosting,"The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. !! Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner in an iterative fashion. !! Gradient boosting is a machine learning technique used in regression and classification tasks, among others. !! Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. !! (This section follows the exposition of gradient boosting by Li. )"
optimization algorithm,"What optimization-based meta-learning algorithms intend for is to adjust the optimization algorithm so that the model can be good at learning with a few examples. !! The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. !! Many optimization algorithms need to start from a feasible point. !! In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."
explicit regression gradient boosting algorithms,"Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean."
boosting methods,"Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner in an iterative fashion. !! A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function."
bio-inspired computing,"Within computer science, bio-inspired computing relates to artificial intelligence and machine learning. !! Bio-inspired computing uses an evolutionary approach, while traditional A. I. uses a 'creationist' approach. !! Bio-inspired computing, short for biologically inspired computing, is a field of study which seeks to solve computer science problems using models of biology. !! Bio-Inspired computing can be distinguished from traditional artificial intelligence by its approach to computer learning. !! Bio-inspired computing is a major subset of natural computation."
biologically inspired computing,"Bio-inspired computing, short for biologically inspired computing, is a field of study which seeks to solve computer science problems using models of biology."
natural computation,Bio-inspired computing is a major subset of natural computation.
computer learning,Bio-Inspired computing can be distinguished from traditional artificial intelligence by its approach to computer learning.
traditional artificial intelligence,Behavior-based robotics sets itself apart from traditional artificial intelligence by using biological systems as a model. !! Bio-Inspired computing can be distinguished from traditional artificial intelligence by its approach to computer learning.
bit slicing,"Bit slicing, although not called that at the time, was also used in computers before large-scale integrated circuits (LSI, the predecessor to today's VLSI, or very-large-scale integration circuits). !! Bit slicing more or less died out due to the advent of the microprocessor. !! The main advantage was that bit slicing made it economically possible in smaller processors to use bipolar transistors, which switch much faster than NMOS or CMOS transistors. !! In more recent times, the term bit slicing was reused by Matthew Kwan to refer to the technique of using a general-purpose CPU to implement multiple parallel simple virtual machines using general logic instructions to perform single-instruction multiple-data (SIMD) operations. !! Bit slicing is a technique for constructing a processor from modules of processors of smaller bit width, for the purpose of increasing the word length; in theory to make an arbitrary n-bit central processing unit (CPU)."
database tuning,Database tuning describes a group of activities used to optimize and homogenize the performance of a database. !! Database tuning aims to maximize use of system resources to perform work as efficiently and rapidly as possible.
markov random field,"The underlying graph of a Markov random field may be finite or infinite. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! In the domain of physics and probability, a Markov random field (MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. !! In other words, a random field is said to be a Markov random field if it satisfies Markov properties. !! The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model."
undirected graph,"Given an undirected graph with non-negative edge weights and a subset of vertices, usually referred to as terminals, the Steiner tree problem in graphs requires a tree of minimum weight that contains all terminals (but may include additional vertices). !! In the domain of physics and probability, a Markov random field (MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. !! It differs from an ordinary or undirected graph, in that the latter is defined in terms of unordered pairs of vertices, which are usually called edges, links or lines."
undirected graphical model,"In the domain of physics and probability, a Markov random field (MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph."
image processing,"Digital image processing is the use of a digital computer to process digital images through an algorithm. !! Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. !! In image processing, line detection is an algorithm that takes a collection of n edge points and finds all the lines on which these edge points lie. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. !! Automated sign language translationGesture recognition can be conducted with techniques from computer vision and image processing. !! Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. !! The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! Mathematical Morphology and Its Applications to Image Processing, J. Serra and P. Soille (Eds. !! Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems."
world wide web,"Web mining is the application of data mining techniques to discover patterns from the World Wide Web. !! Google Images (previously Google Image Search) is a search engine owned by Google that allows users to search the World Wide Web for images. !! Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications. !! In contrast, the World Wide Web is a global collection of documents and other resources, linked by hyperlinks and URIs. !! Tim Berners-Lee founded the World Wide Web Consortium (W3C) which created XML in 1996 and recommended replacing HTML with stricter XHTML. !! A visual search engine is a search engine designed to search for information on the World Wide Web through the input of an image or a search engine with a visual display of the search results. !! The World Wide Web (WWW), commonly known as the Web, was originally a hypertext document management system accessed over the Internet. !! The terms Internet and World Wide Web are often used without much distinction. !! Web frameworks provide a standard way to build and deploy web applications on the World Wide Web. !! Viewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser or by following a hyperlink to that page or resource. !! One particularly common use for certificate authorities is to sign certificates used in HTTPS, the secure browsing protocol for the World Wide Web."
image search tool,"Google's developers worked on developing this further, and they realized that an image search tool was required to answer ""the most popular search query"" they had seen to date: the green Versace dress of Jennifer Lopez worn in February 2000."
software verification method,Runtime error detection is a software verification method that analyzes a software application as it executes and reports defects that are detected during that execution.
runtime error detection,"Runtime error detection can identify defects that manifest themselves only at runtime (for example, file overwrites) and zeroing in on the root causes of the application crashing, running slowly, or behaving unpredictably. !! Buffer overflowsRuntime error detection tools can only detect errors in the executed control flow of the application. !! Runtime error detection is a software verification method that analyzes a software application as it executes and reports defects that are detected during that execution."
software application,"The concept of reverse computation is somewhat simpler than reversible computing in that reverse computation is only required to restore the equivalent state of a software application, rather than support the reversibility of the set of all possible instructions. !! Reverse computation is a software application of the concept of reversible computing. !! Runtime error detection is a software verification method that analyzes a software application as it executes and reports defects that are detected during that execution."
application crashing,"Runtime error detection can identify defects that manifest themselves only at runtime (for example, file overwrites) and zeroing in on the root causes of the application crashing, running slowly, or behaving unpredictably."
file overwrites,"Runtime error detection can identify defects that manifest themselves only at runtime (for example, file overwrites) and zeroing in on the root causes of the application crashing, running slowly, or behaving unpredictably."
buffer overflowsruntime error detection tools,Buffer overflowsRuntime error detection tools can only detect errors in the executed control flow of the application.
computational intelligence,"The expression computational intelligence (CI) usually refers to the ability of a computer to learn a specific task from data or experimental observation. !! Computational heuristic intelligence (CHI) refers to specialized programming techniques in computational intelligence (also called artificial intelligence, or AI). !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence. !! Generally, computational intelligence is a set of nature-inspired computational methodologies and approaches to address complex real-world problems to which mathematical or traditional modelling can be useless for a few reasons: the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature. !! Computational Intelligence therefore provides solutions for such problems."
soft computing,"Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence. !! A complete overview of FML and related applications can be found in the book titled On the power of Fuzzy Markup Language edited by Giovanni Acampora, Chang-Shing Lee, Vincenzo Loia and Mei-Hui Wang, and published by Springer in the series Studies on Fuzziness and Soft Computing."
associative classifier,An associative classifier (AC) is a kind of supervised learning model that uses association rules to assign a target value.
supervised learning model,An associative classifier (AC) is a kind of supervised learning model that uses association rules to assign a target value.
graphics system,"Primitives are basic units which a graphics system may combine to create more complex images or models. !! In the field of realistic rendering, Japan's Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, for the purpose of rendering realistic 3D computer graphics. !! The 3D Core Graphics System (or Core) was the first graphical standard to be developed."
cholesky decomposition,"The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition. !! For this reason, the LDL decomposition is often called the square-root-free Cholesky decomposition. !! where L is a lower triangular matrix with real and positive diagonal entries, and L* denotes the conjugate transpose of L. Every Hermitian positive-definite matrix (and thus also every real-valued symmetric positive-definite matrix) has a unique Cholesky decomposition. !! Some indefinite matrices for which no Cholesky decomposition exists have an LDL decomposition with negative entries in D: it suffices that the first n1 leading principal minors of A are non-singular. !! When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations. !! A frontal solver builds a LU or Cholesky decomposition of a sparse matrix given as the assembly of element matrices by assembling the matrix and eliminating equations only on a subset of elements at a time. !! In numerical analysis the minimum degree algorithm is an algorithm used to permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, to reduce the number of non-zeros in the Cholesky factor. !! In linear algebra, the Cholesky decomposition or Cholesky factorization (pronounced sh-LES-kee) is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful for efficient numerical solutions, e. g. , Monte Carlo simulations."
multifrontal solver,A multifrontal solver of Duff and Reid is an improvement of the frontal solver that uses several independent fronts at the same time.
fuzzy markup language,"A complete overview of FML and related applications can be found in the book titled On the power of Fuzzy Markup Language edited by Giovanni Acampora, Chang-Shing Lee, Vincenzo Loia and Mei-Hui Wang, and published by Springer in the series Studies on Fuzziness and Soft Computing. !! Fuzzy Markup Language (FML) is a specific purpose markup language based on XML, used for describing the structure and behavior of a fuzzy system independently of the hardware architecture devoted to host and run it. !! Diet assessment based on type-2 fuzzy ontology and fuzzy markup language."
point distribution model,"Point distribution models rely on landmark points. !! The point distribution model concept has been developed by Cootes, Taylor et al. !! The point distribution model is a model for representing the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes."
c4 model,"C4 model documents the architecture of a software system, by showing multiple points of view that explain the decomposition of a system into containers and components, the relationship between these elements, and, where appropriate, the relation with its users. !! C4 model is a lean graphical notation technique for modelling the architecture of software systems. !! For level 1 to 3, the C4 model uses 5 basic diagramming elements: persons, software systems, containers, components and relationships. !! C4 model relies at this level on existing notations such as Unified Modelling Language (UML), Entity Relation Diagrams (ERD) or diagrams generated by Integrated Development Environments (IDE). !! C4 model was created by the software architect Simon Brown between 2006 and 2011 on the roots of Unified Modelling Language (UML) and the 4+1 architectural view model."
software systems,"Temporal logic has found an important application in formal verification, where it is used to state requirements of hardware or software systems. !! Software diagnosis (also: software diagnostics) refers to concepts, techniques, and tools that allow for obtaining findings, conclusions, and evaluations about software systems and their implementation, composition, behaviour, and evolution. !! C4 model is a lean graphical notation technique for modelling the architecture of software systems. !! Service-oriented modeling is the discipline of modeling business and software systems, for the purpose of designing and specifying service-oriented business systems within a variety of architectural styles and paradigms, such as application architecture, service-oriented architecture, microservices, and cloud computing. !! For level 1 to 3, the C4 model uses 5 basic diagramming elements: persons, software systems, containers, components and relationships. !! In the context of hardware and software systems, formal verification is the act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics. !! End-User Development can be defined as a set of methods, techniques, and tools that allow users of software systems, who are acting as non-professional software developers, at some point to create, modify or extend a software artifact."
unified modelling language,"C4 model was created by the software architect Simon Brown between 2006 and 2011 on the roots of Unified Modelling Language (UML) and the 4+1 architectural view model. !! C4 model relies at this level on existing notations such as Unified Modelling Language (UML), Entity Relation Diagrams (ERD) or diagrams generated by Integrated Development Environments (IDE)."
software system,"A debugging pattern describes a generic set of steps to rectify or correct a bug within a software system. !! Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. !! In computer science, program optimization, code optimization, or software optimization is the process of modifying a software system to make some aspect of it work more efficiently or use fewer resources. !! C4 model documents the architecture of a software system, by showing multiple points of view that explain the decomposition of a system into containers and components, the relationship between these elements, and, where appropriate, the relation with its users. !! In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. !! A software metric is a standard of measure of a degree to which a software system or process possesses some property. !! In computing, the Distributed Computing Environment (DCE) software system was developed in the early 1990s from the work of the Open Software Foundation (OSF), a consortium (founded in 1988) that included Apollo Computer (part of Hewlett-Packard from 1989), IBM, Digital Equipment Corporation, and others."
memory management unit,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU. !! A memory management unit (MMU), sometimes called paged memory management unit (PMMU), is a computer hardware unit having all memory references passed through itself, primarily performing the translation of virtual memory addresses to physical addresses. !! However, the TLB cache is part of the memory management unit (MMU) and not directly related to the CPU caches."
virtual memory addresses,"A memory management unit (MMU), sometimes called paged memory management unit (PMMU), is a computer hardware unit having all memory references passed through itself, primarily performing the translation of virtual memory addresses to physical addresses."
computer hardware unit,"A memory management unit (MMU), sometimes called paged memory management unit (PMMU), is a computer hardware unit having all memory references passed through itself, primarily performing the translation of virtual memory addresses to physical addresses."
memory allocation,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU. !! For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it."
memory protection,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU."
memory sharing,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU."
programming language semantics,"In programming language semantics, normalisation by evaluation (NBE) is a style of obtaining the normal form of terms in the -calculus by appealing to their denotational semantics."
tridiagonal matrix algorithm,"The derivation of the tridiagonal matrix algorithm is a special case of Gaussian elimination. !! In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations. !! This can be done efficiently if both solutions are computed at once, as the forward portion of the pure tridiagonal matrix algorithm can be shared."
gaussian elimination,"The derivation of the tridiagonal matrix algorithm is a special case of Gaussian elimination. !! In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations. !! LU decomposition can be viewed as the matrix form of Gaussian elimination."
systems theory,"Systems design could be seen as the application of systems theory to product development. !! Algorithmic art, also known as computer-generated art, is a subset of generative art (generated by an autonomous system) and is related to systems art (influenced by systems theory)."
sorted binary tree,"In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree data structure whose internal nodes each store a key greater than all the keys in the node's left subtree and less than those in its right subtree."
first-order reduction,"A first-order reduction is a reduction where each component is restricted to be in the class FO of problems calculable in first-order logic. !! Many important complexity classes are closed under first-order reductions, and many of the traditional complete problems are first-order complete as well (Immerman 1999 p. 49-50). !! , the first-order reductions are stronger reductions than the logspace reductions. !! In computer science, a first-order reduction is a very strong type of reduction between two computational problems in computational complexity theory."
logspace reductions,", the first-order reductions are stronger reductions than the logspace reductions."
deterministic encryption scheme,"A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm."
deterministic encryption,"While deterministic encryption schemes can never be semantically secure, they have some advantages over probabilistic schemes. !! Examples of deterministic encryption algorithms include RSA cryptosystem (without encryption padding), and many block ciphers when used in ECB mode or with a constant initialization vector. !! One primary motivation for the use of deterministic encryption is the efficient searching of encrypted data. !! Deterministic encryption can leak information to an eavesdropper, who may recognize known ciphertexts. !! A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm."
probabilistic encryption scheme,"A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm."
encryption algorithm,"A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm. !! Dynamic Encryption is a cryptographic principle that enables two parties to change the encryption algorithm for every transaction."
ecb mode,"Examples of deterministic encryption algorithms include RSA cryptosystem (without encryption padding), and many block ciphers when used in ECB mode or with a constant initialization vector."
deterministic encryption schemes,"While deterministic encryption schemes can never be semantically secure, they have some advantages over probabilistic schemes."
encrypted data,One primary motivation for the use of deterministic encryption is the efficient searching of encrypted data.
visual programming languages,"Visual Programming Languages - Snapshots !! Some graphical parsing algorithms have been designed for visual programming languages. !! The following contains a list of notable visual programming languages. !! A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output. !! Godot game engine allows game scripts and graphics shaders to be built using node-graph visual programming languages. !! languages of the Microsoft Visual Studio IDE are not visual programming languages: the representation of algorithms etc. !! Parsers for visual programming languages can be implemented using graph grammars."
graphical program elements,"A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output."
spectral methods,"Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains. !! Partially for this reason, spectral methods have excellent error properties, with the so-called ""exponential convergence"" being the fastest possible, when the solution is smooth. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform. !! Spectral methods can be used to solve ordinary differential equations (ODEs), partial differential equations (PDEs) and eigenvalue problems involving differential equations."
fast fourier transform,"Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform."
spectral method,"In other words, spectral methods take on a global approach while finite element methods use a local approach. !! Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains. !! Partially for this reason, spectral methods have excellent error properties, with the so-called ""exponential convergence"" being the fastest possible, when the solution is smooth. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform. !! Spectral methods can be used to solve ordinary differential equations (ODEs), partial differential equations (PDEs) and eigenvalue problems involving differential equations."
applied mathematics,"Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. !! Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set. !! Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering. !! Geometric modeling is a branch of applied mathematics and computational geometry that studies methods and algorithms for the mathematical description of shapes. !! Structural analysis employs the fields of applied mechanics, materials science and applied mathematics to compute a structure's deformations, internal forces, stresses, support reactions, accelerations, and stability. !! Polynomial sequences are a topic of interest in enumerative combinatorics and algebraic combinatorics, as well as applied mathematics. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform."
finite element methods,"Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains."
exponential convergence,"Partially for this reason, spectral methods have excellent error properties, with the so-called ""exponential convergence"" being the fastest possible, when the solution is smooth."
advanced computing environment,"The Advanced Computing Environment (ACE) was defined by an industry consortium in the early 1990s to be the next generation commodity computing platform, the successor to personal computers based on Intel's 32-bit instruction set architecture."
affective computing,"Using affective computing technology, computers can judge the learners' affection and learning state by recognizing their facial expressions. !! While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing and her book Affective Computing published by MIT Press. !! Another area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. !! Affectiva is a company (co-founded by Rosalind Picard and Rana El Kaliouby) directly related to affective computing and aims at investigating solutions and software for facial affect detection. !! Artificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic. !! Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects."
gradient boosting algorithms,"Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean."
artificial intelligent agents,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents."
computational ethics,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents."
machine ethics,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents. !! Although the definition of ""Machine Ethics"" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI Magazine article ""A Question of Responsibility"":""However, one thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. !! Perhaps what we need is, in fact, a theory and practice of machine ethics, in the spirit of Asimovs three laws of robotics. "" !! Machine ethics should not be confused with computer ethics, which focuses on human use of computers. !! Machine ethics differs from other ethical fields related to engineering and technology."
computational morality,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents."
computer ethics,"Machine ethics should not be confused with computer ethics, which focuses on human use of computers."
intelligent machines,"Although the definition of ""Machine Ethics"" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI Magazine article ""A Question of Responsibility"":""However, one thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. !! Oriented energy filters are used to grant sight to intelligent machines and sensors."
monte carlo algorithm,"Las Vegas algorithms are a dual of Monte Carlo algorithms that never return an incorrect answer. !! While the answer returned by a deterministic algorithm is always expected to be correct, this is not the case for Monte Carlo algorithms. !! If there is a procedure for verifying whether the answer given by a Monte Carlo algorithm is correct, and the probability of a correct answer is bounded above zero, then with probability one running the algorithm repeatedly while testing the answers will eventually give a correct answer. !! Two examples of such algorithms are KargerStein algorithm and Monte Carlo algorithm for minimum Feedback arc set. !! In computing, a Monte Carlo algorithm is a randomized algorithm whose output may be incorrect with a certain (typically small) probability."
kargerstein algorithm,Two examples of such algorithms are KargerStein algorithm and Monte Carlo algorithm for minimum Feedback arc set.
las vegas algorithms,Las Vegas algorithms are a dual of Monte Carlo algorithms that never return an incorrect answer.
monte carlo algorithms,"While the answer returned by a deterministic algorithm is always expected to be correct, this is not the case for Monte Carlo algorithms. !! Las Vegas algorithms are a dual of Monte Carlo algorithms that never return an incorrect answer."
deterministic algorithm,"While the answer returned by a deterministic algorithm is always expected to be correct, this is not the case for Monte Carlo algorithms. !! A few routing algorithms do not use a deterministic algorithm to find the best link for a packet to get from its original source to its final destination."
internal network,Remote access policy is a document which outlines and defines acceptable methods of remotely connecting to the internal network.
organizational network,Cable modemThis remote access policy defines standards for connecting to the organizational network and security standards for computers that are allowed to connect to the organizational network.
unrolled linked list,"One of the primary benefits of unrolled linked lists is decreased storage requirements. !! In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. !! Because unrolled linked list nodes each store a count next to the next field, retrieving the kth element of an unrolled linked list (indexing) can be done in n/m + 1 cache misses, up to a factor of m better than ordinary linked lists. !! Moreover, many popular memory allocators will keep a small amount of metadata for each node allocated, increasing the effective overhead v. Both of these make unrolled linked lists more attractive. !! Unrolled linked lists effectively spread the overhead v over a number of elements of the list."
linked list,"They can be used to implement several other common abstract data types, including lists, stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement those data structures directly without using a linked list as the basis. !! In computer science, a linked list is a linear collection of data elements whose order is not given by their physical placement in memory. !! Linked lists are among the simplest and most common data structures. !! Arrays have better cache locality compared to linked lists. !! In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. !! A drawback of linked lists is that access time is linear (and difficult to pipeline)."
unrolled linked lists,One of the primary benefits of unrolled linked lists is decreased storage requirements.
character encoding,"Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode. !! Unicode, a well defined and extensible encoding system, has supplanted most earlier character encodings, but the path of code development to the present is fairly well known. !! Character encoding is the process of assigning numbers to graphical characters, especially the written characters of human language, allowing them to be stored, transmitted, and transformed using digital computers. !! The numerical values that make up a character encoding are known as ""code points"" and collectively comprise a ""code space"", a ""code page"", or a ""character map"". !! Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form."
graphical characters,"Character encoding is the process of assigning numbers to graphical characters, especially the written characters of human language, allowing them to be stored, transmitted, and transformed using digital computers."
character map,"The numerical values that make up a character encoding are known as ""code points"" and collectively comprise a ""code space"", a ""code page"", or a ""character map""."
code points,"The numerical values that make up a character encoding are known as ""code points"" and collectively comprise a ""code space"", a ""code page"", or a ""character map""."
code page,"The numerical values that make up a character encoding are known as ""code points"" and collectively comprise a ""code space"", a ""code page"", or a ""character map""."
code space,"The numerical values that make up a character encoding are known as ""code points"" and collectively comprise a ""code space"", a ""code page"", or a ""character map""."
information interchange,"Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode."
baudot code,"Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode."
code development,"Unicode, a well defined and extensible encoding system, has supplanted most earlier character encodings, but the path of code development to the present is fairly well known."
topological quantum computer,"Various topologically ordered states have interesting properties, such as (1) topological degeneracy and fractional statistics or non-abelian statistics that can be used to realize a topological quantum computer; (2) perfect conducting edge states that may have important device applications; (3) emergent gauge field and Fermi statistics that suggest a quantum information origin of elementary particles; (4) topological entanglement entropy that reveals the entanglement origin of topological order, etc."
optimization software,"ASTOS AeroSpace Trajectory Optimization Software for launcher, re-entry and generic aerospace problems. !! The use of optimization software requires that the function f is defined in a suitable programming language and connected at compile or run time to the optimization software. !! The optimization software will deliver input values in A, the software module realizing f will deliver the computed value f(x) and, in some cases, additional information about the function like derivatives. !! The following tables provide a list of notable optimization software organized according to license and business model type. !! The Unscrambler X product formulation and process optimization software."
process optimization software,The Unscrambler X product formulation and process optimization software.
vector quantization,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster."
cluster centers,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster."
k-means clustering,", xn), where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into k ( n) sets S = {S1, S2, . !! k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. !! The Spherical k-means clustering algorithm is suitable for textual data. !! k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. !! They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes."
squared euclidean distances,"k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances."
textual data,The Spherical k-means clustering algorithm is suitable for textual data.
stochastic roadmap simulation,"Stochastic roadmap simulation is used to explore the kinetics of molecular motion by simultaneously examining multiple pathways in the roadmap. !! Ensemble properties of molecular motion (e. g. , probability of folding (PFold), escape time in ligand-protein binding) is computed efficiently and accurately with stochastic roadmap simulation. !! Stochastic roadmap simulation is inspired by probabilistic roadmap methods (PRM) developed for robot motion planning."
training data,"A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. !! The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e. g. weights of connections between neurons in artificial neural networks) of the model. !! In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). !! The model (e. g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. !! In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries. !! The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. !! The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e. g. the number of hidden unitslayers and layer widthsin a neural network)."
lazy learning,"In practice, as stated earlier, lazy learning is applied to situations where any learning performed in advance soon becomes obsolete because of changes in the data. !! The main advantage gained in employing a lazy learning method is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm. !! In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries. !! The primary motivation for employing lazy learning, as in the K-nearest neighbors algorithm, used by online recommendation systems (""people who viewed/purchased/listened to this movie/item/tune also . !! Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain."
eager learning,"In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries."
nearest neighbor algorithm,"The main advantage gained in employing a lazy learning method is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm."
primitive recursive function,"The importance of primitive recursive functions lies on the fact that most computable functions that are studied in number theory (and more generally in mathematics) are primitive recursive. !! In fact, for showing that a computable function is primitive recursive, it suffices to show that its time complexity is bounded above by a primitive recursive function of the input size. !! Primitive recursive functions form a strict subset of those general recursive functions that are also total functions. !! The set of primitive recursive functions is known as PR in computational complexity theory. !! In computability theory, a primitive recursive function is roughly speaking a function that can be computed by a computer program whose loops are all ""for"" loops (that is, an upper bound of the number of iterations of every loop can be determined before entering the loop)."
computability theory,"Computable functions are the basic objects of study in computability theory. !! Computability theory originated in the 1930s, with work of Kurt Gdel, Alonzo Church, Rzsa Pter, Alan Turing, Stephen Kleene, and Emil Post. !! In these areas, computability theory overlaps with proof theory and effective descriptive set theory. !! Models of Computation: An Introduction to Computability Theory. !! The c. e. sets, although not decidable in general, have been studied in detail in computability theory. !! In computability theory, the theory of real computation deals with hypothetical computing machines using infinite-precision real numbers. !! In computability theory and computational complexity theory, an undecidable problem is a decision problem for which it is proved to be impossible to construct an algorithm that always leads to a correct yes-or-no answer. !! The main form of computability studied in computability theory was introduced by Turing (1936). !! In mathematics and computer science, computable analysis is the study of mathematical analysis from the perspective of computability theory. !! In computability theory, a primitive recursive function is roughly speaking a function that can be computed by a computer program whose loops are all ""for"" loops (that is, an upper bound of the number of iterations of every loop can be determined before entering the loop). !! Thus, android epistemology incorporates artificial intelligence, computational cognitive psychology, computability theory and other related disciplines. !! Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees."
general recursive functions,Particular models of computability that give rise to the set of computable functions are the Turing-computable functions and the general recursive functions. !! Primitive recursive functions form a strict subset of those general recursive functions that are also total functions.
computable functions,"The importance of primitive recursive functions lies on the fact that most computable functions that are studied in number theory (and more generally in mathematics) are primitive recursive. !! Computable functions are the basic objects of study in computability theory. !! Particular models of computability that give rise to the set of computable functions are the Turing-computable functions and the general recursive functions. !! Computable functions are the formalized analogue of the intuitive notion of algorithms, in the sense that a function is computable if there exists an algorithm that can do the job of the function, i. e. given an input of the function domain it can return the corresponding output. !! Computable functions are used to discuss computability without referring to any concrete model of computation such as Turing machines or register machines. !! Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees."
computable function,"Computable functions are the basic objects of study in computability theory. !! In fact, for showing that a computable function is primitive recursive, it suffices to show that its time complexity is bounded above by a primitive recursive function of the input size. !! Particular models of computability that give rise to the set of computable functions are the Turing-computable functions and the general recursive functions. !! Computable functions are the formalized analogue of the intuitive notion of algorithms, in the sense that a function is computable if there exists an algorithm that can do the job of the function, i. e. given an input of the function domain it can return the corresponding output. !! Computable functions are used to discuss computability without referring to any concrete model of computation such as Turing machines or register machines. !! Before the precise definition of computable function, mathematicians often used the informal term effectively calculable."
primitive recursive functions,The set of primitive recursive functions is known as PR in computational complexity theory.
activation function,"A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input. !! In biologically inspired neural networks, the activation function is usually an abstraction representing the rate of action potential firing in the cell. !! However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities. !! In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. !! The most common activation functions can be divided in three categories: ridge functions, radial functions and fold functions."
digital network,"A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input."
activation functions,"However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities. !! A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input."
fold functions,"The most common activation functions can be divided in three categories: ridge functions, radial functions and fold functions."
radial functions,"The most common activation functions can be divided in three categories: ridge functions, radial functions and fold functions."
ridge functions,"The most common activation functions can be divided in three categories: ridge functions, radial functions and fold functions."
file exchange protocol,File eXchange Protocol (FXP or FXSP) is a method of data transfer which uses FTP to transfer data from one remote server to another (inter-server) without routing this data through the client's connection.
markov chain monte carlo,"Markov chain Monte Carlo methods that change dimensionality have long been used in statistical physics applications, where for some problems a distribution that is a grand canonical ensemble is used (e. g. , when the number of molecules in a box is variable). !! Markov chain Monte Carlo methods create samples from a continuous random variable, with probability density proportional to a known function. !! But the reversible-jump variant is useful when doing Markov chain Monte Carlo or Gibbs sampling over nonparametric Bayesian models such as those involving the Dirichlet process or Chinese restaurant process, where the number of mixing components/clusters/etc. !! In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! In principle, any Markov chain Monte Carlo sampler can be turned into an interacting Markov chain Monte Carlo sampler."
dirichlet process,"But the reversible-jump variant is useful when doing Markov chain Monte Carlo or Gibbs sampling over nonparametric Bayesian models such as those involving the Dirichlet process or Chinese restaurant process, where the number of mixing components/clusters/etc."
nonparametric bayesian models,"But the reversible-jump variant is useful when doing Markov chain Monte Carlo or Gibbs sampling over nonparametric Bayesian models such as those involving the Dirichlet process or Chinese restaurant process, where the number of mixing components/clusters/etc."
markov chain monte carlo sampler,"In principle, any Markov chain Monte Carlo sampler can be turned into an interacting Markov chain Monte Carlo sampler."
memory dependence prediction,"Besides using store to load (RAW or true) memory dependence prediction for the out-of-order scheduling of loads and stores, other applications of memory dependence prediction have been proposed. !! Memory dependence prediction is a technique, employed by high-performance out-of-order execution microprocessors that execute memory access operations (loads and stores) out of program order, to predict true dependencies between loads and stores at instruction execution time. !! Selective memory dependence prediction stalls specific loads until it is certain that no violation may occur. !! Memory dependence prediction is an optimization on top of memory dependency speculation. !! In general, memory dependence prediction predicts whether two memory operations are dependent, that is, if they interact by accessing the same memory location."
recursive algorithm,"Recursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. !! In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. !! For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the input becomes small. !! Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort. !! Many well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it."
recursive algorithms,"Recursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns."
hybrid recursive algorithms,"Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort."
hybrid merge sort,"Hybrid recursive algorithms can often be further refined, as in Timsort, derived from a hybrid merge sort/insertion sort."
stack space,"In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms."
iterative algorithms,"In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms."
call stack,"A stack machine has 2 or more stack registers one of them keeps track of a call stack, the other(s) keep track of other stack(s). !! A stack register is a computer central processor register whose purpose is to keep track of a call stack. !! In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms."
stochastic matrix,"A right stochastic matrix is a real square matrix, with each row summing to 1. !! :911 The stochastic matrix was first developed by Andrey Markov at the beginning of the 20th century, and has found use throughout a wide variety of scientific fields, including probability theory, statistics, mathematical finance and linear algebra, as well as computer science and population genetics. !! A doubly stochastic matrix is a square matrix of nonnegative real numbers with each row and column summing to 1. !! In mathematics, a stochastic matrix is a square matrix used to describe the transitions of a Markov chain. !! A left stochastic matrix is a real square matrix, with each column summing to 1."
right stochastic matrix,"A right stochastic matrix is a real square matrix, with each row summing to 1."
left stochastic matrix,"A left stochastic matrix is a real square matrix, with each column summing to 1."
jacobi iteration method,"In mathematics, the Jacobi method for complex Hermitian matrices is a generalization of the Jacobi iteration method."
complex hermitian matrices,"In mathematics, the Jacobi method for complex Hermitian matrices is a generalization of the Jacobi iteration method. !! However the complex Hermitian matrices do form a vector space over the real numbers R. In the 2n2-dimensional vector space of complex nn matrices over R, the complex Hermitian matrices form a subspace of dimension n2."
jacobi method,"In mathematics, the Jacobi method for complex Hermitian matrices is a generalization of the Jacobi iteration method."
factory method pattern,"In a software design pattern view, lazy initialization is often used together with a factory method pattern. !! In class-based programming, the factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created."
Factory method,"Create an object by calling a factory method. !! In class-based programming, the factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created. !! This is done by creating objects by calling a factory methodeither specified in an interface and implemented by child classes, or implemented in a base class and optionally overridden by derived classesrather than by calling a constructor. !! The Factory method lets a class defer instantiation it uses to subclasses. "" !! Define a separate operation (factory method) for creating an object."
creational pattern,"In class-based programming, the factory method pattern is a creational pattern that uses factory methods to deal with the problem of creating objects without having to specify the exact class of the object that will be created."
factory method,Create an object by calling a factory method. !! Define a separate operation (factory method) for creating an object.
temporal sequence,A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence.
convolutional neural network,"or in other words ""A convolutional neural network (CNN) is a type of artificial neural network used in image recognition and processing that is specifically designed to process pixel data. "" !! Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. !! The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. !! In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of Artificial Neural Network(ANN), most commonly applied to analyze visual imagery. !! A convolutional neural network consists of an input layer, hidden layers and an output layer. !! The name ""convolutional neural network"" indicates that the network employs a mathematical operation called convolution."
uri normalization,URI normalization is the process by which URIs are modified and standardized in a consistent manner. !! Web crawlers perform URI normalization in order to avoid crawling the same resource more than once. !! Search engines employ URI normalization in order to assign importance to web pages and to reduce indexing of duplicate pages.
web pages,"Metadata web indexing involves assigning keywords, description or phrases to web pages or web sites within a metadata tag (or ""meta-tag"") field, so that the web page or web site can be retrieved with a list. !! Search engines employ URI normalization in order to assign importance to web pages and to reduce indexing of duplicate pages."
quantum programming,"Quantum programming languages help express quantum algorithms using high-level constructs. !! There are two main groups of quantum programming languages: imperative quantum programming languages and functional quantum programming languages. !! A quantum programming environment and optimizing compiler developed by Cambridge Quantum Computing that targets simulators and several quantum hardware back-ends, released in December 2018. !! Quantum Computation Language (QCL) is one of the first implemented quantum programming languages. !! Quantum programming is the process of assembling sequences of instructions, called quantum programs, that are capable of running on a quantum computer."
quantum programming languages,There are two main groups of quantum programming languages: imperative quantum programming languages and functional quantum programming languages.
functional quantum programming languages,There are two main groups of quantum programming languages: imperative quantum programming languages and functional quantum programming languages.
imperative quantum programming languages,There are two main groups of quantum programming languages: imperative quantum programming languages and functional quantum programming languages.
quantum computation language,Quantum Computation Language (QCL) is one of the first implemented quantum programming languages.
search algorithm,"Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes. !! In computer science, the min-conflicts algorithm is a search algorithm or heuristic method to solve constraint satisfaction problems. !! In computer science, binary search, also known as half-interval search, logarithmic search, or binary chop, is a search algorithm that finds the position of a target value within a sorted array. !! Search algorithms work to retrieve information stored within some data structure, or calculated in the search space of a problem domain, with either discrete or continuous values. !! If an admissible heuristic is used in an algorithm that, per iteration, progresses only the path of lowest evaluation (current cost + heuristic) of several candidate paths, terminates the moment it's exploration reaches the goal and, crucially, never closes all optimal paths before terminating (something that's possible with A* search algorithm if special care isn't taken), then this algorithm can only terminate on an optimal path. !! Classic search algorithms are typically evaluated on how fast they can find a solution, and whether that solution is guaranteed to be optimal. !! In computer science, a search algorithm is an algorithm (typically involving a multitude of other, more specific algorithms ) which solves a search problem. !! The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. !! The A* search algorithm is an example of a best-first search algorithm, as is B*. !! Several pathfinding algorithms, including Dijkstra's algorithm and the A* search algorithm, internally build a spanning tree as an intermediate step in solving the problem."
data structure,"In computer science, a binomial heap is a data structure that acts as a priority queue but also allows pairs of heaps to be merged. !! In computer science, a search data structure is any data structure that allows the efficient retrieval of specific items from a set of items, such as a specific record from a database. !! In computer science, a radix tree (also radix trie or compact prefix tree) is a data structure that represents a space-optimized trie (prefix tree) in which each node that is the only child is merged with its parent. !! On the other hand, a data dictionary is a data structure that stores metadata, i. e. , (structured) data about information. !! Search algorithms work to retrieve information stored within some data structure, or calculated in the search space of a problem domain, with either discrete or continuous values. !! Bit manipulation, in some cases, can obviate or reduce the need to loop over a data structure and can give many-fold speed ups, as bit manipulations are processed in parallel. !! In computer science, a binary decision diagram (BDD) or branching program is a data structure that is used to represent a Boolean function. !! Algorithmic information theory (AIT) is a branch of theoretical computer science that concerns itself with the relationship between computation and information of computably generated objects (as opposed to stochastically generated), such as strings or any other data structure. !! In computer science, a Fibonacci heap is a data structure for priority queue operations, consisting of a collection of heap-ordered trees. !! In computer science, a Range Query Tree, or RQT, is a term for referring to a data structure that is used for performing range queries and updates on an underlying array, which is treated as the leaves of the tree. !! In computing and graph theory, a dynamic connectivity structure is a data structure that dynamically maintains information about the connected components of a graph. !! A radix heap is a data structure for realizing the operations of a monotone priority queue. !! The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. !! A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. !! In computer science, a succinct data structure is a data structure which uses an amount of space that is ""close"" to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations."
classic search algorithms,"Classic search algorithms are typically evaluated on how fast they can find a solution, and whether that solution is guaranteed to be optimal."
search algorithms,"Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes. !! Binary search trees are also efficacious in sorting algorithms and search algorithms. !! Search algorithms work to retrieve information stored within some data structure, or calculated in the search space of a problem domain, with either discrete or continuous values. !! Classic search algorithms are typically evaluated on how fast they can find a solution, and whether that solution is guaranteed to be optimal. !! Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, allow significantly faster searching for all but short lists. !! Best-first search is a class of search algorithms, which explore a graph by expanding the most promising node chosen according to a specified rule. !! Linear search algorithms check every record for the one associated with a target key in a linear fashion. !! In prune and search algorithms S(n) is typically at least linear (since the whole input must be processed). !! Search algorithms can be classified based on their mechanism of searching into three types of algorithms: linear, binary, and hashing."
search trees,"Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes. !! Binary search trees are also efficacious in sorting algorithms and search algorithms. !! Binary search trees support three main operations: lookup (checking whether a key is present), insertion, and deletion of an element. !! :299-302 Binary search trees are also a fundamental data structure used in construction of abstract data structures such as sets, multisets, and associative arrays. !! Binary search trees allow binary search for fast lookup, addition, and removal of data items, and can be used to implement dynamic sets and lookup tables. !! Binary search trees are used in implementing priority queues, using the element or node's key as priorities."
hash maps,"Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes."
database indexes,"Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes."
rational data type,"Languages that support a rational data type usually provide special syntax for building such values, and also extend the basic arithmetic operations ('+', '', '', '/', integer powers) and comparisons ('=', '<', '>', '') to act on them either natively or through operator overloading facilities provided by the language. !! Some programming languages provide a built-in (primitive) rational data type to represent rational numbers like 1/3 and -11/17 without rounding, and to do arithmetic on them."
xml data,"eXtensible Text Framework (XTF) is a programming and data representation framework created and maintained by the California Digital Library (CDL) based on XML data, XSLT 2."
extensible text framework,"eXtensible Text Framework (XTF) is a programming and data representation framework created and maintained by the California Digital Library (CDL) based on XML data, XSLT 2."
semantic hashing,Semantic hashing is a technique that attempts to map input items to addresses such that closer inputs have higher semantic similarity.
bit manipulation,"Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization. !! Bit manipulation, in some cases, can obviate or reduce the need to loop over a data structure and can give many-fold speed ups, as bit manipulations are processed in parallel. !! Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields. !! Bit manipulation is the act of algorithmically manipulating bits or other pieces of data shorter than a word. !! Bit twiddling and bit bashing are often used interchangeably with bit manipulation, but sometimes exclusively refer to clever or non-obvious ways or uses of bit manipulation, or tedious or challenging low-level device control data manipulation tasks."
correction algorithms,"Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization."
encryption algorithms,"Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization."
test bits,"Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields."
zero fields,"Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields."
bit manipulations,"Bit manipulation, in some cases, can obviate or reduce the need to loop over a data structure and can give many-fold speed ups, as bit manipulations are processed in parallel."
bit twiddling,"Bit twiddling and bit bashing are often used interchangeably with bit manipulation, but sometimes exclusively refer to clever or non-obvious ways or uses of bit manipulation, or tedious or challenging low-level device control data manipulation tasks."
bit bashing,"Bit twiddling and bit bashing are often used interchangeably with bit manipulation, but sometimes exclusively refer to clever or non-obvious ways or uses of bit manipulation, or tedious or challenging low-level device control data manipulation tasks."
parsimonious reductions,"Parsimonious reductions are commonly used in computational complexity for proving the hardness of counting problems, for counting complexity classes such as #P. Additionally, they are used in game complexity, as a way to design hard puzzles that have a unique solution, as many types of puzzles require. !! Just as many-one reductions are important for proving NP-completeness, parsimonious reductions are important for proving completeness for counting complexity classes such as P."
transformation algorithm,Specific types of parsimonious reductions may be defined by the computational complexity or other properties of the transformation algorithm.
functional presence engine,"The first Functional Presence Engine was deployed in 2001 by Spectre AI Incorporated. !! A Functional Presence Engine, or FPE, is a probabilistic parsing mechanism that uses at least four components to respond to input patterns. !! A Functional Presence Engines is, subsequently, a stimulus-response mechanism that allows for a higher variability of inputs to elicit response patterns with a high likelihood of correctness, even from incomplete training."
functional presence engines,"A Functional Presence Engines is, subsequently, a stimulus-response mechanism that allows for a higher variability of inputs to elicit response patterns with a high likelihood of correctness, even from incomplete training."
stack register,"A stack machine has 2 or more stack registers one of them keeps track of a call stack, the other(s) keep track of other stack(s). !! In 8086, the main stack register is called stack pointer - SP. !! A stack register is a computer central processor register whose purpose is to keep track of a call stack."
stack machine,"A stack machine has 2 or more stack registers one of them keeps track of a call stack, the other(s) keep track of other stack(s)."
main stack register,"In 8086, the main stack register is called stack pointer - SP."
amdahl's law,"Amdahl's law does represent the law of diminishing returns if on considering what sort of return one gets by adding more processors to a machine, if one is running a fixed-size computation that will use all available processors to their capacity. !! In computer architecture, Amdahl's law (or Amdahl's argument) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved. !! Amdahl's law is often conflated with the law of diminishing returns, whereas only a special case of applying Amdahl's law demonstrates law of diminishing returns. !! Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors. !! Amdahl's law applies only to the cases where the problem size is fixed."
parallel computing,"Amdahl's law is often used in parallel computing to predict the theoretical speedup when using multiple processors. !! Automatic vectorization, in parallel computing, is a special case of automatic parallelization, where a computer program is converted from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once."
computer interface,A peripheral nerve interface is the bridge between the peripheral nervous system and a computer interface which serves as a bidirectional information transducer recording and sending signals between the human body and a machine processor. !! Brain painting is a non-invasive P300-based brain-computer interface (BCI) that allows painting without the use of muscular activity.
machine processor,A peripheral nerve interface is the bridge between the peripheral nervous system and a computer interface which serves as a bidirectional information transducer recording and sending signals between the human body and a machine processor.
quantum information theory,"This was the first historical appearance of quantum information theory. !! Although the logic has also been studied for its own sake, more broadly, ideas from linear logic have been influential in fields such as programming languages, game semantics, and quantum physics (because linear logic can be seen as the logic of quantum information theory), as well as linguistics, particularly because of its emphasis on resource-boundedness, duality, and interaction. !! The theorem was one of the earliest results of quantum information theory. !! Despite all the excitement and interest over studying isolated quantum systems and trying to find a way to circumvent the theory of relativity, research in quantum information theory became stagnant in the 1980s. !! It is the basic entity of study in quantum information theory, and can be manipulated using quantum information processing techniques. !! The history of quantum information theory began at the turn of the 20th century when classical physics was revolutionized into quantum physics."
image recognition,"or in other words ""A convolutional neural network (CNN) is a type of artificial neural network used in image recognition and processing that is specifically designed to process pixel data. """
input layer,"A convolutional neural network consists of an input layer, hidden layers and an output layer."
output layer,"A convolutional neural network consists of an input layer, hidden layers and an output layer."
iterative refinement,"As a rule of thumb, iterative refinement for Gaussian elimination produces a solution correct to working precision if double the working precision is used in the computation of r, e. g. by using quad or double extended precision IEEE 754 floating point, and if A is not too ill-conditioned (and the iteration and the rate of convergence are determined by the condition number of A). !! The truncated SPIKE algorithm can be wrapped inside some outer iterative scheme (e. g. , BiCGSTAB or iterative refinement) to improve the accuracy of the solution. !! Iterative refinement is an iterative method proposed by James H. Wilkinson to improve the accuracy of numerical solutions to systems of linear equations."
polynomial time algorithm,"is a polynomial time algorithm. !! and is a polynomial time algorithm. !! In general, the numeric value of the input is exponential in the input length, which is why a pseudo-polynomial time algorithm does not necessarily run in polynomial time with respect to the input length."
quasilinear time algorithms,"Quasilinear time algorithms are also O(n1+) for every constant > 0, and thus run faster than any polynomial time algorithm whose time bound includes a term nc for any c > 1."
knapsack problem,"Typical combinatorial optimization problems are the travelling salesman problem (""TSP""), the minimum spanning tree problem (""MST""), and the knapsack problem. !! SSP is a special case of the knapsack problem and of the multiple subset sum problem."
algorithm theory,"Combinatorial optimization is related to operations research, algorithm theory, and computational complexity theory."
matroid problems,"Some examples of combinatorial optimization problems that are covered by this framework are shortest paths and shortest-path trees, flows and circulations, spanning trees, matching, and matroid problems."
spanning trees,"Some examples of combinatorial optimization problems that are covered by this framework are shortest paths and shortest-path trees, flows and circulations, spanning trees, matching, and matroid problems."
relational model,"The most popular example of a database model is the relational model, which uses a table-based format. !! A relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970. !! The purpose of the relational model is to provide a declarative method for specifying data and queries: users directly state what information the database contains and what information they want from it, and let the database management system software take care of describing data structures for storing the data and retrieval procedures for answering queries. !! However, SQL databases deviate from the relational model in many details, and Codd fiercely argued against deviations that compromise the original principles. !! Most relational databases use the SQL data definition and query language; these systems implement what can be regarded as an engineering approximation to the relational model. !! The relational model (RM) for database management is an approach to managing data using a structure and language consistent with first-order predicate logic, first described in 1969 by English computer scientist Edgar F. Codd, where all data is represented in terms of tuples, grouped into relations. !! An alternative definition for a relational database management system is a database management system (DBMS) based on the relational model. !! A database organized in terms of the relational model is a relational database. !! Modern decision, and classical statistical databases are often closer to the relational model than the multidimensional model commonly used in OLAP systems today."
combinatorial optimization problems,"The greedy randomized adaptive search procedure (also known as GRASP) is a metaheuristic algorithm commonly applied to combinatorial optimization problems. !! Some examples of combinatorial optimization problems that are covered by this framework are shortest paths and shortest-path trees, flows and circulations, spanning trees, matching, and matroid problems."
data base model,Hierarchical database modelIt is the oldest form of data base model.
relational database models,"Three key terms are used extensively in relational database models: relations, attributes, and domains."
reverse lookup,"For example, to do a reverse lookup of the IP address 8. !! To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address."
ip address,"In the context of application design, an address pool may be the availability of a set of addresses (IP address, MAC address) available to an application that is shared among its users, or available for allocation to users, such as in host configurations with the Dynamic Host Configuration Protocol (DHCP). !! DNS spoofing, also referred to as DNS cache poisoning, is a form of computer security hacking in which corrupt Domain Name System data is introduced into the DNS resolver's cache, causing the name server to return an incorrect result record, e. g. an IP address. !! To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address."
computer network programming,Computer network programming involves writing computer programs that enable processes to communicate with each other across a computer network.
markov model,"In probability theory, a Markov model is a stochastic model used to model pseudo-randomly changing systems. !! Several well-known algorithms for hidden Markov models exist. !! The simplest Markov model is the Markov chain. !! For example, given a sequence of observations, the Viterbi algorithm will compute the most-likely corresponding sequence of states, the forward algorithm will compute the probability of the sequence of observations, and the BaumWelch algorithm will estimate the starting probabilities, the transition function, and the observation function of a hidden Markov model. !! A hidden Markov model is a Markov chain for which the state is only partially observable or noisily observable."
probability theory,"In probability theory, a Markov model is a stochastic model used to model pseudo-randomly changing systems. !! In probability theory and information theory, adjusted mutual information, a variation of mutual information may be used for comparing clusterings. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! In probability theory and statistics, the logistic distribution is a continuous probability distribution. !! In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i. e. every finite linear combination of them is normally distributed. !! In probability theory and in particular in information theory, total correlation (Watanabe 1960) is one of several generalizations of the mutual information. !! In probability theory and statistics, the chi-squared distribution (also chi-square or 2-distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. !! Although Bayes' theorem is a fundamental result of probability theory, it has a specific interpretation in Bayesian statistics. !! In statistics, probability theory, and information theory, a statistical distance quantifies the distance between two statistical objects, which can be two random variables, or two probability distributions or samples, or the distance can be between an individual sample point and a population or a wider sample of points."
hidden markov model,"The parameters of a hidden Markov model are of two types, transition probabilities and emission probabilities (also known as output probabilities). !! The entire system is that of a hidden Markov model (HMM). !! Several inference problems are associated with hidden Markov models, as outlined below. !! In speaker diarisation one of the most popular methods is to use a Gaussian mixture model to model each of the speakers, and assign the corresponding frames for each speaker with the help of a Hidden Markov Model. !! In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). !! The layered hidden Markov model (LHMM) is a statistical model derived from the hidden Markov model (HMM). !! For example, given a sequence of observations, the Viterbi algorithm will compute the most-likely corresponding sequence of states, the forward algorithm will compute the probability of the sequence of observations, and the BaumWelch algorithm will estimate the starting probabilities, the transition function, and the observation function of a hidden Markov model. !! A hidden Markov model is a Markov chain for which the state is only partially observable or noisily observable. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."
baumwelch algorithm,"For example, given a sequence of observations, the Viterbi algorithm will compute the most-likely corresponding sequence of states, the forward algorithm will compute the probability of the sequence of observations, and the BaumWelch algorithm will estimate the starting probabilities, the transition function, and the observation function of a hidden Markov model."
forward algorithm,"For example, given a sequence of observations, the Viterbi algorithm will compute the most-likely corresponding sequence of states, the forward algorithm will compute the probability of the sequence of observations, and the BaumWelch algorithm will estimate the starting probabilities, the transition function, and the observation function of a hidden Markov model."
transition function,"For example, given a sequence of observations, the Viterbi algorithm will compute the most-likely corresponding sequence of states, the forward algorithm will compute the probability of the sequence of observations, and the BaumWelch algorithm will estimate the starting probabilities, the transition function, and the observation function of a hidden Markov model."
viterbi algorithm,"The Viterbi algorithm is named after Andrew Viterbi, who proposed it in 1967 as a decoding algorithm for convolutional codes over noisy digital communication links. !! A generalization of the Viterbi algorithm, termed the max-sum algorithm (or max-product algorithm) can be used to find the most likely assignment of all or some subset of latent variables in a large number of graphical models, e. g. Bayesian networks, Markov random fields and conditional random fields. !! For sequential decoding to a good choice of decoding algorithm, the number of states explored wants to remain small (otherwise an algorithm which deliberately explores all states, e. g. the Viterbi algorithm, may be more suitable). !! For example, given a sequence of observations, the Viterbi algorithm will compute the most-likely corresponding sequence of states, the forward algorithm will compute the probability of the sequence of observations, and the BaumWelch algorithm will estimate the starting probabilities, the transition function, and the observation function of a hidden Markov model. !! The Viterbi algorithm finds the most likely string of text given the acoustic signal. !! The Viterbi algorithm is a dynamic programming algorithm for obtaining the maximum a posteriori probability estimate of the most likely sequence of hidden statescalled the Viterbi paththat results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM). !! Viterbi path and Viterbi algorithm have become standard terms for the application of dynamic programming algorithms to maximization problems involving probabilities."
range queries,"Reverse indexes are just as efficient as unreversed indexes for finding specific values, although they aren't helpful for range queries."
reverse indexes,"Reverse indexes are just as efficient as unreversed indexes for finding specific values, although they aren't helpful for range queries."
unreversed indexes,"Reverse indexes are just as efficient as unreversed indexes for finding specific values, although they aren't helpful for range queries."
industrial protocol,The Common Industrial Protocol (CIP) is an industrial protocol for industrial automation applications.
humancomputer interfaces,The ACM Symposium on User Interface Software and Technology (UIST) is an annual conference for technical innovations in humancomputer interfaces.
user interface software,The ACM Symposium on User Interface Software and Technology (UIST) is an annual conference for technical innovations in humancomputer interfaces.
neurocomputational speech processing,"Neurocomputational speech processing is speech processing by artificial neural networks. !! Neurocomputational speech processing is computer-simulation of speech production and speech perception by referring to the natural neuronal processes of speech production and speech perception, as they occur in the human nervous system (central nervous system and peripheral nervous system)."
speech processing,"Linear predictive coding (LPC), a speech processing algorithm, was first proposed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966. !! Early attempts at speech processing and recognition were primarily focused on understanding a handful of simple phonetic elements such as vowels. !! The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. !! Voice activity detection (VAD), also known as speech activity detection or speech detection, is the detection of the presence or absence of human speech, used in speech processing. !! Neurocomputational speech processing is speech processing by artificial neural networks. !! Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! Speech processing is the study of speech signals and the processing methods of signals."
reasoning system,"Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems. !! In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. !! Reasoning systems come in two modes: interactive and batch processing. !! Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing. !! By the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision."
reasoning systems,"By the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision. !! Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing."
predictive analytics,"Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing."
business rule processing,"Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing."
complex event processing,"Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing."
intrusion detection,"Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing."
software implementations,There are both hardware (in modems) and software implementations of a Viterbi decoder. !! Powerful and efficient analysisOne of the main reasons there is interest in formal specifications is that they will provide an ability to perform proofs on software implementations.
viterbi decoder,"Since it does it in inverse direction, a viterbi decoder comprises a FILO (first-in-last-out) buffer to reconstruct a correct order. !! There are both hardware (in modems) and software implementations of a Viterbi decoder. !! A hard decision Viterbi decoder receives a simple bitstream on its input, and a Hamming distance is used as a metric. !! A soft decision Viterbi decoder receives a bitstream containing information about the reliability of each received symbol. !! There are hard decision and soft decision Viterbi decoders."
soft decision viterbi decoders,There are hard decision and soft decision Viterbi decoders.
hamming distance,"A hard decision Viterbi decoder receives a simple bitstream on its input, and a Hamming distance is used as a metric. !! In the mathematical field of combinatorics, a bent function is a special type of Boolean function which is maximally non-linear; it is as different as possible from the set of all linear and affine functions when measured by Hamming distance between truth tables."
speech detection,"Voice activity detection (VAD), also known as speech activity detection or speech detection, is the detection of the presence or absence of human speech, used in speech processing."
speech activity detection,"Voice activity detection (VAD), also known as speech activity detection or speech detection, is the detection of the presence or absence of human speech, used in speech processing."
transfer learning,"Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. !! The paper gives a mathematical and geometrical model of transfer learning. !! Both positive and negative transfer learning was experimentally demonstrated. !! In 1976, Stevo Bozinovski and Ante Fulgosi published a paper explicitly addressing transfer learning in neural networks training. !! In 1981, a report was given on the application of transfer learning in training a neural network on a dataset of images representing letters of computer terminals."
computer terminals,"In 1981, a report was given on the application of transfer learning in training a neural network on a dataset of images representing letters of computer terminals."
negative transfer learning,Both positive and negative transfer learning was experimentally demonstrated.
graph state,"In quantum computing, a graph state is a special type of multi-qubit state that can be represented by a graph. !! Quantum graph states can be defined in two equivalent ways: through the notion of quantum circuits and stabilizer formalism. !! Graph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models. !! Multiparty entanglement in graph states. !! More generally, two graph states are locally equivalent if and only if the corresponding graphs are related by a sequence of so-called ""local complementation"" steps, as shown by Van den Nest et al."
quantum computing,"In quantum computing, a graph state is a special type of multi-qubit state that can be represented by a graph. !! In recent years, investment in quantum computing research has increased in the public and private sectors. !! The study of quantum computing is a subfield of quantum information science. !! In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. !! Quantum computing is a type of computation that harnesses the collective properties of quantum states, such as superposition, interference, and entanglement, to perform calculations. !! Often superconducting computing is applied to quantum computing, with an important application known as superconducting quantum computing. !! Quantum computing began in 1980 when physicist Paul Benioff proposed a quantum mechanical model of the Turing machine. !! Despite ongoing experimental progress since the late 1990s, most researchers believe that ""fault-tolerant quantum computing [is] still a rather distant dream. """
entanglement measurement,"Graph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models."
graph states,"Graph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models. !! Multiparty entanglement in graph states."
quantum error,"Graph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models."
quantum graph states,Quantum graph states can be defined in two equivalent ways: through the notion of quantum circuits and stabilizer formalism.
unlabeled data,"Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. !! Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. !! Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning."
labeled data,Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training.
semi-supervised learning,"Transductive support-vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. !! In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes. !! Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. !! Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning. !! Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning. !! In such situations, semi-supervised learning can be of great practical value. !! Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. !! Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data)."
labeled training data,Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).
software pipelining,"Software pipelining has been known to assembly language programmers of machines with instruction-level parallelism since such architectures existed. !! Some computer architectures have explicit support for software pipelining, notably Intel's IA-64 architecture. !! In computer science, software pipelining is a technique used to optimize loops, in a manner that parallels hardware pipelining. !! It is important to distinguish software pipelining, which is a target code technique for overlapping loop iterations, from modulo scheduling, the currently most effective known compiler technique for generating software pipelined loops. !! Software pipelining is a type of out-of-order execution, except that the reordering is done by a compiler (or in the case of hand written assembly code, by the programmer) instead of the processor."
computer architectures,"Some computer architectures have explicit support for software pipelining, notably Intel's IA-64 architecture."
modulo scheduling,"It is important to distinguish software pipelining, which is a target code technique for overlapping loop iterations, from modulo scheduling, the currently most effective known compiler technique for generating software pipelined loops."
assembly language programmers,Software pipelining has been known to assembly language programmers of machines with instruction-level parallelism since such architectures existed.
backfitting algorithm,"Following, we can formulate the backfitting algorithm explicitly for the two dimensional case. !! In most cases, the backfitting algorithm is equivalent to the GaussSeidel method algorithm for solving a certain linear system of equations. !! In statistics, the backfitting algorithm is a simple iterative procedure used to fit a generalized additive model. !! We can modify the backfitting algorithm to make it easier to provide a unique solution. !! A modification of the backfitting algorithm involving projections onto the eigenspace of S can remedy this problem."
gaussseidel method algorithm,"In most cases, the backfitting algorithm is equivalent to the GaussSeidel method algorithm for solving a certain linear system of equations."
symbolic artificial intelligence,"Symbolic Artificial Intelligence, Connectionist Networks & Beyond (Technical report). !! Subsymbolic artificial intelligence is the set of alternative approaches which do not use explicit high level symbols, such as mathematical optimization, statistical classifiers and neural networks. !! Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Towards a Resolution of the Dichotomy. !! In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search."
statistical classifiers,"Subsymbolic artificial intelligence is the set of alternative approaches which do not use explicit high level symbols, such as mathematical optimization, statistical classifiers and neural networks."
subsymbolic artificial intelligence,"Subsymbolic artificial intelligence is the set of alternative approaches which do not use explicit high level symbols, such as mathematical optimization, statistical classifiers and neural networks."
connectionist networks,"Symbolic Artificial Intelligence, Connectionist Networks & Beyond (Technical report)."
numeric artificial neural networks,Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Towards a Resolution of the Dichotomy.
log analysis,"Log Analysis is often compared to other analytics tools such as application performance management (APM) and error monitoring. !! In computer log management and intelligence, log analysis (or system and network log analysis) is an art and science seeking to make sense out of computer-generated records (also called log or audit trail records). !! In log analysis, this means recognizing and ignoring the regular, common log messages that result from the normal operation of the system, and therefore are not too interesting."
Log analysis,"In computer log management and intelligence, log analysis (or system and network log analysis) is an art and science seeking to make sense out of computer-generated records (also called log or audit trail records). !! Hence, log analysis must interpret messages within the context of an application, vendor, system or configuration in order to make useful comparisons to messages from different log sources. !! In log analysis, this means recognizing and ignoring the regular, common log messages that result from the normal operation of the system, and therefore are not too interesting. !! Hence, log analysis practices exist on the continuum from text retrieval to reverse engineering of software. !! Log Analysis is often compared to other analytics tools such as application performance management (APM) and error monitoring."
audit trail records,"In computer log management and intelligence, log analysis (or system and network log analysis) is an art and science seeking to make sense out of computer-generated records (also called log or audit trail records)."
computer log management,"In computer log management and intelligence, log analysis (or system and network log analysis) is an art and science seeking to make sense out of computer-generated records (also called log or audit trail records)."
network log analysis,"In computer log management and intelligence, log analysis (or system and network log analysis) is an art and science seeking to make sense out of computer-generated records (also called log or audit trail records)."
error monitoring,Log Analysis is often compared to other analytics tools such as application performance management (APM) and error monitoring.
dependency network,"The partial correlation based dependency network is a class of correlation network, capable of uncovering hidden relationships between its nodes. !! The dependency network approach provides a system level analysis of the activity and topology of directed networks. !! One of the main results of this work is that for the investigated time period (20012003), the structure of the network is dominated by companies belonging to the financial sector, which are the hubs in the dependency network. !! Following this work, the dependency network methodology has been applied to the study of the immune system, and semantic networks."
system level analysis,The dependency network approach provides a system level analysis of the activity and topology of directed networks.
directed networks,The dependency network approach provides a system level analysis of the activity and topology of directed networks.
correlation network,"The partial correlation based dependency network is a class of correlation network, capable of uncovering hidden relationships between its nodes."
semantic networks,"Following this work, the dependency network methodology has been applied to the study of the immune system, and semantic networks. !! Spreading activation in semantic networks as a model were invented in cognitive psychology to model the fan out effect. !! Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks."
information distance,"To determine the similarity of objects such as genomes, languages, music, internet attacks and worms, software programs, and so on, information distance is normalized and the Kolmogorov complexity terms approximated by real-world compressors (the Kolmogorov complexity is a lower bound to the length in bits of a compressed version of the object). !! A python package for computing all information distances and volumes, multivariate mutual information, conditional mutual information, joint entropies, total correlations, in a dataset of n variables is available . !! Information distance was material in the textbook, it occurs in the Encyclopedia on Distances. !! Information distance was first defined and investigated in based on thermodynamic principles, see also. !! The Kolmogorov complexity of a single finite object is the information in that object; the information distance between a pair of finite objects is the minimum information required to go from one object to the other or vice versa."
kolmogorov complexity,"To determine the similarity of objects such as genomes, languages, music, internet attacks and worms, software programs, and so on, information distance is normalized and the Kolmogorov complexity terms approximated by real-world compressors (the Kolmogorov complexity is a lower bound to the length in bits of a compressed version of the object). !! The Kolmogorov complexity of a single finite object is the information in that object; the information distance between a pair of finite objects is the minimum information required to go from one object to the other or vice versa. !! In an attempt to explain how humans perceive relevance, cognitive complexity is defined as an extension of the notion of Kolmogorov complexity. !! Minimum Message Length and Kolmogorov Complexity."
compressed version,"To determine the similarity of objects such as genomes, languages, music, internet attacks and worms, software programs, and so on, information distance is normalized and the Kolmogorov complexity terms approximated by real-world compressors (the Kolmogorov complexity is a lower bound to the length in bits of a compressed version of the object)."
software programs,"To determine the similarity of objects such as genomes, languages, music, internet attacks and worms, software programs, and so on, information distance is normalized and the Kolmogorov complexity terms approximated by real-world compressors (the Kolmogorov complexity is a lower bound to the length in bits of a compressed version of the object)."
python package,"A python package for computing all information distances and volumes, multivariate mutual information, conditional mutual information, joint entropies, total correlations, in a dataset of n variables is available ."
conditional mutual information,"A python package for computing all information distances and volumes, multivariate mutual information, conditional mutual information, joint entropies, total correlations, in a dataset of n variables is available ."
joint entropies,"A python package for computing all information distances and volumes, multivariate mutual information, conditional mutual information, joint entropies, total correlations, in a dataset of n variables is available ."
information distances,"A python package for computing all information distances and volumes, multivariate mutual information, conditional mutual information, joint entropies, total correlations, in a dataset of n variables is available ."
multivariate mutual information,"A python package for computing all information distances and volumes, multivariate mutual information, conditional mutual information, joint entropies, total correlations, in a dataset of n variables is available ."
first order logic,"While the expressive power of combinatory logic typically exceeds that of first-order logic, the expressive power of predicate functor logic is identical to that of first order logic (Quine 1960, 1966, 1976)."
predicate functor logic,"While the expressive power of combinatory logic typically exceeds that of first-order logic, the expressive power of predicate functor logic is identical to that of first order logic (Quine 1960, 1966, 1976)."
structure mining,Structure mining or structured data mining is the process of finding and extracting useful information from semi-structured data sets. !! The addition of these data types related to the structure of a document or message facilitates structure mining.
structured data mining,"Sequential pattern mining is a special case of structured data mining. !! Structure mining or structured data mining is the process of finding and extracting useful information from semi-structured data sets. !! Graph mining, sequential pattern mining and molecule mining are special cases of structured data mining."
structured data sets,Structure mining or structured data mining is the process of finding and extracting useful information from semi-structured data sets.
state machine operating,"In computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set)."
abstract state machine,"In computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set). !! E. Brger and R. Strk, Abstract State Machines: A Method for High-Level System Design and Analysis, Springer-Verlag, 2003. !! Y. Gurevich, Sequential Abstract State Machines capture Sequential Algorithms, ACM Transactions on Computational Logic 1(1) (July 2000), 77111."
arbitrary data structures,"In computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set)."
abstract state machines,"E. Brger and R. Strk, Abstract State Machines: A Method for High-Level System Design and Analysis, Springer-Verlag, 2003."
3d user interaction,"For the full development of a 3D User Interaction system, is required to have access to a few basic parameters, all this technology-based system should know, or at least partially, as the relative position of the user, the absolute position, angular velocity, rotation data, orientation or height. !! 3D user interaction systems are based primarily on motion tracking technologies, to obtain all the necessary information from the user through the analysis of their movements or gestures, these technologies are called, tracking technologies. !! Towards a Universal Implementation of 3D User Interaction Techniques [Proceedings of Specification, Authoring, Adaptation of Mixed Reality User Interfaces Workshop, IEEE VR]."
3d user interaction systems,"3D user interaction systems are based primarily on motion tracking technologies, to obtain all the necessary information from the user through the analysis of their movements or gestures, these technologies are called, tracking technologies."
3d user interaction system,"For the full development of a 3D User Interaction system, is required to have access to a few basic parameters, all this technology-based system should know, or at least partially, as the relative position of the user, the absolute position, angular velocity, rotation data, orientation or height."
3d user interaction techniques,"Towards a Universal Implementation of 3D User Interaction Techniques [Proceedings of Specification, Authoring, Adaptation of Mixed Reality User Interfaces Workshop, IEEE VR]."
message authentication codes,"a small change to a message should change the hash value so extensively that a new hash value appears uncorrelated with the old hash value (avalanche effect)Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. !! ISO/IEC 29192-6 Lightweight cryptography - Message authentication codesISO/IEC 9797-1 and -2 define generic models and algorithms that can be used with any block cipher or hash function, and a variety of different parameters. !! Message authentication codes and data origin authentication have been also discussed in the framework of quantum cryptography."
data origin authentication,Message authentication codes and data origin authentication have been also discussed in the framework of quantum cryptography.
block cipher,"For example, a block cipher encryption algorithm might take a 128-bit block of plaintext as input, and output a corresponding 128-bit block of ciphertext. !! However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators. !! In cryptography, a block cipher is a deterministic algorithm operating on fixed-length groups of bits, called blocks. !! ISO/IEC 29192-6 Lightweight cryptography - Message authentication codesISO/IEC 9797-1 and -2 define generic models and algorithms that can be used with any block cipher or hash function, and a variety of different parameters. !! Even a secure block cipher is suitable for the encryption of only a single block of data at a time, using a fixed key. !! A block cipher consists of two paired algorithms, one for encryption, E, and the other for decryption, D. Both algorithms accept two inputs: an input block of size n bits and a key of size k bits; and both yield an n-bit output block."
hash function,"Linear probing can provide high performance because of its good locality of reference, but is more sensitive to the quality of its hash function than some other collision resolution schemes. !! Use of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision). !! ISO/IEC 29192-6 Lightweight cryptography - Message authentication codesISO/IEC 9797-1 and -2 define generic models and algorithms that can be used with any block cipher or hash function, and a variety of different parameters. !! Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. !! A perfect hash function can, as any hash function, be used to implement hash tables, with the advantage that no collision resolution has to be implemented. !! A hash function is any function that can be used to map data of arbitrary size to fixed-size values. !! Use of a hash function to index a hash table is called hashing or scatter storage addressing. !! In computer science, a perfect hash function h for a set S is a hash function that maps distinct elements in S to a set of m integers, with no collisions. !! The values returned by a hash function are called hash values, hash codes, digests, or simply hashes."
advanced message queuing protocol,The Advanced Message Queuing Protocol (AMQP) is an open standard application layer protocol for message-oriented middleware.
session identifier cookie,"Cross-site cooking can be used to perform session fixation attacks, as a malicious site can fixate the session identifier cookie of another site."
analytical hierarchy process,"Saaty, Thomas L. Decision Making for Leaders: The Analytical Hierarchy Process for Decisions in a Complex World (1982)."
analytic hierarchy process,"The analytic hierarchy process (AHP), also analytical hierarchy process, is a structured technique for organizing and analyzing complex decisions, based on mathematics and psychology."
regression models,Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where linearly independent variables are highly correlated.
linearly independent variables,Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where linearly independent variables are highly correlated.
ridge regression,The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers RIDGE regressions: biased estimation of nonorthogonal problems and RIDGE regressions: applications in nonorthogonal problems. !! Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where linearly independent variables are highly correlated. !! Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variablesby creating a ridge regression estimator (RR).
highly correlated,Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variablesby creating a ridge regression estimator (RR). !! Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where linearly independent variables are highly correlated.
nonorthogonal problems,The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers RIDGE regressions: biased estimation of nonorthogonal problems and RIDGE regressions: applications in nonorthogonal problems.
ridge regressions,The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers RIDGE regressions: biased estimation of nonorthogonal problems and RIDGE regressions: applications in nonorthogonal problems.
ridge regression estimator,Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variablesby creating a ridge regression estimator (RR).
least square estimators,Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variablesby creating a ridge regression estimator (RR).
linear regression models,Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variablesby creating a ridge regression estimator (RR).
schwartzian transform,"In computer programming, the Schwartzian transform is a technique used to improve the efficiency of sorting a list of items. !! The Schwartzian transform is notable in that it does not use named temporary arrays. !! The term ""Schwartzian transform"" applied solely to Perl programming for a number of years, but it has later been adopted by some users of other languages, such as Python, to refer to similar idioms in those languages. !! The Schwartzian transform is a version of a Lisp idiom known as decorate-sort-undecorate, which avoids recomputing the sort keys by temporarily associating them with the input items. !! The term ""Schwartzian transform"" indicates a specific idiom, and not the algorithm in general."
sort keys,"The Schwartzian transform is a version of a Lisp idiom known as decorate-sort-undecorate, which avoids recomputing the sort keys by temporarily associating them with the input items."
perl programming,"The term ""Schwartzian transform"" applied solely to Perl programming for a number of years, but it has later been adopted by some users of other languages, such as Python, to refer to similar idioms in those languages."
spatiotemporal pattern,"Any kind of traveling wave is a good example of a spatiotemporal pattern. !! Any type of reactiondiffusion system that produces spatial patterns will also, due to the time-dependency of both reactions and diffusion, produce spatiotemporal patterns. !! In contrast to ""static"", pure spatial patterns, the full complexity of spatiotemporal patterns can only be recognized over time. !! It is thus apt to say that spatiotemporal patterns in nature are the rule rather than the exception. !! Spatiotemporal patterns are patterns that occur in a wide range of natural phenoma and are characterized by a spatial and a temporal patterning."
spatiotemporal patterns,"In contrast to ""static"", pure spatial patterns, the full complexity of spatiotemporal patterns can only be recognized over time. !! It is thus apt to say that spatiotemporal patterns in nature are the rule rather than the exception. !! Spatiotemporal patterns are patterns that occur in a wide range of natural phenoma and are characterized by a spatial and a temporal patterning."
temporal patterning,Spatiotemporal patterns are patterns that occur in a wide range of natural phenoma and are characterized by a spatial and a temporal patterning.
stochastic diffusion search,"Convergence Analysis of Stochastic Diffusion Search. !! To solve the problem delegates decide to employ a stochastic diffusion search. !! Stochastic diffusion search (SDS) was first described in 1989 as a population-based, pattern-matching algorithm. !! Minimum Stable Convergence Criteria for Stochastic Diffusion Search To be published in Electronics Letters. !! Analysis of Resource Allocation of Stochastic Diffusion Search."
convergence analysis,Convergence Analysis of Stochastic Diffusion Search.
receive quality metric,"The residual bit error rate (RBER) is a receive quality metric in digital transmission, one of several used to quantify the accuracy of the received data."
digital transmission,"The residual bit error rate (RBER) is a receive quality metric in digital transmission, one of several used to quantify the accuracy of the received data."
residual bit error rate,"The residual bit error rate (RBER) is a receive quality metric in digital transmission, one of several used to quantify the accuracy of the received data. !! When digital communication systems are being designed, the maximum acceptable residual bit error rate can be used, along with other quality metrics, to calculate the minimum acceptable signal to noise ratio in the system."
noise ratio,"When digital communication systems are being designed, the maximum acceptable residual bit error rate can be used, along with other quality metrics, to calculate the minimum acceptable signal to noise ratio in the system."
quality metrics,"When digital communication systems are being designed, the maximum acceptable residual bit error rate can be used, along with other quality metrics, to calculate the minimum acceptable signal to noise ratio in the system."
graph traversal,"The problem of graph exploration can be seen as a variant of graph traversal. !! A universal traversal sequence is a sequence of instructions comprising a graph traversal for any regular graph with a set number of vertices and for any starting vertex. !! Tree traversal is a special case of graph traversal. !! In computer science, graph traversal (also known as graph search) refers to the process of visiting (checking and/or updating) each vertex in a graph. !! Precisely, a topological sort is a graph traversal in which each node v is visited only after all its dependencies are visited. !! Unlike tree traversal, graph traversal may require that some vertices be visited more than once, since it is not necessarily known before transitioning to a vertex that it has already been explored."
graph search,"is finite, as it is in hardware, model checking reduces to a graph search. !! In computer science, graph traversal (also known as graph search) refers to the process of visiting (checking and/or updating) each vertex in a graph."
tree traversal,Tree traversal is a special case of graph traversal.
graph exploration,The problem of graph exploration can be seen as a variant of graph traversal.
knowledge engineering,"These issues led to the second approach to knowledge engineering: development of custom methodologies specifically designed to build expert systems. !! Expert Systems: The Journal of Knowledge Engineering Wiley-Blackwell !! Knowledge engineering (KE) refers to all technical, scientific and social aspects involved in building, maintaining and using knowledge-based systems."
randomized meldable priority queue,"In computer science, a randomized meldable heap (also Meldable Heap or Randomized Meldable Priority Queue) is a priority queue based data structure in which the underlying structure is also a heap-ordered binary tree."
priority queue based data structure,"In computer science, a randomized meldable heap (also Meldable Heap or Randomized Meldable Priority Queue) is a priority queue based data structure in which the underlying structure is also a heap-ordered binary tree."
meldable heap implementation,"While the randomized meldable heap is the simplest form of a meldable heap implementation, others do exist."
prune and search,Prune and search is a method of solving optimization problems suggested by Nimrod Megiddo in 1983. !! In prune and search algorithms S(n) is typically at least linear (since the whole input must be processed).
probabilistic model,A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
conditional dependence structure,A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
graphical model,"Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution. !! This type of graphical model is known as a directed graphical model, Bayesian network, or belief network. !! A graphical model with many repeated subunits can be represented with plate notation. !! A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. !! The next figure depicts a graphical model with a cycle."
structured probabilistic model,A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
directed graphical model,"This type of graphical model is known as a directed graphical model, Bayesian network, or belief network."
recurrent neural nets,Backpropagation through structure (BPTS) is a gradient-based technique for training recursive neural nets (a superset of recurrent neural nets) and is extensively described in a 1996 paper written by Christoph Goller and Andreas Kchler.
adaptive equalizer,Adaptive equalizers are a subclass of adaptive filters. !! An adaptive equalizer is an equalizer that automatically adapts to time-varying properties of the communication channel.
adaptive equalizers,Adaptive equalizers are a subclass of adaptive filters.
adaptive filters,Adaptive equalizers are a subclass of adaptive filters.
cholesky factorization,"In numerical analysis, an incomplete Cholesky factorization of a symmetric positive definite matrix is a sparse approximation of the Cholesky factorization. !! In linear algebra, the Cholesky decomposition or Cholesky factorization (pronounced sh-LES-kee) is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful for efficient numerical solutions, e. g. , Monte Carlo simulations."
octave scripting language,Implementation of the incomplete Cholesky factorization in the Octave scripting language.
software requirements analysis,One of the main components of software design is the software requirements analysis (SRA).
central processing unit,"Addressing modes are an aspect of the instruction set architecture in most central processing unit (CPU) designs. !! The instruction cycle (also known as the fetchdecodeexecute cycle, or simply the fetch-execute cycle) is the cycle that the central processing unit (CPU) follows from boot-up until the computer has shut down in order to process instructions. !! The ""central processing unit"" term has been in use since as early as 1955. !! Cloud computing can involve subdividing CPU operation into virtual central processing units (vCPUs). !! A central processing unit (CPU), also called a central processor, main processor or just processor, is the electronic circuitry that executes instructions comprising a computer program. !! The AMD Accelerated Processing Unit (APU), formerly known as Fusion, is the marketing term for a series of 64-bit microprocessors from Advanced Micro Devices (AMD), designed to act as a central processing unit (CPU) and graphics processing unit (GPU) on a single die. !! A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. !! Since the introduction of the first commercially available microprocessor, the Intel 4004 in 1971, and the first widely used microprocessor, the Intel 8080 in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods."
graphics processing unit,"Later, in 1994, Sony used the term (now standing for graphics processing unit) in reference to the PlayStation console's Toshiba-designed Sony GPU in 1994. !! It was used in a number of graphics cards, and was licensed for clones such as the Intel 82720, the first of Intel's graphics processing units. !! Integrated graphics processing unit (IGPU), Integrated graphics, shared graphics solutions, integrated graphics processors (IGP) or unified memory architecture (UMA) utilize a portion of a computer's system RAM rather than dedicated graphics memory. !! It is becoming increasingly common to use a general purpose graphics processing unit (GPGPU) as a modified form of stream processor (or a vector processor), running compute kernels. !! A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. !! The AMD Accelerated Processing Unit (APU), formerly known as Fusion, is the marketing term for a series of 64-bit microprocessors from Advanced Micro Devices (AMD), designed to act as a central processing unit (CPU) and graphics processing unit (GPU) on a single die."
software solution,"""Software design usually involves problem-solving and planning a software solution."
accelerated processing unit,"The AMD Accelerated Processing Unit (APU), formerly known as Fusion, is the marketing term for a series of 64-bit microprocessors from Advanced Micro Devices (AMD), designed to act as a central processing unit (CPU) and graphics processing unit (GPU) on a single die. !! The following table shows features of AMD's APUs (see also: List of AMD accelerated processing units)."
transductive learning,An early explanation of transductive learning.
statistical learning theory,"Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. !! Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. !! Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics. !! Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. !! From the perspective of statistical learning theory, supervised learning is best understood. !! In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions."
learnable function class,"In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions."
design problem,A design pattern is the re-usable form of a solution to a design problem.
design pattern,"Since two houses may be very different from one another, a design pattern for houses must be broad enough to apply to both of them, but not so vague that it doesn't help the designer make decisions. !! An organized collection of design patterns that relate to a particular field is called a pattern language. !! Pattern gardening, in gardeningBusiness models also have design patterns. !! A design pattern is the re-usable form of a solution to a design problem. !! Row Data Gateway is a design pattern in which an object acts as a gateway to a single database row."
storage space,A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.
database index,A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.
linear probing,"Along with quadratic probing and double hashing, linear probing is a form of open addressing. !! Linear probing can provide high performance because of its good locality of reference, but is more sensitive to the quality of its hash function than some other collision resolution schemes. !! When the hash function causes a collision by mapping a new key to a cell of the hash table that is already occupied by another key, linear probing searches the table for the closest following free location and inserts the new key there. !! Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of keyvalue pairs and looking up the value associated with a given key. !! As Thorup & Zhang (2012) write, ""Hash tables are the most commonly used nontrivial data structures, and the most popular implementation on standard hardware uses linear probing, which is both fast and simple. """
quadratic probing,"Along with quadratic probing and double hashing, linear probing is a form of open addressing."
hash table,"When the hash function causes a collision by mapping a new key to a cell of the hash table that is already occupied by another key, linear probing searches the table for the closest following free location and inserts the new key there. !! Use of a hash function to index a hash table is called hashing or scatter storage addressing. !! In computer science, lazy deletion refers to a method of deleting elements from a hash table that uses open addressing."
collision resolution schemes,"Linear probing can provide high performance because of its good locality of reference, but is more sensitive to the quality of its hash function than some other collision resolution schemes."
evolutionary algorithms,"Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. !! Reward-based selection is a technique used in evolutionary algorithms for selecting potentially useful solutions for recombination. !! Bck, T. (1996), Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms, Oxford Univ. !! A possible limitation of many evolutionary algorithms is their lack of a clear genotypephenotype distinction. !! Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. !! In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA)."
genetic algorithms,"In some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e. g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used. !! Although crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms. !! Bck, T. (1996), Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms, Oxford Univ. !! Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. !! Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. !! Genetic algorithms are simple to implement, but their behavior is difficult to understand. !! ""Because highly fit schemata of low defining length and low order play such an important role in the action of genetic algorithms, we have already given them a special name: building blocks."
evolutionary programming,"Bck, T. (1996), Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms, Oxford Univ."
evolution strategies,"Bck, T. (1996), Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms, Oxford Univ."
gesture recognition,"However, the identification and recognition of posture, gait, proxemics, and human behaviors is also the subject of gesture recognition techniques. !! Current focuses in the field include emotion recognition from face and hand gesture recognition. !! Automated sign language translationGesture recognition can be conducted with techniques from computer vision and image processing. !! Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. !! Gesture recognition can be seen as a way for computers to begin to understand human body language, thus building a richer bridge between machines and humans than primitive text user interfaces or even GUIs (graphical user interfaces), which still limit the majority of input to keyboard and mouse and interact naturally without any mechanical devices. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."
hidden markov models,"Hidden Markov models can also be generalized to allow continuous state spaces. !! Several inference problems are associated with hidden Markov models, as outlined below. !! Hidden Markov models were described in a series of statistical papers by Leonard E. Baum and other authors in the second half of the 1960s. !! In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). !! The Viterbi algorithm is a dynamic programming algorithm for obtaining the maximum a posteriori probability estimate of the most likely sequence of hidden statescalled the Viterbi paththat results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM). !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."
gaussian distribution,"In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). !! In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). !! The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution)."
state space,"In the theory of dynamical systems, the state space of a discrete system defined by a function can be modeled as a directed graph where each possible state of the dynamical system is represented by a vertex with a directed edge from a to b if and only if (a) = b. !! A ""counter"" system, where states are the natural numbers starting at 1 and are incremented over time has an infinite discrete state space. !! The angular position of an undamped pendulum is a continuous (and therefore infinite) state space. !! In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). !! In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). !! State space search often differs from traditional computer science search methods because the state space is implicit: the typical state space graph is much too large to generate and store in memory. !! A state space is the set of all possible configurations of a system. !! For instance, the toy problem Vacuum World has a discrete finite state space in which there are a limited set of configurations that the vacuum and dirt can be in."
web applications,"Web application security is a branch of information security that deals specifically with the security of websites, web applications and web services. !! Cross-site scripting (XSS) is a type of security vulnerability that can be found in some web applications."
cross-site scripting,"A cross-site scripting vulnerability may be used by attackers to bypass access controls such as the same-origin policy. !! Cross-site cooking is similar in concept to cross-site scripting, cross-site request forgery, cross-site tracing, cross-zone scripting etc. !! Cross-site scripting carried out on websites accounted for roughly 84% of all security vulnerabilities documented by Symantec up until 2007. !! Cross-site scripting (XSS) is a type of security vulnerability that can be found in some web applications. !! Cross-site scripting attacks use known vulnerabilities in web-based applications, their servers, or the plug-in systems on which they rely. !! Cross-site scripting attacks are a case of code injection."
neural turing machines,"Differentiable neural computers are an outgrowth of Neural Turing machines, with attention mechanisms that control where the memory is active, and improve performance."
differentiable neural computers,"Differentiable neural computers are an outgrowth of Neural Turing machines, with attention mechanisms that control where the memory is active, and improve performance."
attention mechanisms,"Differentiable neural computers are an outgrowth of Neural Turing machines, with attention mechanisms that control where the memory is active, and improve performance."
temporal coding,"Temporal coding supplies an alternate explanation for the noise,"" suggesting that it actually encodes information and affects neural processing. !! Temporal coding allows the sequence 000111000111 to mean something different from 001100110011, even though the mean firing rate is the same for both sequences. !! Whether neurons use rate coding or temporal coding is a topic of intense debate within the neuroscience community, even though there is no clear definition of what these terms mean. !! The modifications can themselves depend on spike timing patterns (temporal coding), i. e. , can be a special case of spike-timing-dependent plasticity. !! In temporal coding, learning can be explained by activity-dependent synaptic delay modifications."
functional programming,"In object-oriented and functional programming, an immutable object (unchangeable object) is an object whose state cannot be modified after it is created."
runtime efficiency,Strings and other concrete objects are typically expressed as immutable objects to improve readability and runtime efficiency in object-oriented programming.
immutable objects,"Immutable objects are also useful because they are inherently thread-safe. !! A technique that blends the advantages of mutable and immutable objects, and is supported directly in almost all modern hardware, is copy-on-write (COW). !! Strings and other concrete objects are typically expressed as immutable objects to improve readability and runtime efficiency in object-oriented programming."
software architect,"International Association of Software Architects (IASA) !! A software architect is a software development expert who makes high-level design choices and tries to enforce technical standards, including software coding standards, tools, and platforms. !! The software architect concept began to take hold when object-oriented programming or OOP, was coming into more widespread use (in the late 1990s and early years of the 21st century)."
software architects,International Association of Software Architects (IASA)
flexible cluster extraction,"ELKI includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK, CLINK and Anderberg algorithms, flexible cluster extraction from dendrograms and various other cluster analysis algorithms."
cluster analysis algorithms,"ELKI includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK, CLINK and Anderberg algorithms, flexible cluster extraction from dendrograms and various other cluster analysis algorithms."
anderberg algorithms,"ELKI includes multiple hierarchical clustering algorithms, various linkage strategies and also includes the efficient SLINK, CLINK and Anderberg algorithms, flexible cluster extraction from dendrograms and various other cluster analysis algorithms."
primitive data types,"In computer science, primitive data types are a set of basic data types from which all other data types are constructed. !! In some programming environments the term complex data type (in contrast to primitive data types) is a synonym for the composite data type. !! Primitive data types are typically types that are built-in or basic to a language implementation. !! Primitive data types which are native to the processor have a one-to-one correspondence with objects in the computer's memory, and operations on these types are often the fastest possible in most cases. !! Most processors support a similar set of primitive data types, although the specific representations vary. !! More generally ""primitive data types"" may refer to the standard data types built into a programming language."
term complex data type,In some programming environments the term complex data type (in contrast to primitive data types) is a synonym for the composite data type.
programming environments,In some programming environments the term complex data type (in contrast to primitive data types) is a synonym for the composite data type.
composite data type,In some programming environments the term complex data type (in contrast to primitive data types) is a synonym for the composite data type.
reinforcement learning agent,"The mountain car problem, although fairly simple, is commonly applied because it requires a reinforcement learning agent to learn on two continuous variables: position and velocity."
approximation theory,"L. N. Trefethen, ""Approximation theory and approximation practice"", SIAM 2013. !! In mathematics, approximation theory is concerned with how functions can best be approximated with simpler functions, and with quantitatively characterizing the errors introduced thereby. !! K. -G. Steffens, ""The History of Approximation Theory: From Euler to Bernstein,"" Birkhauser, Boston 2006 ISBN 0-8176-4353-2. !! Surveys in Approximation Theory (SAT)"
adjacency matrix,"The adjacency matrix of a graph should be distinguished from its incidence matrix, a different matrix representation whose elements indicate whether vertexedge pairs are incident or not, and its degree matrix, which contains information about the degree of each vertex. !! If the graph is undirected (i. e. all of its edges are bidirectional), the adjacency matrix is symmetric. !! In the special case of a finite simple graph, the adjacency matrix is a (0,1)-matrix with zeros on its diagonal. !! The relationship between a graph and the eigenvalues and eigenvectors of its adjacency matrix is studied in spectral graph theory. !! In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph."
spectral graph theory,The relationship between a graph and the eigenvalues and eigenvectors of its adjacency matrix is studied in spectral graph theory.
incidence matrix,"The adjacency matrix of a graph should be distinguished from its incidence matrix, a different matrix representation whose elements indicate whether vertexedge pairs are incident or not, and its degree matrix, which contains information about the degree of each vertex."
degree matrix,"The adjacency matrix of a graph should be distinguished from its incidence matrix, a different matrix representation whose elements indicate whether vertexedge pairs are incident or not, and its degree matrix, which contains information about the degree of each vertex."
computational power,"Note that, unlike in computational complexity theory, communication complexity is not concerned with the amount of computation performed by Alice or Bob, or the size of the memory used, as we generally assume nothing about the computational power of either Alice or Bob."
text analysis,"Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. !! In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning."
sentiment analysis,"There are various other types of sentiment analysis like- Aspect Based sentiment analysis, Grading sentiment analysis (positive, negative, neutral), Multilingual sentiment analysis and detection of emotions. !! Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. !! A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect levelwhether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. !! Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. !! The objective and challenges of sentiment analysis can be shown through some simple examples."
grading sentiment analysis,"There are various other types of sentiment analysis like- Aspect Based sentiment analysis, Grading sentiment analysis (positive, negative, neutral), Multilingual sentiment analysis and detection of emotions."
computability logic,"Computability logic is more expressive, constructive and computationally meaningful than classical logic. !! Computability Logic Homepage Comprehensive survey of the subject. !! Computability logic (CoL) is a research program and mathematical framework for redeveloping logic as a systematic formal theory of computability, as opposed to classical logic which is a formal theory of truth. !! A Survey of Computability Logic (PDF) Downloadable equivalent of the above homepage."
classical logic,"Computability logic is more expressive, constructive and computationally meaningful than classical logic. !! Computability logic (CoL) is a research program and mathematical framework for redeveloping logic as a systematic formal theory of computability, as opposed to classical logic which is a formal theory of truth. !! In terms of simple denotational models, linear logic may be seen as refining the interpretation of intuitionistic logic by replacing cartesian (closed) categories by symmetric monoidal (closed) categories, or the interpretation of classical logic by replacing Boolean algebras by C*-algebras."
modeling software,"Wagner, F. , ""Modeling Software with Finite State Machines: A Practical Approach"", Auerbach Publications, 2006, ISBN 0-8493-8086-3."
finite state machines,"Runtime verification specifications are typically expressed in trace predicate formalisms, such as finite state machines, regular expressions, context-free patterns, linear temporal logics, etc. !! Timothy Kam, Synthesis of Finite State Machines: Functional Optimization. !! Tiziano Villa, Synthesis of Finite State Machines: Logic Optimization. !! Wagner, F. , ""Modeling Software with Finite State Machines: A Practical Approach"", Auerbach Publications, 2006, ISBN 0-8493-8086-3."
functional optimization,"Timothy Kam, Synthesis of Finite State Machines: Functional Optimization."
logic optimization,"Tiziano Villa, Synthesis of Finite State Machines: Logic Optimization."
spice protocol,Red Hat Virtualization uses the SPICE protocol and VDSM (Virtual Desktop Server Manager) with a RHEL-based centralized management server.
red hat virtualization,Rebranding to Red Hat Virtualization. !! Red Hat Virtualization uses the SPICE protocol and VDSM (Virtual Desktop Server Manager) with a RHEL-based centralized management server. !! Some of the technologies of Red Hat Virtualization came from Red Hat's acquisition of Qumranet.
linear regression,"The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. !! This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. !! In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). !! Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis. !! In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data."
scalar response,"In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables)."
explanatory variables,"In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables)."
multivariate linear regression,"This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable."
joint probability distribution,"Relational Dependency Networks (or RDNs) aims to get the joint probability distribution over the variables of a dataset represented in the relational domain. !! Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis."
conditional probability distribution,"Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis."
ai control problem,"In addition, some scholars argue that research into the AI control problem might be useful in preventing unintended consequences from existing weak AI. !! In artificial intelligence (AI) and philosophy, AI alignment and the AI control problem are aspects of how to build AI systems such that they will aid rather than harm their creators."
data analysis,"Data analysis is a process of inspecting, cleansing, transforming, and modelling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. !! In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour. !! Data science is a ""concept to unify statistics, data analysis, informatics, and their related methods"" in order to ""understand and analyze actual phenomena"" with data. !! In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively. !! Data mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. !! Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. !! Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology. !! In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). !! Computational Statistics & Data Analysis is a monthly peer-reviewed scientific journal covering research on and applications of computational statistics and data analysis."
stochastic differential equations,"Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology."
markov chains,"Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology. !! Markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics."
deep boltzmann machine,"A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. !! Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines."
deep belief networks,"Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines."
deep generative models,"Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines."
deep boltzmann machines,"Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines."
object-oriented design,"Object-oriented design is a method of design encompassing the process of object-oriented decomposition and a notation for depicting both logical and physical as well as state and dynamic models of the system under design. !! In object-oriented design, the dependency inversion principle is a specific methodology for loosely coupling software modules. !! The input for object-oriented design is provided by the output of object-oriented analysis. !! Object-oriented design is the discipline of defining the objects and their interactions to solve a problem that was identified and documented during object-oriented analysis. !! What follows is a description of the class-based subset of object-oriented design, which does not include object prototype-based approaches where objects are not typically obtained by instantiating classes but by cloning other (prototype) objects. !! Object-oriented design is the process of planning a system of interacting objects for the purpose of solving a software problem."
software problem,Object-oriented design is the process of planning a system of interacting objects for the purpose of solving a software problem.
ambient networks,Ambient networks is a network integration design that seeks to solve problems relating to switching between networks to maintain contact with the outside world. !! Ambient Networks was a collaborative project within the European Union's Sixth Framework Programme that investigates future communications systems beyond fixed and 3rd generation mobile networks.
fibonacci search technique,"In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. !! Applications of Fibonacci numbers include computer algorithms such as the Fibonacci search technique and the Fibonacci heap data structure, and graphs called Fibonacci cubes used for interconnecting parallel and distributed systems."
internet applications,"An application delivery network (ADN) is a suite of technologies that, when deployed together, provide availability, security, visibility, and acceleration for Internet applications such as websites."
greedy algorithm,"Typically, a greedy algorithm is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step. !! The choice made by a greedy algorithm may depend on choices made so far, but not on future choices or all the solutions to the subproblem. !! In computer science, Prim's algorithm (also known as Jarnk's algorithm) is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. !! Greedy algorithms produce good solutions on some mathematical problems, but not on others. !! In other words, a greedy algorithm never reconsiders its choices. !! In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids and give constant-factor approximations to optimization problems with the submodular structure. !! A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage."
vector space,"In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. !! However the complex Hermitian matrices do form a vector space over the real numbers R. In the 2n2-dimensional vector space of complex nn matrices over R, the complex Hermitian matrices form a subspace of dimension n2. !! In mathematics, more specifically in multilinear algebra, an alternating multilinear map is a multilinear map with all arguments belonging to the same vector space (for example, a bilinear form or a multilinear form) that is zero whenever any pair of arguments is equal."
linear combinations,"Principal component analysis creates variables that are linear combinations of the original variables. !! The observed variables are modelled as linear combinations of the potential factors plus ""error"" terms, hence factor analysis can be thought of as a special case of errors-in-variables models. !! Basic Linear Algebra Subprograms (BLAS) is a specification that prescribes a set of low-level routines for performing common linear algebra operations such as vector addition, scalar multiplication, dot products, linear combinations, and matrix multiplication."
phase distinction,"Most statically typed languages conform to the principle of phase distinction. !! A concise rule for determining whether phase distinction is preserved in a language or not has been proposed by Luca Cardelli - If A is a compile-time term and B is a subterm of A, then B must also be a compile-time term. !! In an optimizing compiler, phase distinction marks the boundary between expressions which are safe to erase. !! A language with phase distinction may have separate namespaces for types and run-time variables. !! Phase Distinction is a property of programming languages that observe a strict division between types and terms."
optimizing compiler,"In an optimizing compiler, phase distinction marks the boundary between expressions which are safe to erase."
transfer entropy,"While it was originally defined for bivariate analysis, transfer entropy has been extended to multivariate forms, either conditioning on other potential source variables or considering transfer from a collection of sources, although these forms require more samples again. !! Transfer entropy is a non-parametric statistic measuring the amount of directed (time-asymmetric) transfer of information between two random processes. !! Transfer entropy reduces to Granger causality for vector auto-regressive processes. !! Transfer entropy from a process X to another process Y is the amount of uncertainty reduced in future values of Y by knowing the past values of X given past values of Y. !! The above definition of transfer entropy has been extended by other types of entropy measures such as Rnyi entropy."
ux design,"User experience design (UX design, UXD, UED, or XD) is the process of creating evidence-based, interaction designs between human users and products or websites."
user experience design,"User experience design (UX design, UXD, UED, or XD) is the process of creating evidence-based, interaction designs between human users and products or websites. !! User experience design draws from design approaches like human-computer interaction and user-centered design, and includes elements from similar disciplines like interaction design, visual design, information architecture, user research, and others. !! The section of usability that intersects with user experience design is related to humans' ability to use a system or application. !! Therefore, User Experience Design evolved into a multidisciplinary design branch that involves multiple technical aspects from motion graphics design and animation to programming. !! The field of user experience design is a conceptual design discipline and has its roots in human factors and ergonomics, a field that, since the late 1940s, has focused on the interaction between human users, machines, and the contextual environments to design systems that address the user's experience."
conceptual design discipline,"The field of user experience design is a conceptual design discipline and has its roots in human factors and ergonomics, a field that, since the late 1940s, has focused on the interaction between human users, machines, and the contextual environments to design systems that address the user's experience."
visual design,"User experience design draws from design approaches like human-computer interaction and user-centered design, and includes elements from similar disciplines like interaction design, visual design, information architecture, user research, and others. !! The field of information visualization has emerged ""from research in humancomputer interaction, computer science, graphics, visual design, psychology, and business methods."
information architecture,"User experience design draws from design approaches like human-computer interaction and user-centered design, and includes elements from similar disciplines like interaction design, visual design, information architecture, user research, and others."
motion graphics design,"Therefore, User Experience Design evolved into a multidisciplinary design branch that involves multiple technical aspects from motion graphics design and animation to programming."
weighted graph,"In computer science, the maximum weight matching problem is the problem of finding, in a weighted graph, a matching in which the sum of weights is maximized."
flowers method,"time algorithm to find a maximum matching or a maximum weight matching in a graph that is not bipartite; it is due to Jack Edmonds, is called the paths, trees, and flowers method or simply Edmonds' algorithm, and uses bidirected edges."
approximation algorithm,"Gap reductions can be used to demonstrate inapproximability results, as if a problem may be approximated to a better factor than the size of gap, then the approximation algorithm can be used to solve the corresponding gap problem. !! Their work proposes an approximation algorithm for the maximum weight matching problem, which runs in linear time for any fixed error bound."
fingerprinting algorithms,Some fingerprinting algorithms allow the fingerprint of a composite file to be computed from the fingerprints of its constituent parts.
sequential linear programming,"Successive Linear Programming (SLP), also known as Sequential Linear Programming, is an optimization technique for approximately solving nonlinear optimization problems."
successive linear programming,"Nonlinear Optimization by Successive Linear Programming. !! Successive Linear Programming (SLP), also known as Sequential Linear Programming, is an optimization technique for approximately solving nonlinear optimization problems."
nonlinear optimization,Nonlinear Optimization by Successive Linear Programming.
markup languages,"Some markup languages, such as the widely used HTML, have pre-defined presentation semantics, meaning that their specification prescribes some aspects of how to present the structured data on particular media. !! Older markup languages, which typically focus on typography and presentation, include troff, TeX and LaTeX. !! One extremely important characteristic of most markup languages is that they allow intermingling markup with document content such as text and pictures. !! These are sometimes called lightweight markup languages. !! In recent years, a number of markup languages have been developed with ease of use as a key goal, and without input from standards organizations, aimed at allowing authors to create formatted text via web browsers, for example in wikis and in web forums."
structured data,"Some markup languages, such as the widely used HTML, have pre-defined presentation semantics, meaning that their specification prescribes some aspects of how to present the structured data on particular media."
sparse grid,"""Sparse Grids in a Nutshell"" (PDF). !! Sparse grids are numerical techniques to represent, integrate or interpolate high dimensional functions. !! Sparse Grids and Applications. !! ""Sparse Grids"" (PDF). !! Using Adaptive Sparse Grids to Solve High-Dimensional Dynamic Models."
sparse grids,"""Sparse Grids in a Nutshell"" (PDF). !! Sparse Grids and Applications. !! Sparse grids are numerical techniques to represent, integrate or interpolate high dimensional functions. !! ""Sparse Grids"" (PDF)."
common language runtime,"The Common Language Runtime (CLR), the virtual machine component of Microsoft ."
directed edge,"In the theory of dynamical systems, the state space of a discrete system defined by a function can be modeled as a directed graph where each possible state of the dynamical system is represented by a vertex with a directed edge from a to b if and only if (a) = b."
directed graph,"In the theory of dynamical systems, the state space of a discrete system defined by a function can be modeled as a directed graph where each possible state of the dynamical system is represented by a vertex with a directed edge from a to b if and only if (a) = b. !! In mathematics, and more specifically in graph theory, a directed graph (or digraph) is a graph that is made up of a set of vertices connected by directed edges often called arcs. !! In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation. !! On the other hand, the aforementioned definition allows a directed graph to have loops (that is, arcs that directly connect nodes with themselves), but some authors consider a narrower definition that doesn't allow directed graphs to have loops. !! More specifically, directed graphs without loops are addressed as simple directed graphs, while directed graphs with loops are addressed as loop-digraphs (see section Types of directed graphs). !! In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. !! In the field of computer science, a pre-topological order or pre-topological ordering of a directed graph is a linear ordering of its vertices such that if there is a directed path from vertex u to vertex v and v comes before u in the ordering, then there is also a directed path from vertex v to vertex u. !! A rooted tree itself has been defined by some authors as a directed graph. !! The aforementioned definition does not allow a directed graph to have multiple arrows with the same source and target nodes, but some authors consider a broader definition that allows directed graphs to have such multiple arcs (namely, they allow the arc set to be a multiset). !! It differs from an ordinary or undirected graph, in that the latter is defined in terms of unordered pairs of vertices, which are usually called edges, links or lines."
compound algorithm,"In machine learning, weighted majority algorithm (WMA) is a meta learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts."
weighted majority algorithm,"There are many variations of the weighted majority algorithm to handle different situations, like shifting targets, infinite pools, or randomized predictions. !! In machine learning, weighted majority algorithm (WMA) is a meta learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts."
prediction algorithms,"In machine learning, weighted majority algorithm (WMA) is a meta learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts."
randomized predictions,"There are many variations of the weighted majority algorithm to handle different situations, like shifting targets, infinite pools, or randomized predictions."
stochastic optimization,"The way in which results of stochastic optimization algorithms are usually presented (e. g. , presenting only the average, or even the best, out of N runs without any mention of the spread), may also result in a positive bias towards randomness. !! In such cases, online optimization can be used, which is different from other approaches such as robust optimization, stochastic optimization and Markov decision processes. !! Stochastic optimization (SO) methods are optimization methods that generate and use random variables. !! Stochastic optimization methods generalize deterministic methods for deterministic problems. !! Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. !! Stochastic optimization methods also include methods with random iterates."
stochastic optimization algorithms,"The way in which results of stochastic optimization algorithms are usually presented (e. g. , presenting only the average, or even the best, out of N runs without any mention of the spread), may also result in a positive bias towards randomness."
polymorphic recursion,"As these systems assume the expressions have already been typed in an underlying type system (not necessary employing polymorphic recursion), inference can be made decidable again. !! In type-based program analysis polymorphic recursion is often essential in gaining high precision of the analysis. !! Type inference for polymorphic recursion is equivalent to semi-unification and therefore undecidable and requires the use of a semi-algorithm or programmer supplied type annotations. !! Notable examples of systems employing polymorphic recursion include Dussart, Henglein and Mossin's binding-time analysis and the TofteTalpin region-based memory management system. !! In computer science, polymorphic recursion (also referred to as MilnerMycroft typability or the MilnerMycroft calculus) refers to a recursive parametrically polymorphic function where the type parameter changes with each recursive invocation made, instead of staying constant."
type inference,"In such a setting, type inference cannot only become more complex, but also more helpful, as it allows to collect a complete description of everything in a composed scene, while still being able to detect conflicting or unintended uses. !! If there is a way to derive a type for E, then we have accomplished type inference. !! Type inference refers to the automatic detection of the type of an expression in a formal language. !! Some languages that include type inference include C++11, C# (starting with version 3. !! Type inference for polymorphic recursion is equivalent to semi-unification and therefore undecidable and requires the use of a semi-algorithm or programmer supplied type annotations. !! The process of deducing the type from an expression and its context is type inference."
structured svm,"After training, the structured SVM model allows one to predict for new sample instances the corresponding output label; that is, given a natural language sentence, the classifier can produce the most likely parse tree. !! Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels. !! , the structured SVM minimizes the following regularized risk function. !! The standard structured SVM primal formulation is given as follows."
multiclass classification,"Whereas the SVM classifier supports binary classification, multiclass classification and regression, the structured SVM allows training of a classifier for general structured output labels. !! Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to."
global memory,"One type of data occurring simultaneously in different cache memory is called cache coherence, or in some systems, global memory."
program optimization,"Program analysis focuses on two major areas: program optimization and program correctness. !! In computer science, program optimization, code optimization, or software optimization is the process of modifying a software system to make some aspect of it work more efficiently or use fewer resources. !! Meta-heuristics and machine learning are used to address the complexity of program optimization. !! Since many parameters influence the program performance, the program optimization space is large. !! In computing, partial evaluation is a technique for several different types of program optimization by specialization."
implementation language,"If Istatic is source code designed to run inside that interpreter, then partial evaluation of the interpreter with respect to this data/program produces prog*, a version of the interpreter that only runs that source code, is written in the implementation language of the interpreter, does not require the source code to be resupplied, and runs faster than the original combination of the interpreter and the source."
reward-based selection,Reward-based selection is a technique used in evolutionary algorithms for selecting potentially useful solutions for recombination. !! Reward-based selection can quickly identify the most fruitful directions of search by maximizing the cumulative reward of individuals. !! Reward-based selection can be used within Multi-armed bandit framework for Multi-objective optimization to obtain a better approximation of the Pareto front.
cumulative reward,Reward-based selection can quickly identify the most fruitful directions of search by maximizing the cumulative reward of individuals. !! Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward.
heuristic method,"In computer science, the min-conflicts algorithm is a search algorithm or heuristic method to solve constraint satisfaction problems."
conflicts algorithm,"In computer science, the min-conflicts algorithm is a search algorithm or heuristic method to solve constraint satisfaction problems."
symbolic-numeric computation,"In mathematics and computer science, symbolic-numeric computation is the use of software that combines symbolic and numeric methods to solve problems. !! Symbolic-numeric Computation. !! The Fourth International Workshop on Symbolic-Numeric Computation (SNC2011)."
algorithmic paradigm,"In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement. !! In computational geometry, a sweep line algorithm or plane sweep algorithm is an algorithmic paradigm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. !! An algorithmic paradigm or algorithm design paradigm is a generic model or framework which underlies the design of a class of algorithms. !! An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program."
exhaustive search,"In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement."
brute-force search,"While a brute-force search is simple to implement and will always find a solution if it exists, implementation costs are proportional to the number of candidate solutions which in many practical problems tends to grow very quickly as the size of the problem increases (Combinatorial explosion). !! Indeed, brute-force search can be viewed as the simplest metaheuristic. !! In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement. !! Brute-force search is also useful as a baseline method when benchmarking other algorithms or metaheuristics. !! Therefore, brute-force search is typically used when the problem size is limited, or when there are problem-specific heuristics that can be used to reduce the set of candidate solutions to a manageable size. !! Brute-force attacks are an application of brute-force search, the general problem-solving technique of enumerating all candidates and checking each one."
combinatorial explosion,"While a brute-force search is simple to implement and will always find a solution if it exists, implementation costs are proportional to the number of candidate solutions which in many practical problems tends to grow very quickly as the size of the problem increases (Combinatorial explosion)."
bounds checking,"Because performing bounds checking during every usage is time-consuming, it is not always done. !! Many programming languages, such as C, never perform automatic bounds checking to raise speed. !! The D and OCaml languages have run time bounds checking that is enabled or disabled with a compiler switch. !! Bounds-checking elimination is a compiler optimization technique that eliminates unneeded bounds checking. !! In computer programming, bounds checking is any method of detecting whether a variable is within some bounds before it is used."
compiler switch,The D and OCaml languages have run time bounds checking that is enabled or disabled with a compiler switch.
ocaml languages,The D and OCaml languages have run time bounds checking that is enabled or disabled with a compiler switch.
compressed sensing,This article is about Compressed sensing in speech signals. !! One of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery.
delta encoding,"Delta encoding is sometimes called delta compression, particularly where archival histories of changes are required (e. g. , in revision control software). !! Delta encoding is a way of storing or transmitting data in the form of differences (deltas) between sequential data rather than complete files; more generally this is known as data differencing. !! A variation of delta encoding which encodes differences between the prefixes or suffixes of strings is called incremental encoding. !! Unfortunately, not even all 8-bit sound samples compress better when delta encoded, and the usability of delta encoding is even smaller for 16-bit and better samples. !! In situations where differences are small for example, the change of a few words in a large document or the change of a few records in a large table delta encoding greatly reduces data redundancy."
revision control software,"Delta encoding is sometimes called delta compression, particularly where archival histories of changes are required (e. g. , in revision control software)."
predicate logic,"First-order logicalso known as predicate logic, quantificational logic, and first-order predicate calculusis a collection of formal systems used in mathematics, philosophy, linguistics, and computer science."
erp software,"The software structure, modularization, core algorithms and main interfaces do not differ from other ERPs, and ERP software suppliers manage to adapt their systems to government agencies. !! This is because the procedure can be readily codified within the ERP software and replicated with confidence across multiple businesses that share that business requirement. !! It is therefore crucial that organizations thoroughly analyze processes before they deploy an ERP software. !! Two-tier ERP software and hardware lets companies run the equivalent of two ERP systems at once: one at the corporate level and one at the division or subsidiary level."
core algorithms,"The software structure, modularization, core algorithms and main interfaces do not differ from other ERPs, and ERP software suppliers manage to adapt their systems to government agencies."
software structure,"The software structure, modularization, core algorithms and main interfaces do not differ from other ERPs, and ERP software suppliers manage to adapt their systems to government agencies."
gaussian mixture model,"In speaker diarisation one of the most popular methods is to use a Gaussian mixture model to model each of the speakers, and assign the corresponding frames for each speaker with the help of a Hidden Markov Model."
sweep line algorithm,"In computational geometry, a sweep line algorithm or plane sweep algorithm is an algorithmic paradigm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. !! Topological sweeping is a form of the plane sweep with a relaxed ordering of processing points, which avoids the necessity of completely sorting the points; it allows some sweep line algorithms to be performed more efficiently."
computational geometry,"Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics. !! While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity. !! Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. !! The range searching problem and the data structures that solve it are a fundamental topic of computational geometry. !! Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. !! In computational geometry, a sweep line algorithm or plane sweep algorithm is an algorithmic paradigm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. !! The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization. !! Geometric modeling is a branch of applied mathematics and computational geometry that studies methods and algorithms for the mathematical description of shapes. !! Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory."
sweep surface,"In computational geometry, a sweep line algorithm or plane sweep algorithm is an algorithmic paradigm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space."
plane sweep,"Topological sweeping is a form of the plane sweep with a relaxed ordering of processing points, which avoids the necessity of completely sorting the points; it allows some sweep line algorithms to be performed more efficiently."
sweep line algorithms,"Topological sweeping is a form of the plane sweep with a relaxed ordering of processing points, which avoids the necessity of completely sorting the points; it allows some sweep line algorithms to be performed more efficiently."
lazy evaluation,"In programming language theory, lazy evaluation, or call-by-need, is an evaluation strategy which delays the evaluation of an expression until its value is needed (non-strict evaluation) and which also avoids repeated evaluations (sharing). !! Lazy evaluation is often combined with memoization, as described in Jon Bentley's Writing Efficient Programs. !! Lazy evaluation was introduced for lambda calculus by Christopher Wadsworth and employed by the Plessey System 250 as a critical part of a Lambda-Calculus Meta-Machine, reducing the resolution overhead for access to objects in a capability-limited address space. !! Efficient purely functional data structures may require the use of lazy evaluation and memoization. !! The opposite of lazy evaluation is eager evaluation, sometimes known as strict evaluation. !! Lazy evaluation is difficult to combine with imperative features such as exception handling and input/output, because the order of operations becomes indeterminate."
imperative features,"Lazy evaluation is difficult to combine with imperative features such as exception handling and input/output, because the order of operations becomes indeterminate."
programming language theory,"In programming language theory, lazy evaluation, or call-by-need, is an evaluation strategy which delays the evaluation of an expression until its value is needed (non-strict evaluation) and which also avoids repeated evaluations (sharing)."
exception handling,"Structured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed. !! Lazy evaluation is difficult to combine with imperative features such as exception handling and input/output, because the order of operations becomes indeterminate."
decision rules,A data-flow diagram has no control flowthere are no decision rules and no loops.
structured analysis,"Structured analysis and design techniques are fundamental tools of systems analysis. !! The data-flow diagram is a tool that is part of structured analysis and data modeling. !! These techniques were combined in various published system development methodologies, including structured systems analysis and design method, profitable information by design (PRIDE), Nastec structured analysis & design, SDM/70 and the Spectrum structured system development methodology. !! In software engineering, structured analysis (SA) and structured design (SD) are methods for analyzing business requirements and developing specifications for converting practices into computer programs, hardware configurations, and related manual procedures. !! Structured analysis consists of interpreting the system concept (or real world situations) into data and control terminology represented by data flow diagrams. !! Structured analysis became popular in the 1980s and is still in use today."
markup language,"Some markup languages, such as the widely used HTML, have pre-defined presentation semantics, meaning that their specification prescribes some aspects of how to present the structured data on particular media. !! Older markup languages, which typically focus on typography and presentation, include troff, TeX and LaTeX. !! One extremely important characteristic of most markup languages is that they allow intermingling markup with document content such as text and pictures. !! These are sometimes called lightweight markup languages. !! In recent years, a number of markup languages have been developed with ease of use as a key goal, and without input from standards organizations, aimed at allowing authors to create formatted text via web browsers, for example in wikis and in web forums."
data-flow diagram,"A data-flow diagram has no control flowthere are no decision rules and no loops. !! The refined representation of a process can be done in another data-flow diagram, which subdivides this process into sub-processes. !! A data-flow diagram is a way of representing a flow of data through a process or a system (usually an information system). !! The data-flow diagram is a tool that is part of structured analysis and data modeling. !! There are several notations for displaying data-flow diagrams."
combinatorial data analysis,"In statistics, combinatorial data analysis (CDA) is the study of data sets where the order in which objects are arranged is important."
rule-based machine learning,"Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply. !! Rule-based machine learning approaches include learning classifier systems, association rule learning, artificial immune systems, and any other method that relies on a set of rules, each covering contextual knowledge. !! Therefore rule-based machine learning methods typically comprise a set of rules, or knowledge base, that collectively make up the prediction model. !! While rule-based machine learning is conceptually a type of rule-based system, it is distinct from traditional rule-based systems, which are often hand-crafted, and other rule-based decision makers. !! This is because rule-based machine learning applies some form of learning algorithm to automatically identify useful rules, rather than a human needing to apply prior domain knowledge to manually construct rules and curate a rule set."
association rule learning,"Rule-based machine learning approaches include learning classifier systems, association rule learning, artificial immune systems, and any other method that relies on a set of rules, each covering contextual knowledge."
learning algorithm,"This is because rule-based machine learning applies some form of learning algorithm to automatically identify useful rules, rather than a human needing to apply prior domain knowledge to manually construct rules and curate a rule set. !! Interest in learning classifier systems was reinvigorated in the mid 1990s largely due to two events; the development of the Q-Learning algorithm for reinforcement learning, and the introduction of significantly simplified Michigan-style LCS architectures by Stewart Wilson."
prediction model,"Therefore rule-based machine learning methods typically comprise a set of rules, or knowledge base, that collectively make up the prediction model."
knowledge base,"In ""Applications of Abduction: Knowledge-Level Modeling"", Menzies proposes a new knowledge level modeling approach, called KLB, which specifies that ""a knowledge base should be divided into domain-specific facts and domain-independent abstract problem solving inference procedures. "" !! Therefore rule-based machine learning methods typically comprise a set of rules, or knowledge base, that collectively make up the prediction model."
graph polynomial,"In mathematics, a graph polynomial is a graph invariant whose values are polynomials."
decision model,"In business analysis, the Decision Model and Notation (DMN) is a standard published by the Object Management Group."
business analysis,"In business analysis, the Decision Model and Notation (DMN) is a standard published by the Object Management Group."
layered hidden markov model,"A layered hidden Markov model (LHMM) consists of N levels of HMMs, where the HMMs on level i + 1 correspond to observation symbols or probability generators at level i. !! The layered hidden Markov model (LHMM) is a statistical model derived from the hidden Markov model (HMM)."
private key set,Distributed key generation (DKG) is a cryptographic process in which multiple parties contribute to the calculation of a shared public and private key set.
cryptographic process,Distributed key generation (DKG) is a cryptographic process in which multiple parties contribute to the calculation of a shared public and private key set.
distributed key generation,"The involvement of many parties requires Distributed key generation to ensure secrecy in the presence of malicious contributions to the key calculation. !! Distributed key generation prevents single parties from having access to a private key. !! Distributed Key Generation is commonly used to decrypt shared ciphertexts or create group digital signatures. !! Distributed key generation (DKG) is a cryptographic process in which multiple parties contribute to the calculation of a shared public and private key set. !! Unlike most public key encryption models, distributed key generation does not rely on Trusted Third Parties."
public key encryption models,"Unlike most public key encryption models, distributed key generation does not rely on Trusted Third Parties."
decision model and notation,"Constraint Decision Model and Notation (cDMN) is a formal notation for expressing knowledge in a tabular, intuitive format. !! In business analysis, the Decision Model and Notation (DMN) is a standard published by the Object Management Group."
private key,Distributed key generation prevents single parties from having access to a private key.
mnist database,"Visualization of the MNIST database groups of images of MNIST handwritten digits on GitHub !! The MNIST database contains 60,000 training images and 10,000 testing images. !! The set of images in the MNIST database was created in 1998 as a combination of two of NIST's databases: Special Database 1 and Special Database 3. !! The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. !! Some researchers have achieved ""near-human performance"" on the MNIST database, using a committee of neural networks; in the same paper, the authors achieve performance double that of humans on other recognition tasks."
recognition tasks,"Some researchers have achieved ""near-human performance"" on the MNIST database, using a committee of neural networks; in the same paper, the authors achieve performance double that of humans on other recognition tasks."
comparative sorting algorithm,"In computer science, radix sort is a non-comparative sorting algorithm."
radix sort,"In computer science, radix sort is a non-comparative sorting algorithm. !! Radix sort can be applied to data that can be sorted lexicographically, be they integers, words, punch cards, playing cards, or the mail. !! Radix sorting algorithms came into common use as a way to sort punched cards as early as 1923. !! Radix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines. !! For this reason, radix sort has also been called bucket sort and digital sort."
computer algebra systems,"The development of the computer algebra systems in the second half of the 20th century is part of the discipline of ""computer algebra"" or ""symbolic computation"", which has spurred work in algorithms over mathematical objects such as polynomials. !! This large amount of required computer capabilities explains the small number of general-purpose computer algebra systems. !! Computer algebra systems may be divided into two classes: specialized and general-purpose. !! Computer algebra systems began to appear in the 1960s and evolved out of two quite different sourcesthe requirements of theoretical physicists and research into artificial intelligence. !! General-purpose computer algebra systems aim to be useful to a user working in any scientific field that requires manipulation of mathematical expressions."
computer algebra,"Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc. !! The development of the computer algebra systems in the second half of the 20th century is part of the discipline of ""computer algebra"" or ""symbolic computation"", which has spurred work in algorithms over mathematical objects such as polynomials. !! In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. !! Some authors distinguish computer algebra from symbolic computation using the latter name to refer to kinds of symbolic computation other than the computation with mathematical formulas. !! Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols. !! Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs."
shuffling algorithm,"For this reason, many online gambling sites provide descriptions of their shuffling algorithms and the sources of randomness used to drive these algorithms, with some gambling sites also providing auditors' reports of the performance of their systems."
shuffling algorithms,"For this reason, many online gambling sites provide descriptions of their shuffling algorithms and the sources of randomness used to drive these algorithms, with some gambling sites also providing auditors' reports of the performance of their systems."
multiplexing technique,"Spatial multiplexing or space-division multiplexing (often abbreviated SM, SDM or SMX) is a multiplexing technique in MIMO wireless communication, fibre-optic communication and other communications technologies used to transmit independent channels separated in space."
spatial multiplexing,"Spatial multiplexing or space-division multiplexing (often abbreviated SM, SDM or SMX) is a multiplexing technique in MIMO wireless communication, fibre-optic communication and other communications technologies used to transmit independent channels separated in space. !! One such extension which is being considered for DVB-NGH systems is the so-called enhanced Spatial Multiplexing (eSM) scheme. !! An often encountered problem in open loop spatial multiplexing is to guard against instance of high channel correlation and strong power imbalances between the multiple streams."
random binary tree,"Adding and removing nodes directly in a random binary tree will in general disrupt its random structure, but the treap and related randomized binary search tree data structures use the principle of binary trees formed from a random permutation in order to maintain a balanced binary search tree dynamically as nodes are inserted and deleted. !! The searching on l's random binary tree only counts max records on (2, 1) and min records on (4, 6, 5, 1)this is why it is twice a Harmonic number. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! In some cases the analysis of random binary trees under the random permutation model can be automatically transferred to the uniform model. !! In applications of binary search tree data structures, it is rare for the values in the tree to be inserted without deletion in a random order, limiting the direct applications of random binary trees."
binary search tree data structures,"In applications of binary search tree data structures, it is rare for the values in the tree to be inserted without deletion in a random order, limiting the direct applications of random binary trees."
random binary trees,"In some cases the analysis of random binary trees under the random permutation model can be automatically transferred to the uniform model. !! In applications of binary search tree data structures, it is rare for the values in the tree to be inserted without deletion in a random order, limiting the direct applications of random binary trees."
random permutation model,In some cases the analysis of random binary trees under the random permutation model can be automatically transferred to the uniform model.
User interface design,"There are several phases and processes in the user interface design, some of which are more demanded upon than others, depending on the project. !! Good user interface design facilitates finishing the task at hand without drawing unnecessary attention to itself. !! The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design). !! User interface design is a craft in which designers, perform an important function in creating the user experience. !! User interface design requires a good understanding of user needs."
user interface design,"Generally, the goal of user interface design is to produce a user interface that makes it easy, efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result (i. e. maximum usability). !! There are several phases and processes in the user interface design, some of which are more demanded upon than others, depending on the project. !! User interface design is a craft in which designers, perform an important function in creating the user experience. !! The goal of user interface design is to make the user's interaction as simple and efficient as possible, in terms of accomplishing user goals (user-centered design)."
batch learning techniques,"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once."
online machine learning,"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. !! Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e. g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning."
conjunctive normal form,"In computational complexity theory, the maximum satisfiability problem (MAX-SAT) is the problem of determining the maximum number of clauses, of a given Boolean formula in conjunctive normal form, that can be made true by an assignment of truth values to the variables of the formula."
logical variables,"A fuzzy control system is a control system based on fuzzy logica mathematical system that analyzes analog input values in terms of logical variables that take on continuous values between 0 and 1, in contrast to classical or digital logic, which operates on discrete values of either 1 or 0 (true or false, respectively). !! A truth table is a mathematical table used in logicspecifically in connection with Boolean algebra, boolean functions, and propositional calculuswhich sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables."
fuzzy control system,"The US Environmental Protection Agency has investigated fuzzy control for energy-efficient motors, and NASA has studied fuzzy control for automated space docking: simulations show that a fuzzy control system can greatly reduce fuel consumption. !! The fuzzy control system uses 13 rules and requires 1. !! The camera's fuzzy control system uses 12 inputs: 6 to obtain the current clarity data provided by the CCD and 6 to measure the rate of change of lens movement. !! Interest in fuzzy systems was sparked by Seiji Yasunobu and Soji Miyamoto of Hitachi, who in 1985 provided simulations that demonstrated the feasibility of fuzzy control systems for the Sendai Subway. !! A fuzzy control system is a control system based on fuzzy logica mathematical system that analyzes analog input values in terms of logical variables that take on continuous values between 0 and 1, in contrast to classical or digital logic, which operates on discrete values of either 1 or 0 (true or false, respectively)."
digital logic,"A carry-lookahead adder (CLA) or fast adder is a type of electronics adder used in digital logic. !! A fuzzy control system is a control system based on fuzzy logica mathematical system that analyzes analog input values in terms of logical variables that take on continuous values between 0 and 1, in contrast to classical or digital logic, which operates on discrete values of either 1 or 0 (true or false, respectively)."
fuzzy control systems,"Interest in fuzzy systems was sparked by Seiji Yasunobu and Soji Miyamoto of Hitachi, who in 1985 provided simulations that demonstrated the feasibility of fuzzy control systems for the Sendai Subway."
fuzzy systems,"Interest in fuzzy systems was sparked by Seiji Yasunobu and Soji Miyamoto of Hitachi, who in 1985 provided simulations that demonstrated the feasibility of fuzzy control systems for the Sendai Subway."
polynomial functions,"According to the definition of polynomial functions, there may be expressions that obviously are not polynomials but nevertheless define polynomial functions. !! For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated scientific problems; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. !! The most commonly encountered symmetric functions are polynomial functions, which are given by the symmetric polynomials. !! If R is commutative, then one can associate with every polynomial P in R[x] a polynomial function f with domain and range equal to R. (More generally, one can take domain and range to be any same unital associative algebra over R. ) One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that, over some rings, different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p). !! While polynomial functions are defined for all values of the variables, a rational function is defined only for the values of the variables for which the denominator is not zero. !! Generally, unless otherwise specified, polynomial functions have complex coefficients, arguments, and values."
linearly dependent,"A system of linear equations behave differently from the general case if the equations are linearly dependent, or if it is inconsistent and has no more equations than unknowns."
probabilistic turing machine,"The most basic randomized complexity class is RP, which is the class of decision problems for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2."
bayesian svm,"Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data. !! Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM."
bayesian kernel support vector machine,"Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM."
stochastic version,"Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM."
linear bayesian svm,"Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM."
online algorithms,"For many problems, online algorithms cannot match the performance of offline algorithms. !! Because it does not know the whole input, an online algorithm is forced to make decisions that may later turn out not to be optimal, and the study of online algorithms has focused on the quality of decision-making that is possible in this setting. !! In operations research, the area in which online algorithms are developed is called online optimization. !! A problem exemplifying the concepts of online algorithms is the Canadian Traveller Problem. !! Bibliography of papers on online algorithms"
offline algorithms,"For many problems, online algorithms cannot match the performance of offline algorithms."
programmable logic controller,"System migration involves moving a set of instructions or programs, e. g. , PLC (programmable logic controller) programs, from one platform to another, minimizing reengineering."
hilbert space,"In functional analysis and quantum measurement theory, a positive-operator-valued measure (POVM) is a measure whose values are positive semi-definite operators on a Hilbert space."
abstract algorithms,"Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages."
abstract data types,"Term algebras also play a role in the semantics of abstract data types, where an abstract data type declaration provides the signature of a multi-sorted algebraic structure and the term algebra is a concrete model of the abstract declaration. !! Data structures serve as the basis for abstract data types (ADT). !! The reason for introducing the notion of abstract data types was to allow interchangeable software modules. !! The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development. !! Modern object-oriented languages, such as C++ and Java, support a form of abstract data types. !! Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages."
software development,"Software engineering is an engineering approach on a software development of systematics application. !! The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development. !! Object-oriented modeling (OOM) is an approach to modeling an application that is used at the beginning of the software life cycle when using an object-oriented approach to software development. !! Common topics of interaction design include design, humancomputer interaction, and software development. !! Software analytics aims to obtain insightful and actionable information from software artifacts that help practitioners accomplish tasks related to software development, systems, and users."
data abstraction,"The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development."
online algorithm,"For many problems, online algorithms cannot match the performance of offline algorithms. !! Because it does not know the whole input, an online algorithm is forced to make decisions that may later turn out not to be optimal, and the study of online algorithms has focused on the quality of decision-making that is possible in this setting. !! In operations research, the area in which online algorithms are developed is called online optimization. !! Thus insertion sort is an online algorithm. !! Randomized algorithms are particularly useful when faced with a malicious ""adversary"" or attacker who deliberately tries to feed a bad input to the algorithm (see worst-case complexity and competitive analysis (online algorithm)) such as in the Prisoner's dilemma. !! In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i. e. , in the order that the input is fed to the algorithm, without having the entire input available from the start. !! If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive."
feedback loop,"Humanistic Intelligence [HI] is intelligence that arises because of a human being in the feedback loop of a computational process, where the human and computer are inextricably intertwined."
humanistic intelligence,"(2002), ""Exploring Humanistic Intelligence Through Physiologically Mediated Reality"" (PDF), Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR'02) 0-7695-1781-1/02, IEEE !! Humanistic Intelligence [HI] is intelligence that arises because of a human being in the feedback loop of a computational process, where the human and computer are inextricably intertwined."
computational process,"Humanistic Intelligence [HI] is intelligence that arises because of a human being in the feedback loop of a computational process, where the human and computer are inextricably intertwined."
DMA attack,"Newer operating systems may take steps to prevent DMA attacks. !! DMA attacks can be prevented by physical security against potentially malicious devices. !! Preventing physical connections to such ports will prevent DMA attacks. !! Systems may still be vulnerable to a DMA attack by an external device if they have a FireWire, ExpressCard, Thunderbolt or other expansion port that, like PCI and PCI Express in general, connects attached devices directly to the physical rather than virtual memory address space. !! A DMA attack is a type of side channel attack in computer security, in which an attacker can penetrate a computer or other device, by exploiting the presence of high-speed expansion ports that permit direct memory access (DMA)."
side channel attack,"A DMA attack is a type of side channel attack in computer security, in which an attacker can penetrate a computer or other device, by exploiting the presence of high-speed expansion ports that permit direct memory access (DMA)."
dma attack,"Systems may still be vulnerable to a DMA attack by an external device if they have a FireWire, ExpressCard, Thunderbolt or other expansion port that, like PCI and PCI Express in general, connects attached devices directly to the physical rather than virtual memory address space. !! A DMA attack is a type of side channel attack in computer security, in which an attacker can penetrate a computer or other device, by exploiting the presence of high-speed expansion ports that permit direct memory access (DMA)."
virtual memory address space,"Systems may still be vulnerable to a DMA attack by an external device if they have a FireWire, ExpressCard, Thunderbolt or other expansion port that, like PCI and PCI Express in general, connects attached devices directly to the physical rather than virtual memory address space."
dma attacks,DMA attacks can be prevented by physical security against potentially malicious devices.
gap analysis naturally flows,Gap analysis naturally flows from benchmarking and from other assessments.
network scheduler,"Weighted round robin (WRR) is a network scheduler for data flows, but also used to schedule processes."
weighted round robin,"Whereas round-robin cycles over the queues or tasks and gives one service opportunity per cycle, weighted round robin offers to each a fixed number of opportunities, as specified by the configured weight which serves to influence the portion of capacity received by each queue or task. !! Weighted round robin (WRR) is a network scheduler for data flows, but also used to schedule processes. !! Like round-robin, weighted round robin scheduling is simple, easy to implement, work conserving and starvation-free. !! Weighted round robin is a generalisation of round-robin scheduling."
fp mining,"Frequent pattern discovery (or FP discovery, FP mining, or Frequent itemset mining) is part of knowledge discovery in databases, Massive Online Analysis, and data mining; it describes the task of finding the most frequent and relevant patterns in large datasets."
fp discovery,"Frequent pattern discovery (or FP discovery, FP mining, or Frequent itemset mining) is part of knowledge discovery in databases, Massive Online Analysis, and data mining; it describes the task of finding the most frequent and relevant patterns in large datasets."
frequent pattern discovery,"Frequent pattern discovery (or FP discovery, FP mining, or Frequent itemset mining) is part of knowledge discovery in databases, Massive Online Analysis, and data mining; it describes the task of finding the most frequent and relevant patterns in large datasets."
knowledge discovery,"Data mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. !! Data stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery. !! Frequent pattern discovery (or FP discovery, FP mining, or Frequent itemset mining) is part of knowledge discovery in databases, Massive Online Analysis, and data mining; it describes the task of finding the most frequent and relevant patterns in large datasets. !! Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure."
skew heap,"Skew heaps are advantageous because of their ability to merge more quickly than binary heaps. !! A skew heap (or self-adjusting heap) is a heap data structure implemented as a binary tree. !! A skew heap is a self-adjusting form of a leftist heap which attempts to maintain balance by unconditionally swapping all nodes in the merge path when merging two heaps. !! With no structural constraints, it may seem that a skew heap would be horribly inefficient. !! Every operation (add, remove_min, merge) on two skew heaps must be done using a special skew heap merge."
skew heaps,Skew heaps are advantageous because of their ability to merge more quickly than binary heaps.
compressed data structure,"The term compressed data structure arises in the computer science subfields of algorithms, data structures, and theoretical computer science. !! When the data are compressible, as is often the case in practice for natural language text, the compressed data structure can occupy space very close to the information-theoretic minimum, and significantly less space than most compression schemes. !! Important examples of compressed data structures include the compressed suffix array and the FM-index, both of which can represent an arbitrary text of characters T for pattern matching. !! The size of the compressed data structure is typically highly dependent upon the entropy of the data being represented. !! In contrast, the size of a compressed data structure depends upon the particular data being represented."
natural language text,"When the data are compressible, as is often the case in practice for natural language text, the compressed data structure can occupy space very close to the information-theoretic minimum, and significantly less space than most compression schemes. !! Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. !! Ontology learning (OL) is used to (semi-)automatically extract whole ontologies from natural language text."
compression schemes,"When the data are compressible, as is often the case in practice for natural language text, the compressed data structure can occupy space very close to the information-theoretic minimum, and significantly less space than most compression schemes."
linear relationships,"Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships."
linear objective function,"More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints."
untyped tree structure,The Abstract Document Pattern allows the developer to store variables like configuration settings in an untyped tree structure and operate on the documents using typed views.
abstract document pattern,The Abstract Document Pattern allows the developer to store variables like configuration settings in an untyped tree structure and operate on the documents using typed views. !! The full implementation of the Abstract Document pattern is available at https://java-design-patterns.
software engineer,"Software engineering is an engineering approach on a software development of systematics application. !! It was difficult to keep up with the hardware which caused many problems for software engineers. !! Beginning in the 1960s, software engineering was seen as its own type of engineering. !! A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. !! Additionally, the development of software engineering was seen as a struggle."
software engineers,"It was difficult to keep up with the hardware which caused many problems for software engineers. !! Software analytics is analytics on software data for managers and software engineers with the aim of empowering software development individuals and teams to gain and share insight form their data to make better decisions. !! Spaghetti code can be caused by several factors, such as volatile project requirements, lack of programming style rules, and software engineers with insufficient ability or experience."
function abstraction,Lambda calculus (also written as -calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution.
turing machine,"The finite-state machine has less computational power than some other models of computation such as the Turing machine. !! Lambda calculus is Turing complete, that is, it is a universal model of computation that can be used to simulate any Turing machine. !! Quantum computing began in 1980 when physicist Paul Benioff proposed a quantum mechanical model of the Turing machine."
typed lambda calculus,"In typed lambda calculus, functions can be applied only if they are capable of accepting the given input's ""type"" of data."
virtual intelligence,"Sun Tzu Virtual Intelligence demonstration, MODSIM World, October 2009 !! With today's VI bots, virtual intelligence has evolved past the constraints of past testing into a new level of the machine's ability to demonstrate intelligence. !! Virtual Intelligence draws a new distinction as to how this application of AI is different due to the environment in which it operates. !! Virtual intelligence is the term given to artificial intelligence that exists within a virtual world."
physical neural network,"A further application of physical neural network is shown in U. S. Patent No. !! A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. !! Alex Nugent describes a physical neural network as one or more nonlinear neuron-like nodes used to sum signals and nanoconnections formed from nanoparticles, nanowires, or nanotubes which determine the signal strength input to the nodes. !! Another example of a physical neural network is taught by U. S. Patent No. !! Numerous applications for such physical neural networks are possible."
neural synapse,A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse.
nonlinear neuron,"Alex Nugent describes a physical neural network as one or more nonlinear neuron-like nodes used to sum signals and nanoconnections formed from nanoparticles, nanowires, or nanotubes which determine the signal strength input to the nodes."
physical neural networks,Numerous applications for such physical neural networks are possible.
search data structure,"Useful search data structures allow faster retrieval; however, they are limited to queries of some specific kind. !! In computer science, a search data structure is any data structure that allows the efficient retrieval of specific items from a set of items, such as a specific record from a database."
sorting lists,Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key values (N) are approximately the same.
pigeonhole sorting,Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key values (N) are approximately the same.
big data applications,"However, since transferring information between the processor and the outside world does still dissipate energy, superconducting computing was seen as well-suited for computations-intensive tasks where the data largely stays in the cryogenic environment, rather than big data applications where large amounts of information are streamed from outside the processor."
digital image processing,"Digital image processing is the use of a digital computer to process digital images through an algorithm. !! Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. !! As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. !! The field of digital image processing is the study of algorithms for their transformation. !! The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased. !! In electrical engineering and computer science, analog image processing is any image processing task conducted on two-dimensional analog signals by analog means (as opposed to digital image processing). !! Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems."
digital signal processing,"The spectral centroid is a measure used in digital signal processing to characterise a spectrum. !! Digital signal processing and analog signal processing are subfields of signal processing. !! Additional technologies for digital signal processing include more powerful general purpose microprocessors, graphics processing units, field-programmable gate arrays (FPGAs), digital signal controllers (mostly for industrial applications such as motor control), and stream processors. !! Although no commercially successful general-purpose computer hardware has used a dataflow architecture, it has been successfully implemented in specialized hardware such as in digital signal processing, network routing, graphics processing, telemetry, and more recently in data warehousing, and artificial intelligence. !! As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. !! Digital signal processing (DSP) is the use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations. !! A digital signal processor (DSP) is a specialized microprocessor chip, with its architecture optimized for the operational needs of digital signal processing. !! The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. !! Digital signal processing is also fundamental to digital technology, such as digital telecommunication and wireless communications. !! Typically, multidimensional signal processing is directly associated with digital signal processing because its complexity warrants the use of computer modelling and computation."
digital picture processing,"Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement."
temporal coupling,"In object-oriented programming, sequential coupling (also known as temporal coupling) is a form of coupling where a class requires its methods to be called in a particular sequence."
template method pattern,Sequential coupling can be refactored with the template method pattern to overcome the problems posed by the usage of this anti-pattern.
support-vector machines,"Transductive support-vector machines were introduced by Vladimir N. Vapnik in 1998. !! In the context of support-vector machines, the optimally separating hyperplane or maximum-margin hyperplane is a hyperplane which separates two convex hulls of points and is equidistant from the two. !! Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support-vector machines (SVM). !! It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support-vector machines, although given enough samples the algorithm still performs well. !! Transductive support-vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. !! In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. !! Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis. !! Multiclass SVM aims to assign labels to instances by using support-vector machines, where the labels are drawn from a finite set of several elements."
lookahead carry unit,A lookahead carry unit (LCU) is a logical unit in digital circuit design used to decrease calculation time in adder units and used in conjunction with carry look-ahead adders (CLAs).
logical unit,A lookahead carry unit (LCU) is a logical unit in digital circuit design used to decrease calculation time in adder units and used in conjunction with carry look-ahead adders (CLAs).
sequential algorithm,"In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially once through, from start to finish, without other processing executing as opposed to concurrently or in parallel. !! ""Sequential algorithm"" may also refer specifically to an algorithm for decoding a convolutional code. !! The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption."
serial algorithm,"In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially once through, from start to finish, without other processing executing as opposed to concurrently or in parallel."
sequential algorithms,"The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption."
concurrent algorithm,"The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption."
cholesky factor,"In numerical analysis the minimum degree algorithm is an algorithm used to permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, to reduce the number of non-zeros in the Cholesky factor."
minimum degree algorithm,"of Markowitz method was described by Tinney and Walker in 1967 and Rose later derived a graph theoretic version of the algorithm where the factorization is only simulated, and this was named the minimum degree algorithm. !! A version of the minimum degree algorithm was implemented in the MATLAB function symmmd (where MMD stands for multiple minimum degree), but has now been superseded by a symmetric approximate multiple minimum degree function symamd, which is faster. !! Minimum degree algorithms are often used in the finite element method where the reordering of nodes can be carried out depending only on the topology of the mesh, rather than the coefficients in the partial differential equation, resulting in efficiency savings when the same mesh is used for a variety of coefficient values. !! The minimum degree algorithm is derived from a method first proposed by Markowitz in 1959 for non-symmetric linear programming problems, which is loosely described as follows. !! In numerical analysis the minimum degree algorithm is an algorithm used to permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, to reduce the number of non-zeros in the Cholesky factor."
minimum degree algorithms,"Minimum degree algorithms are often used in the finite element method where the reordering of nodes can be carried out depending only on the topology of the mesh, rather than the coefficients in the partial differential equation, resulting in efficiency savings when the same mesh is used for a variety of coefficient values."
markowitz method,"of Markowitz method was described by Tinney and Walker in 1967 and Rose later derived a graph theoretic version of the algorithm where the factorization is only simulated, and this was named the minimum degree algorithm."
multi-label classification,"A classifier chain is an alternative method for transforming a multi-label classification problem into several binary classification problems. !! In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. !! 5 algorithm for multi-label classification; the modification involves the entropy calculations. !! Classifier chains is a machine learning method for problem transformation in multi-label classification. !! Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. !! Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y)."
binary vectors,"Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y)."
entropy calculations,5 algorithm for multi-label classification; the modification involves the entropy calculations.
programming language compilers,Finite automata are often used in the frontend of programming language compilers.
finite automata theory,"Kohavi, Z. , Switching and Finite Automata Theory."
spectral centroid,"The spectral centroid is a measure used in digital signal processing to characterise a spectrum. !! Because the spectral centroid is a good predictor of the ""brightness"" of a sound, it is widely used in digital audio and music processing as an automatic measure of musical timbre. !! Some people use ""spectral centroid"" to refer to the median of the spectrum."
mobile software agent,"An automated personal assistant or an Intelligent Personal Assistant is a mobile software agent that can perform tasks, or services, on behalf of an individual based on a combination of user input, location awareness, and the ability to access information from a variety of online sources (such as weather conditions, traffic congestion, news, stock prices, user schedules, retail prices, etc."
mobile computing devices,":815Both types of automated personal assistant technology are enabled by the combination of mobile computing devices, application programming interfaces (APIs), and the proliferation of mobile apps."
logical expressions,"A truth table is a mathematical table used in logicspecifically in connection with Boolean algebra, boolean functions, and propositional calculuswhich sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables."
functional arguments,"A truth table is a mathematical table used in logicspecifically in connection with Boolean algebra, boolean functions, and propositional calculuswhich sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables."
polynomial decomposition theorem,", and the degrees of the components are the same up to linear transformations, but possibly in different order; this is Ritt's polynomial decomposition theorem."
linear transformations,", and the degrees of the components are the same up to linear transformations, but possibly in different order; this is Ritt's polynomial decomposition theorem."
polynomial decomposition,"The first algorithm for polynomial decomposition was published in 1985, though it had been discovered in 1976, and implemented in the Macsyma/Maxima computer algebra system. !! , and the degrees of the components are the same up to linear transformations, but possibly in different order; this is Ritt's polynomial decomposition theorem. !! Polynomial Decomposition"". !! A polynomial decomposition may enable more efficient evaluation of a polynomial. !! A polynomial decomposition enables calculation of symbolic roots using radicals, even for some irreducible polynomials."
nearest neighbor problem,"The predecessor problem is a simple case of the nearest neighbor problem, and data structures that solve it have applications in problems like integer sorting."
runtime environment,"The Common Language Infrastructure (CLI) is an open specification and technical standard originally developed by Microsoft and standardized by ISO (ISO/IEC 23271) and Ecma International (ECMA 335) that describes executable code and a runtime environment that allows multiple high-level languages to be used on different computer platforms without being rewritten for specific architectures. !! When treating the runtime system as distinct from the runtime environment (RTE), the first may be defined as a specific part of the application software (IDE) used for programming, a piece of software that provides the programmer a more convenient environment for running programs during their production (testing and similar) while the second (RTE) would be the very instance of an execution model being applied to the developed program which is itself then run in the aforementioned runtime system."
markov partition,"Markov partitions make homoclinic and heteroclinic orbits particularly easy to describe. !! A Markov partition in mathematics is a tool used in dynamical systems theory, allowing the methods of symbolic dynamics to be applied to the study of hyperbolic dynamics. !! The Markov partition thus allows standard techniques from symbolic dynamics to be applied, including the computation of expectation values, correlations, topological entropy, topological zeta functions, Fredholm determinants and the like. !! By using a Markov partition, the system can be made to resemble a discrete-time Markov process, with the long-term dynamical characteristics of the system represented as a Markov shift. !! Markov partitions have been constructed in several situations."
embedded systems,"During the mid-1990s, many new 32-bit MIPS processors for embedded systems were MIPS II implementations because the introduction of the 64-bit MIPS III architecture in 1991 left MIPS II as the newest 32-bit MIPS architecture until MIPS32 was introduced in 1999. :19MIPS Computer Systems' R4000 microprocessor (1991) was the first MIPS III implementation."
density function,"Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm."
mean shift algorithm,"Although the mean shift algorithm has been widely used in many applications, a rigid proof for the convergence of the algorithm using a general kernel in a high dimensional space is still not known."
database indexing technique,A Block Range Index or BRIN is a database indexing technique.
block range index,A Block Range Index or BRIN is a database indexing technique.
multivariate normal distribution,"In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i. e. every finite linear combination of them is normally distributed."
stochastic process,"In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i. e. every finite linear combination of them is normally distributed."
gaussian process,"Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. !! The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution). !! The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e. g. time or space. !! In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i. e. every finite linear combination of them is normally distributed. !! Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions."
joint distribution,"The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e. g. time or space."
normal distribution,"The chi-squared distribution is used in the common chi-squared tests for goodness of fit of an observed distribution to a theoretical one, the independence of two criteria of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution from a sample standard deviation. !! Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. !! The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution)."
gaussian processes,"Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions. !! Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. !! The concept of Gaussian processes is named after Carl Friedrich Gauss because it is based on the notion of the Gaussian distribution (normal distribution)."
multivariate normal distributions,Gaussian processes can be seen as an infinite-dimensional generalization of multivariate normal distributions.
statistical modelling,"Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. !! Data mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information."
division algorithm,"Division algorithms fall into two main categories: slow division and fast division. !! For these large integers, more efficient division algorithms transform the problem to use a small number of multiplications, which can then be done using an asymptotically efficient multiplication algorithm such as the Karatsuba algorithm, ToomCook multiplication or the SchnhageStrassen algorithm. !! Although very simple, it takes (Q) steps, and so is exponentially slower than even slow division algorithms like long division. !! Slow division algorithms produce one digit of the final quotient per iteration. !! A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of Euclidean division."
toomcook multiplication,"For these large integers, more efficient division algorithms transform the problem to use a small number of multiplications, which can then be done using an asymptotically efficient multiplication algorithm such as the Karatsuba algorithm, ToomCook multiplication or the SchnhageStrassen algorithm. !! Polynomial interpolation is also essential to perform sub-quadratic multiplication and squaring such as Karatsuba multiplication and ToomCook multiplication, where an interpolation through points on a polynomial which defines the product yields the product itself."
support vector machines algorithm,"The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
support vector machines,"Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. !! Support Vector Machines: Hype or Hallelujah !! Some methods for shallow semantic parsing are based on support vector machines. !! An Introduction to Support Vector Machines and other kernel-based learning methods. !! The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
support vectors,"This extends the geometric interpretation of SVMfor linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier. !! The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data."
hinge loss,"Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. !! The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss."
computer security software,"Disk encryption software is computer security software that protects the confidentiality of data stored on computer media (e. g. , a hard disk, floppy disk, or USB device) by using disk encryption."
disk encryption software,"Disk encryption software is computer security software that protects the confidentiality of data stored on computer media (e. g. , a hard disk, floppy disk, or USB device) by using disk encryption. !! Some disk encryption software (e. g. , TrueCrypt or BestCrypt) provide features that generally cannot be accomplished with disk hardware encryption: the ability to mount ""container"" files as encrypted logical disks with their own file system; and encrypted logical ""inner"" volumes which are secretly hidden within the free space of the more obvious ""outer"" volumes. !! When the outer container is brought online thorough the disk encryption software, whether the inner or outer volume is mounted depends on the password provided. !! Some disk encryption software allows encrypted volumes to be resized. !! Well-known examples of disk encryption software include: BitLocker for Windows; FileVault for Apple OS/X; LUKS a standard free software mainly for Linux and TrueCrypt, a non-commercial freeware application, for Windows, OS/X and Linux."
disk hardware encryption,"Some disk encryption software (e. g. , TrueCrypt or BestCrypt) provide features that generally cannot be accomplished with disk hardware encryption: the ability to mount ""container"" files as encrypted logical disks with their own file system; and encrypted logical ""inner"" volumes which are secretly hidden within the free space of the more obvious ""outer"" volumes."
economic game theory,"Algorithmic mechanism design (AMD) lies at the intersection of economic game theory, optimization, and computer science."
algorithmic mechanism design,"Algorithmic mechanism design (AMD) lies at the intersection of economic game theory, optimization, and computer science. !! Dtting, Paul; Geiger, Andreas (May 9, 2007), Algorithmic Mechanism Design (PDF), Seminar Report, University of Karlsruhe, Fakultt fr Informatik, archived from the original (PDF) on June 13, 2015, retrieved June 11, 2015. !! Algorithmic mechanism design differs from classical economic mechanism design in several respects. !! Noam Nisan and Amir Ronen, from the Hebrew University of Jerusalem, first coined ""Algorithmic mechanism design"" in a research paper published in 1999."
null pointer,"It might compare equal to other, valid pointers; or it might compare equal to null pointers. !! In computing, a null pointer or null reference is a value saved for indicating that the pointer or reference does not refer to a valid object. !! Programs routinely use null pointers to represent conditions such as the end of a list of unknown length or the failure to perform some action; this use of null pointers can be compared to nullable types and to the Nothing value in an option type. !! A null pointer should not be confused with an uninitialized pointer: a null pointer is guaranteed to compare unequal to any pointer that points to a valid object. !! In C, two null pointers of any type are guaranteed to compare equal."
null reference,"In computing, a null pointer or null reference is a value saved for indicating that the pointer or reference does not refer to a valid object."
null pointers,"Programs routinely use null pointers to represent conditions such as the end of a list of unknown length or the failure to perform some action; this use of null pointers can be compared to nullable types and to the Nothing value in an option type. !! It might compare equal to other, valid pointers; or it might compare equal to null pointers."
uninitialized pointer,A null pointer should not be confused with an uninitialized pointer: a null pointer is guaranteed to compare unequal to any pointer that points to a valid object.
object recognition technology,Object recognition technology in the field of computer vision for finding and identifying objects in an image or video sequence.
object recognition  technical report icg,"Roth, Peter M. and Winter, Martin ""Survey of Appearance-Based Methods for Object Recognition"", Technical Report ICG-TR-01/08, Inst."
memory management,"Several methods have been devised that increase the effectiveness of memory management. !! Memory management within an address space is generally categorized as either manual memory management or automatic memory management. !! The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed. !! The memory management system must track outstanding allocations to ensure that they do not overlap and that no memory is ever ""lost"" (i. e. that there are no ""memory leaks""). !! Memory management is a form of resource management applied to computer memory."
address space,"Memory management within an address space is generally categorized as either manual memory management or automatic memory management. !! In computer programming, DLL injection is a technique used for running code within the address space of another process by forcing it to load a dynamic-link library."
statistical classification,"Early work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation."
linear discriminant function,"Early work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation."
multi layer perceptron,"Connectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e. g. , fuzzy-multi layer perceptron where linguistic and natural form of inputs are used."
statistical databases,"Statistical databases often incorporate support for advanced statistical analysis techniques, such as correlations, which go beyond SQL. !! Statistical databases typically contain parameter data and the measured data for these parameters. !! The conclusion is that statistical databases are almost always subject to compromise. !! Modern decision, and classical statistical databases are often closer to the relational model than the multidimensional model commonly used in OLAP systems today. !! Many statistical databases are sparse with many null or zero values."
Software design pattern,"In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design."
bidirectional forwarding detection,Bidirectional Forwarding Detection (BFD) is a network protocol that is used to detect faults between two routers or switches connected by a link.
network protocol,Bidirectional Forwarding Detection (BFD) is a network protocol that is used to detect faults between two routers or switches connected by a link.
cryptographic protocols,"However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators. !! Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code."
combinational circuits,"Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code."
program synthesis,Program repair combines techniques from formal verification and program synthesis.
circulant matrix,"corresponding to the first row rather than the first column of the matrix; and possibly with a different direction of shift (which is sometimes called an anti-circulant matrix). !! is called a block-circulant matrix. !! This enables the channel to be represented by a circulant matrix, simplifying channel equalization in the frequency domain. !! In cryptography, a circulant matrix is used in the MixColumns step of the Advanced Encryption Standard. !! In linear algebra, a circulant matrix is a square matrix in which all row vectors are composed of the same elements and each row vector is rotated one element to the right relative to the preceding row vector."
advanced encryption standard,"In cryptography, a circulant matrix is used in the MixColumns step of the Advanced Encryption Standard. !! ""Advanced Encryption Standard (AES)"" (PDF). !! The Advanced Encryption Standard (AES), also known by its original name Rijndael (Dutch pronunciation: [rindal]), is a specification for the encryption of electronic data established by the U. S. National Institute of Standards and Technology (NIST) in 2001. AES is a variant of the Rijndael block cipher developed by two Belgian cryptographers, Joan Daemen and Vincent Rijmen, who submitted a proposal to NIST during the AES selection process."
user virtualization,"Although user virtualization is most closely associated with desktop virtualization, in fact, this technology can be used to manage user profiles on physical desktops as well. !! User virtualization refers to the independent management of all aspects of the user on the desktop environment. !! User virtualization solutions provide consistent and seamless working environments across a range of application delivery mechanisms. !! User virtualization decouples a user's profile, settings and data from the operating system and stores this information into a centralized data share either in the data center or cloud. !! As the range of currently used operating systems expands, and the use of multiple devices by workers to perform their jobs escalates, user virtualization can support the creation of a ""follow-me"" identity that will allow access to a workspace without being tied into only a single device or a single location."
desktop environment,User virtualization refers to the independent management of all aspects of the user on the desktop environment.
data center,"User virtualization decouples a user's profile, settings and data from the operating system and stores this information into a centralized data share either in the data center or cloud."
operating system,"Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers. !! The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials. !! An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs. !! OS-level virtualization is an operating system (OS) paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers (LXC, Solaris containers, Docker, Podman), zones (Solaris containers), virtual private servers (OpenVZ), partitions, virtual environments (VEs), virtual kernels (DragonFly BSD), or jails (FreeBSD jail or chroot jail). !! User virtualization decouples a user's profile, settings and data from the operating system and stores this information into a centralized data share either in the data center or cloud. !! Google announced the project, then based on Ubuntu, in July 2009, conceiving it as an operating system in which both applications and user data reside in the cloud: hence Chrome OS primarily runs web applications. !! The dominant general-purpose personal computer operating system is Microsoft Windows with a market share of around 76. !! For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. !! Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources. !! Backward compatibility (sometimes known as backwards compatibility) is a property of an operating system, product, or technology that allows for interoperability with an older legacy system, or with input designed for such a system, especially in telecommunications and computing. !! While not legally required, new providers may choose to undergo annual security audits (such as WebTrust for certificate authorities in North America and ETSI in Europe) to be included as a trusted root by a web browser or operating system. !! There are several methods that browser hijackers use to gain entry to an operating system. !! Solaris Containers (including Solaris Zones) is an implementation of operating system-level virtualization technology for x86 and SPARC systems, first released publicly in February 2004 in build 51 beta of Solaris 10, and subsequently in the first full release of Solaris 10, 2005."
desktop virtualization,"Although user virtualization is most closely associated with desktop virtualization, in fact, this technology can be used to manage user profiles on physical desktops as well. !! Multiseat desktop virtualization is an entirely new methodology which combines the cost saving benefits and ease of maintenance of server based computing, the time savings of hardware agnostic cloning, and the capabilities of desktop virtualization, with the performance capabilities of real PC functionality."
statistical model comparison,Minimum message length (MML) is a Bayesian information-theoretic method for statistical model comparison and selection.
minimum message length,"Chapter 11: Minimum Message Length, MDL and Generalised Bayesian Networks with Asymmetric Languages. !! Minimum Message Length and Kolmogorov Complexity. !! Resolving the Neyman-Scott Problem by Minimum Message Length. !! Statistical and Inductive Inference by Minimum Message Length. !! Minimum message length (MML) is a Bayesian information-theoretic method for statistical model comparison and selection."
inductive inference,"Statistical and Inductive Inference by Minimum Message Length. !! Solomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: ""A Preliminary Report on a General Theory of Inductive Inference. """
generalised bayesian networks,"Chapter 11: Minimum Message Length, MDL and Generalised Bayesian Networks with Asymmetric Languages."
directed graphs,"More specifically, directed graphs without loops are addressed as simple directed graphs, while directed graphs with loops are addressed as loop-digraphs (see section Types of directed graphs)."
simple directed graphs,"More specifically, directed graphs without loops are addressed as simple directed graphs, while directed graphs with loops are addressed as loop-digraphs (see section Types of directed graphs)."
internet addressing structure,"In the context of the Internet addressing structure, an address pool is a set of Internet Protocol addresses available at any level in the IP address allocation hierarchy."
address pool,"In the context of the Internet addressing structure, an address pool is a set of Internet Protocol addresses available at any level in the IP address allocation hierarchy. !! The total IPv4 address pool contains 4294967296 (232) addresses, while the size of the IPv6 address pool is 2128 (340282366920938463463374607431768211456) addresses. !! At the top level, the IP address pool is managed by the Internet Assigned Numbers Authority (IANA). !! In the context of application design, an address pool may be the availability of a set of addresses (IP address, MAC address) available to an application that is shared among its users, or available for allocation to users, such as in host configurations with the Dynamic Host Configuration Protocol (DHCP)."
ip address pool,"At the top level, the IP address pool is managed by the Internet Assigned Numbers Authority (IANA)."
ipv6 address pool,"The total IPv4 address pool contains 4294967296 (232) addresses, while the size of the IPv6 address pool is 2128 (340282366920938463463374607431768211456) addresses."
dynamic host configuration protocol,"In the context of application design, an address pool may be the availability of a set of addresses (IP address, MAC address) available to an application that is shared among its users, or available for allocation to users, such as in host configurations with the Dynamic Host Configuration Protocol (DHCP)."
mac address,"The Address Resolution Protocol (ARP) is a communication protocol used for discovering the link layer address, such as a MAC address, associated with a given internet layer address, typically an IPv4 address. !! In the context of application design, an address pool may be the availability of a set of addresses (IP address, MAC address) available to an application that is shared among its users, or available for allocation to users, such as in host configurations with the Dynamic Host Configuration Protocol (DHCP)."
application design,"In the context of application design, an address pool may be the availability of a set of addresses (IP address, MAC address) available to an application that is shared among its users, or available for allocation to users, such as in host configurations with the Dynamic Host Configuration Protocol (DHCP)."
principal component analysis,"In quantitative finance, principal component analysis can be directly applied to the risk management of interest rate derivative portfolios. !! Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest. !! One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data. !! The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. !! Principal component analysis creates variables that are linear combinations of the original variables. !! 7 of Jolliffe's Principal Component Analysis), EckartYoung theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al."
empirical orthogonal functions,"7 of Jolliffe's Principal Component Analysis), EckartYoung theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al."
red-black trees,"This is the case for many binary search trees, such as the AVL trees and the redblack trees the latter was called symmetric binary B-tree and was renamed; it can, however, still be confused with the generic concept of self-balancing binary search tree because of the initials."
random number generators,"So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external ""random"" data such as the program's startup time."
adjacency list representation,An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighbouring vertices or edges.
balanced tree,A weight-balanced tree is a binary search tree that stores the sizes of subtrees in the nodes.
nearest neighbors algorithm,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. !! The primary motivation for employing lazy learning, as in the K-nearest neighbors algorithm, used by online recommendation systems (""people who viewed/purchased/listened to this movie/item/tune also . !! For high-dimensional datasets (i. e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality. !! Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks."
multiple correspondence analysis,"One special extension is multiple correspondence analysis, which may be seen as the counterpart of principal component analysis for categorical data."
bernoulli trial,"The mathematical formalisation of the Bernoulli trial is known as the Bernoulli process. !! More generally, given any probability space, for any event (set of outcomes), one can define a Bernoulli trial, corresponding to whether the event occurred or not (event or complementary event). !! Since a Bernoulli trial has only two possible outcomes, it can be framed as some ""yes or no"" question. !! Independent repeated trials of an experiment with exactly two possible outcomes are called Bernoulli trials. !! In the theory of probability and statistics, a Bernoulli trial (or binomial trial) is a random experiment with exactly two possible outcomes, ""success"" and ""failure"", in which the probability of success is the same every time the experiment is conducted."
binomial trial,"In the theory of probability and statistics, a Bernoulli trial (or binomial trial) is a random experiment with exactly two possible outcomes, ""success"" and ""failure"", in which the probability of success is the same every time the experiment is conducted."
bernoulli process,The mathematical formalisation of the Bernoulli trial is known as the Bernoulli process.
probability space,"More generally, given any probability space, for any event (set of outcomes), one can define a Bernoulli trial, corresponding to whether the event occurred or not (event or complementary event)."
complementary event,"More generally, given any probability space, for any event (set of outcomes), one can define a Bernoulli trial, corresponding to whether the event occurred or not (event or complementary event)."
data flow diagrams,Structured analysis consists of interpreting the system concept (or real world situations) into data and control terminology represented by data flow diagrams.
inline caching,"Inline caching is especially useful for dynamically typed languages where most if not all method binding happens at runtime and where virtual method tables often cannot be used. !! Inline caching is an optimization technique employed by some language runtimes, and first developed for Smalltalk. !! To achieve better performance, many language runtimes employ some form of non-inline caching where the results of a limited number of method lookups are stored in an associative data structure. !! The concept of inline caching is based on the empirical observation that the objects that occur at a particular call site are often of the same type. !! The goal of inline caching is to speed up runtime method binding by remembering the results of a previous method lookup directly at the call site."
runtime method binding,The goal of inline caching is to speed up runtime method binding by remembering the results of a previous method lookup directly at the call site.
dynamically typed languages,Inline caching is especially useful for dynamically typed languages where most if not all method binding happens at runtime and where virtual method tables often cannot be used.
associative data structure,"To achieve better performance, many language runtimes employ some form of non-inline caching where the results of a limited number of method lookups are stored in an associative data structure."
polynomial function,"If R is commutative, then one can associate with every polynomial P in R[x] a polynomial function f with domain and range equal to R. (More generally, one can take domain and range to be any same unital associative algebra over R. ) One obtains the value f(r) by substitution of the value r for the symbol x in P. One reason to distinguish between polynomials and polynomial functions is that, over some rings, different polynomials may give rise to the same polynomial function (see Fermat's little theorem for an example where R is the integers modulo p)."
lattice theory,"Mathematical morphology (MM) is a theory and technique for the analysis and processing of geometrical structures, based on set theory, lattice theory, topology, and random functions."
voronoi diagram,"In mathematics, a weighted Voronoi diagram in n dimensions is a generalization of a Voronoi diagram. !! The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet). !! In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. !! The Voronoi diagram of a set of points is dual to its Delaunay triangulation. !! The line segments of the Voronoi diagram are all the points in the plane that are equidistant to the two nearest sites."
Voronoi Diagram,"The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet). !! Voronoi diagrams have practical and theoretical applications in many fields, mainly in science and technology, but also in visual art. !! In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. !! The Voronoi diagram of a set of points is dual to its Delaunay triangulation. !! The line segments of the Voronoi diagram are all the points in the plane that are equidistant to the two nearest sites."
delaunay triangulation,The Voronoi diagram of a set of points is dual to its Delaunay triangulation.
voronoi decomposition,"The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet)."
voronoi tessellation,"The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet)."
voronoi partition,"The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet)."
dirichlet tessellation,"The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet)."
voronoi diagrams,"Voronoi diagrams have practical and theoretical applications in many fields, mainly in science and technology, but also in visual art."
line segments,The line segments of the Voronoi diagram are all the points in the plane that are equidistant to the two nearest sites.
incident edges,"There are more refined algorithms to cope with some of these issues, for example iterated snap rounding guarantees a ""large"" separation between points and non-incident edges."
network scheduling algorithms,"There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms."
automatic target recognition,Automatic target recognition (ATR) is the ability for an algorithm or device to recognize targets or other objects based on data obtained from sensors.
generative adversarial network,Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy. !! A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in June 2014. !! A Style-Based Generator Architecture for Generative Adversarial Networks.
adversarial network,"Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy. !! A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in June 2014. !! An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo. !! Beginning in 2017, GAN technology began to make its presence felt in the fine arts arena with the appearance of a newly developed implementation which was said to have crossed the threshold of being able to generate unique and appealing abstract paintings, and thus dubbed a ""CAN"", for ""creative adversarial network"". !! A Style-Based Generator Architecture for Generative Adversarial Networks."
creative adversarial network,"Beginning in 2017, GAN technology began to make its presence felt in the fine arts arena with the appearance of a newly developed implementation which was said to have crossed the threshold of being able to generate unique and appealing abstract paintings, and thus dubbed a ""CAN"", for ""creative adversarial network""."
ontology learning,"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. !! Ontology Learning and Population: Bridging the Gap between Text and Knowledge, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2008. !! The process is usually split into the following eight tasks, which are not all necessarily applied in every ontology learning system. !! Ontology learning (OL) is used to (semi-)automatically extract whole ontologies from natural language text. !! Ontology Learning from Text: Methods, Evaluation and Applications, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2005."
ontology language,"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval."
ontology acquisition,"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval."
ontology extraction,"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval."
ontology generation,"Ontology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval."
zero polynomial,"The sum, product, or quotient (excepting division by the zero polynomial) of two rational functions is itself a rational function."
rational functions,"Using the definition of rational functions as equivalence classes gets around this, since x/x is equivalent to 1/1. !! The sum, product, or quotient (excepting division by the zero polynomial) of two rational functions is itself a rational function. !! First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc. !! Rational functions are used in numerical analysis for interpolation and approximation of functions, for example the Pad approximations introduced by Henri Pad. !! Rational functions are representative examples of meromorphic functions. !! Iteration of rational functions (maps) on the on the Riemann sphere creates discrete dynamical systems."
meromorphic functions,Rational functions are representative examples of meromorphic functions.
single pass,"Reservoir sampling is a family of randomized algorithms for choosing a simple random sample, without replacement, of k items from a population of unknown size n in a single pass over the items."
reservoir sampling,"Reservoir sampling is a family of randomized algorithms for choosing a simple random sample, without replacement, of k items from a population of unknown size n in a single pass over the items. !! The following algorithm was given by Efraimidis and Spirakis that uses interpretation 1:This algorithm is identical to the algorithm given in Reservoir Sampling with Random Sort except for the generation of the items' keys."
syntactically ambiguous input,Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.
parsing algorithms,"Parsing algorithms for natural language cannot rely on the grammar having 'nice' properties as with manually designed grammars for programming languages. !! Some graphical parsing algorithms have been designed for visual programming languages. !! Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input. !! Adaptive parsing algorithms have been used to construct ""self-extending"" natural language user interfaces."
graphical parsing algorithms,Some graphical parsing algorithms have been designed for visual programming languages.
adaptive parsing algorithms,"Adaptive parsing algorithms have been used to construct ""self-extending"" natural language user interfaces."
mathematical theory,"In the mathematical theory of probability, an absorbing Markov chain is a Markov chain in which every state can reach an absorbing state. !! In queueing theory, a discipline within the mathematical theory of probability, a polling system or polling model is a system where a single server visits a set of queues in some order. !! Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary."
modern cryptography,"Cryptographic hash functions are a basic tool of modern cryptography. !! Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary."
cryptographic algorithms,"The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution. !! Lightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment. !! Cryptographic Hash Functions are cryptographic algorithms that are ways to generate and utilize specific keys to encrypt data for either symmetric or asymmetric encryption, and such functions may be viewed as keys themselves. !! Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary."
asymmetric encryption,"Cryptographic Hash Functions are cryptographic algorithms that are ways to generate and utilize specific keys to encrypt data for either symmetric or asymmetric encryption, and such functions may be viewed as keys themselves."
cryptographic hash functions,"Most cryptographic hash functions are designed to take a string of any length as input and produce a fixed-length hash value. !! Checksum algorithms, such as CRC32 and other cyclic redundancy checks, are designed to meet much weaker requirements and are generally unsuitable as cryptographic hash functions. !! Cryptographic hash functions are a basic tool of modern cryptography. !! a small change to a message should change the hash value so extensively that a new hash value appears uncorrelated with the old hash value (avalanche effect)Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. !! Cryptographic Hash Functions are cryptographic algorithms that are ways to generate and utilize specific keys to encrypt data for either symmetric or asymmetric encryption, and such functions may be viewed as keys themselves. !! Currently, popular cryptographic hash functions are vulnerable to length-extension attacks: given hash(m) and len(m) but not m, by choosing a suitable m an attacker can calculate hash(m m), where denotes concatenation."
utilize specific keys,"Cryptographic Hash Functions are cryptographic algorithms that are ways to generate and utilize specific keys to encrypt data for either symmetric or asymmetric encryption, and such functions may be viewed as keys themselves."
encrypt data,"Cryptographic Hash Functions are cryptographic algorithms that are ways to generate and utilize specific keys to encrypt data for either symmetric or asymmetric encryption, and such functions may be viewed as keys themselves."
lightweight cryptography,Lightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment.
strictly constrained environment,Lightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment.
program execution,"Some programming languages allow a program to operate differently or even have a different control flow than the source code, as long as it exhibits the same user-visible side effects, if undefined behavior never happens during program execution. !! For interpreted languages, however, a syntax error may be detected during program execution, and an interpreter's error messages might not differentiate syntax errors from errors of other kinds."
main memory,The F state in the MESIF protocol is simply a way to choose one of the sharers of a clean cache line to respond to a read request for data using a direct cache-to-cache transfer instead of waiting for the data to come from the main memory. !! A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. !! Memory footprint refers to the amount of main memory that a program uses or references while running.
tlb cache,"However, the TLB cache is part of the memory management unit (MMU) and not directly related to the CPU caches."
cpu caches,"However, the TLB cache is part of the memory management unit (MMU) and not directly related to the CPU caches. !! Memory part 2: CPU caches an article on lwn."
victim cache,A victim cache is a cache used to hold blocks evicted from a CPU cache upon replacement.
computer engineering,"In computer engineering, a tag RAM is used to specify which of the possible memory locations is currently stored in a CPU cache. !! In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. !! Computer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines."
different clusters,"More than a dozen of internal evaluation measures exist, usually based on the intuition that items in the same cluster should be more similar than items in different clusters."
discrete time,"In both differential equations in continuous time and difference equations in discrete time, initial conditions affect the value of the dynamic variables (state variables) at any future time. !! For a system of order k (the number of time lags in discrete time, or the order of the largest derivative in continuous time) and dimension n (that is, with n different evolving variables, which together can be denoted by an n-dimensional coordinate vector), generally nk initial conditions are needed in order to trace the system's variables forward through time. !! When used in this manner, the counter machine is used to model the discrete time-steps of a computational system in relation to memory accesses."
data mapper pattern,"In software engineering, the data mapper pattern is an architectural pattern."
data mapper,"In software engineering, the data mapper pattern is an architectural pattern. !! The goal of the pattern is to keep the in-memory representation and the persistent data store independent of each other and the data mapper itself. !! A Data Mapper is a Data Access Layer that performs bidirectional transfer of data between a persistent data store (often a relational database) and an in-memory data representation (the domain layer)."
architectural pattern,"In software engineering, the data mapper pattern is an architectural pattern. !! In software engineering, the active record pattern is considered an architectural pattern by some people and as an anti-pattern by some others recently. !! Interface-based programming, also known as interface-based architecture, is an architectural pattern for implementing modular programming at the component level in an object-oriented programming language which does not have a module system."
memory data representation,A Data Mapper is a Data Access Layer that performs bidirectional transfer of data between a persistent data store (often a relational database) and an in-memory data representation (the domain layer).
persistent data store,A Data Mapper is a Data Access Layer that performs bidirectional transfer of data between a persistent data store (often a relational database) and an in-memory data representation (the domain layer).
domain layer,A Data Mapper is a Data Access Layer that performs bidirectional transfer of data between a persistent data store (often a relational database) and an in-memory data representation (the domain layer).
data access layer,A Data Mapper is a Data Access Layer that performs bidirectional transfer of data between a persistent data store (often a relational database) and an in-memory data representation (the domain layer).
memory representation,The goal of the pattern is to keep the in-memory representation and the persistent data store independent of each other and the data mapper itself.
exploratory search,"Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search at SIGIR06 and Exploratory Search and HCI at CHI07 (in order to meet with the experts in humancomputer interaction). !! Consequently, exploratory search covers a broader class of activities than typical information retrieval, such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; exploratory data analysis is another example of an information exploration activity. !! Exploratory search is distinguished from known-item search, for which the searcher has a particular target in mind. !! In 2005, the Exploratory Search Interfaces workshop focused on beginning to define some of the key challenges in the field. !! Exploratory search is a topic that has grown from the fields of information retrieval and information seeking but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a Google-like keyword search)."
item search,"Exploratory search is distinguished from known-item search, for which the searcher has a particular target in mind."
information exploration activity,"Consequently, exploratory search covers a broader class of activities than typical information retrieval, such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; exploratory data analysis is another example of an information exploration activity."
exploratory data analysis,"In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). !! Consequently, exploratory search covers a broader class of activities than typical information retrieval, such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; exploratory data analysis is another example of an information exploration activity."
information seeking,Exploratory search is a topic that has grown from the fields of information retrieval and information seeking but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a Google-like keyword search).
evaluating exploratory search,Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search at SIGIR06 and Exploratory Search and HCI at CHI07 (in order to meet with the experts in humancomputer interaction).
forward declaration,"In other languages forward declarations are not necessary, which generally requires instead a multi-pass compiler and for some compilation to be deferred to link time. !! In C and C++, the line above represents a forward declaration of a function and is the function's prototype. !! Variables may have only forward declaration and lack definition. !! In computer programming, a forward declaration is a declaration of an identifier (denoting an entity such as a type, a variable, a constant, or a function) for which the programmer has not yet given a complete definition. !! Forward declaration is used in languages that require declaration before use; it is necessary for mutual recursion in such languages, as it is impossible to define such functions (or data structures) without a forward reference in one definition: one of the functions (respectively, data structures) must be defined first."
mutual recursion,"Forward declaration is used in languages that require declaration before use; it is necessary for mutual recursion in such languages, as it is impossible to define such functions (or data structures) without a forward reference in one definition: one of the functions (respectively, data structures) must be defined first."
forward reference,"Forward declaration is used in languages that require declaration before use; it is necessary for mutual recursion in such languages, as it is impossible to define such functions (or data structures) without a forward reference in one definition: one of the functions (respectively, data structures) must be defined first."
link time,"In other languages forward declarations are not necessary, which generally requires instead a multi-pass compiler and for some compilation to be deferred to link time."
kaczmarz method,"[2] Comments on the randomized Kaczmarz method !! The general rate of the Gower-Richtarik algorithm precisely recovers the rate of the randomized Kaczmarz method in the special case when it reduced to it. !! The Kaczmarz method is applicable to any linear system of equations, but its computational advantage relative to other methods depends on the system being sparse. !! Strohmer, Thomas; Vershynin, Roman (2009b), ""Comments on the randomized Kaczmarz method"", Journal of Fourier Analysis and Applications, 15 (4): 437440,"
randomized kaczmarz method,[2] Comments on the randomized Kaczmarz method !! The general rate of the Gower-Richtarik algorithm precisely recovers the rate of the randomized Kaczmarz method in the special case when it reduced to it.
fourier analysis,"Strohmer, Thomas; Vershynin, Roman (2009b), ""Comments on the randomized Kaczmarz method"", Journal of Fourier Analysis and Applications, 15 (4): 437440,"
line detection,"In image processing, line detection is an algorithm that takes a collection of n edge points and finds all the lines on which these edge points lie."
lazy initialization,"In a software design pattern view, lazy initialization is often used together with a factory method pattern. !! Here is an example in Smalltalk, of a typical accessor method to return the value of a variable using lazy initialization. !! In computer programming, lazy initialization is the tactic of delaying the creation of an object, the calculation of a value, or some other expensive process until the first time it is needed. !! Note that lazy initialization can also be used in non-object-oriented languages. !! Here is an example of lazy initialization in PHP 7."
typical accessor method,"Here is an example in Smalltalk, of a typical accessor method to return the value of a variable using lazy initialization."
search problems,"Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection."
polling model,"In queueing theory, a discipline within the mathematical theory of probability, a polling system or polling model is a system where a single server visits a set of queues in some order."
highest frequency,"Trace scheduling uses a basic block scheduling method to schedule the instructions in each entire trace, beginning with the trace with the highest frequency."
decision problem,"In computability theory and computational complexity theory, an undecidable problem is a decision problem for which it is proved to be impossible to construct an algorithm that always leads to a correct yes-or-no answer. !! In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem. !! If the longest path problem could be solved in polynomial time, it could be used to solve this decision problem, by finding a longest path and then comparing its length to the number k. Therefore, the longest path problem is NP-hard. !! The subset sum problem (SSP) is a decision problem in computer science."
subset sum problem,SSP is a special case of the knapsack problem and of the multiple subset sum problem. !! Multiple subset sum problem - a generalization off SSP in which one should choose several subsets. !! The subset sum problem (SSP) is a decision problem in computer science. !! algorithm for the subset sum problem where T is the sum we need to find. !! Solving low-density subset sum problems.
multiple subset sum problem,SSP is a special case of the knapsack problem and of the multiple subset sum problem. !! Multiple subset sum problem - a generalization off SSP in which one should choose several subsets.
collision detection,"Collision detection utilizes time coherence to allow even finer time steps without much increasing CPU demand, such as in air traffic control. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics. !! Collision detection algorithms can be divided into operating on 2D and 3D objects. !! In addition to the a posteriori and a priori distinction, almost all modern collision detection algorithms are broken into a hierarchy of algorithms. !! Collision detection is the computational problem of detecting the intersection of two or more objects."
computational problem,Collision detection is the computational problem of detecting the intersection of two or more objects.
computer simulations,"Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics."
collision detection algorithms,Collision detection algorithms can be divided into operating on 2D and 3D objects.
modern collision detection algorithms,"In addition to the a posteriori and a priori distinction, almost all modern collision detection algorithms are broken into a hierarchy of algorithms."
fibonacci heap,"Michael L. Fredman and Robert E. Tarjan developed Fibonacci heaps in 1984 and published them in a scientific journal in 1987. !! For the Fibonacci heap, the find-minimum operation takes constant (O(1)) amortized time. !! In computer science, a Fibonacci heap is a data structure for priority queue operations, consisting of a collection of heap-ordered trees. !! A Fibonacci heap is thus better than a binary or binomial heap when b is smaller than a by a non-constant factor. !! Fibonacci heaps are named after the Fibonacci numbers, which are used in their running time analysis."
priority queue operations,"In computer science, a Fibonacci heap is a data structure for priority queue operations, consisting of a collection of heap-ordered trees."
fibonacci heaps,"Fibonacci heaps are named after the Fibonacci numbers, which are used in their running time analysis."
running time analysis,"Fibonacci heaps are named after the Fibonacci numbers, which are used in their running time analysis."
binomial heap,"In computer science, a binomial heap is a data structure that acts as a priority queue but also allows pairs of heaps to be merged. !! This feature is central to the merge operation of a binomial heap, which is its major advantage over other conventional heaps. !! , and thus a binomial heap with 13 nodes will consist of three binomial trees of orders 3, 2, and 0 (see figure below). !! Example of a binomial heap containing 13 nodes with distinct keys. !! Binomial heaps were invented in 1978 by Jean Vuillemin. !! A Fibonacci heap is thus better than a binary or binomial heap when b is smaller than a by a non-constant factor."
portable operating system interface,The development of the open-system environment reference model started early 1990s by the NIST as refinement of the POSIX (Portable Operating System Interface) standard.
textual case-based reasoning,"Textual Case-Based Reasoning Wiki !! Textual case-based reasoning (TCBR) is a subtopic of case-based reasoning, in short CBR, a popular area in artificial intelligence."
computer screen,"If a scientist sets up a computerized thermometer which records the temperature of a chemical mixture in a test tube every minute, the list of temperature readings for every minute, as printed out on a spreadsheet or viewed on a computer screen are ""raw data""."
data entry errors,"Raw data have not been subjected to processing, ""cleaning"" by researchers to remove outliers, obvious instrument reading errors or data entry errors, or any analysis (e. g. , determining central tendency aspects such as the average or median result)."
software program,"As well, raw data have not been subject to any other manipulation by a software program or a human researcher, analyst or technician."
ubiquitous computing,"The descriptions provided by Townsend in his foreword and by Foth in his preface to the Handbook of Research on Urban Informatics emphasize two key aspects: (1) the new possibilities (including real-time data) for both citizens and city administrations afforded by ubiquitous computing, and (2) the convergence of physical and digital aspects of the city. !! The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials. !! The Mobile Life Centre at Stockholm University in Kista, Sweden, conducts research in mobile services and ubiquitous computing. !! In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. !! Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! Responsive computer-aided design is enabled by ubiquitous computing and the Internet of Things, concepts which describe the capacity for everyday objects to contain computing and sensing technologies. !! Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. !! Rather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described. !! Since then, the emergence and growing popularity of ubiquitous computing, open data and big data analytics, as well as smart cities, contributed to a surge in interest in urban informatics, not just from academics but also from industry and city governments seeking to explore and apply the possibilities and opportunities of urban informatics."
hardware engineering,"Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. !! In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically rigorous techniques for the specification, development and verification of software and hardware systems."
desktop computing,"In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format."
computer networks,"The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials. !! Round-robin scheduling can be applied to other scheduling problems, such as data packet scheduling in computer networks. !! DuPont and Fidler provide a historical perspective of ARPANET encryption devices in the broader evolution of computer networks and cybersecurity."
mobile code,"The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials."
advanced middleware,"The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials."
mobile protocols,"The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials."
ubiquitous systems,"Rather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described."
mobile networking,"Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence."
mobile computing,"Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management."
location computing,"Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence."
ubiquitous computing touches,"Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence."
lossless compression algorithms,"However, many ordinary lossless compression algorithms produce headers, wrappers, tables, or other predictable output that might instead make cryptanalysis easier. !! Some of the most common lossless compression algorithms are listed below. !! Lossless compression algorithms and their implementations are routinely tested in head-to-head benchmarks. !! The ""trick"" that allows lossless compression algorithms, used on the type of data they were designed for, to consistently compress such files to a shorter form is that the files the algorithms are designed to act on all have some form of easily modeled redundancy that the algorithm is designed to remove, and thus belong to the subset of files that that algorithm can make shorter, whereas other files would not get compressed or even get bigger."
adaptive histogram equalization,"Contrast Limited AHE (CLAHE) is a variant of adaptive histogram equalization in which the contrast amplification is limited, so as to reduce this problem of noise amplification. !! Adaptive histogram equalization (AHE) is a computer image processing technique used to improve contrast in images. !! Adaptive histogram equalization (AHE) improves on this by transforming each pixel with a transformation function derived from a neighbourhood region. !! A variant of adaptive histogram equalization called contrast limited adaptive histogram equalization (CLAHE) prevents this by limiting the amplification. !! Adaptive histogram equalization in its straightforward form presented above, both with and without contrast limiting, requires the computation of a different neighbourhood histogram and transformation function for each pixel in the image."
neighbourhood region,Adaptive histogram equalization (AHE) improves on this by transforming each pixel with a transformation function derived from a neighbourhood region.
contrast limited ahe,"Contrast Limited AHE (CLAHE) is a variant of adaptive histogram equalization in which the contrast amplification is limited, so as to reduce this problem of noise amplification."
key exchange problem,The best known example of quantum cryptography is quantum key distribution which offers an information-theoretically secure solution to the key exchange problem.
theoretically secure solution,The best known example of quantum cryptography is quantum key distribution which offers an information-theoretically secure solution to the key exchange problem.
quantum key distribution,The best known example of quantum cryptography is quantum key distribution which offers an information-theoretically secure solution to the key exchange problem.
quantum cryptography attributes,Quantum cryptography attributes its beginning by the work of Stephen Wiesner and Gilles Brassard.
numerical solution,"In the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation. !! In mathematics, the generalized minimal residual method (GMRES) is an iterative method for the numerical solution of an indefinite nonsymmetric system of linear equations. !! In numerical linear algebra, the biconjugate gradient stabilized method, often abbreviated as BiCGSTAB, is an iterative method developed by H. A. van der Vorst for the numerical solution of nonsymmetric linear systems. !! The approximation of derivatives by finite differences plays a central role in finite difference methods for the numerical solution of differential equations, especially boundary value problems. !! In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is positive-definite."
iterative algorithm,"The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition."
nonlinear optimization problems,Various nonlinear conjugate gradient methods seek minima of nonlinear optimization problems.
approximate solution,"In the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation."
elliptic partial differential equations,"In the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation."
stiffness matrix,"In the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation. !! , the coefficients ui are determined by the linear system Au = F. The stiffness matrix is symmetric, i. e. Aij = Aji, so all its eigenvalues are real. !! For example, the stiffness matrix when piecewise quadratic finite elements are used will have more degrees of freedom than piecewise linear elements. !! Note that the stiffness matrix will be different depending on the computational grid used for the domain and what type of finite element is used. !! Determining the stiffness matrix for other PDE follows essentially the same procedure, but it can be complicated by the choice of boundary conditions."
differential equation,"In the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation. !! In a spiking neural network, a neuron's current state is defined as its membrane potential (possibly modeled as a differential equation). !! In mathematics, a stiff equation is a differential equation for which certain numerical methods for solving the equation are numerically unstable, unless the step size is taken to be extremely small."
boundary conditions,"Topology optimization is a mathematical method that optimizes material layout within a given design space, for a given set of loads, boundary conditions and constraints with the goal of maximizing the performance of the system. !! Determining the stiffness matrix for other PDE follows essentially the same procedure, but it can be complicated by the choice of boundary conditions."
scalable networks,"Augmented tree-based routing (ATR) protocol, first proposed in 2007, is a multi-path DHT-based routing protocol for scalable networks."
augmented tree-based routing,"Augmented Tree-based Routing Protocol for Scalable Ad Hoc Networks. !! Augmented tree-based routing (ATR) protocol, first proposed in 2007, is a multi-path DHT-based routing protocol for scalable networks."
scalable ad hoc networks,Augmented Tree-based Routing Protocol for Scalable Ad Hoc Networks.
heuristic algorithms,"Metaheuristic: Methods for controlling and tuning basic heuristic algorithms, usually with usage of memory and learning."
nonnegative extended real number,"In mathematics, the topological entropy of a topological dynamical system is a nonnegative extended real number that is a measure of the complexity of the system. !! A topological dynamical system consists of a Hausdorff topological space X (usually assumed to be compact) and a continuous self-map f. Its topological entropy is a nonnegative extended real number that can be defined in various ways, which are known to be equivalent."
topological dynamical system,"In mathematics, the topological entropy of a topological dynamical system is a nonnegative extended real number that is a measure of the complexity of the system."
exponential growth rate,"The second definition clarified the meaning of the topological entropy: for a system given by an iterated function, the topological entropy represents the exponential growth rate of the number of distinguishable orbits of the iterates."
somos sequence,"In mathematics, a Somos sequence is a sequence of numbers defined by a certain recurrence relation, described below. !! From the form of their defining recurrence (which involves division), one would expect the terms of the sequence to be fractions, but nevertheless many Somos sequences have the property that all of their members are integers. !! The form of the recurrences describing the Somos sequences involves divisions, making it appear likely that the sequences defined by these recurrence will contain fractional values. !! While in the usual definition of the Somos sequences, the values of ai for i < k are all set equal to 1, it is also possible to define other sequences by using the same recurrences with different initial values. !! Nevertheless, for k 7 the Somos sequences contain only integer values."
somos sequences,"While in the usual definition of the Somos sequences, the values of ai for i < k are all set equal to 1, it is also possible to define other sequences by using the same recurrences with different initial values."
travelling salesperson problem,"The travelling salesman problem (also called the travelling salesperson problem or TSP) asks the following question: ""Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city"""
travelling salesman problem,"The travelling salesman problem (also called the travelling salesperson problem or TSP) asks the following question: ""Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city"" !! The earliest publication using the phrase ""travelling salesman problem"" was the 1949 RAND Corporation report by Julia Robinson, ""On the Hamiltonian game (a traveling salesman problem). !! The travelling salesman problem was mathematically formulated in the 19th century by the Irish mathematician W. R. Hamilton and by the British mathematician Thomas Kirkman. !! The BeardwoodHaltonHammersley theorem provides a practical solution to the travelling salesman problem. !! The origins of the travelling salesman problem are unclear."
hamiltonian game,"The earliest publication using the phrase ""travelling salesman problem"" was the 1949 RAND Corporation report by Julia Robinson, ""On the Hamiltonian game (a traveling salesman problem)."
traveling salesman problem,"The earliest publication using the phrase ""travelling salesman problem"" was the 1949 RAND Corporation report by Julia Robinson, ""On the Hamiltonian game (a traveling salesman problem)."
time travel debugging,Time travel debugging or time traveling debugging is the process of stepping back in time through source code to understand what is happening during execution of a computer program.
alternating decision tree,An introduction to Boosting and ADTrees (Has many graphical examples of alternating decision trees in practice). !! An alternating decision tree (ADTree) is a machine learning method for classification. !! Alternating decision trees introduce structure to the set of hypotheses by requiring that they build off a hypothesis that was produced in an earlier iteration. !! An alternating decision tree consists of decision nodes and prediction nodes.
decision nodes,An alternating decision tree consists of decision nodes and prediction nodes.
prediction nodes,An alternating decision tree consists of decision nodes and prediction nodes.
alternating decision trees,An introduction to Boosting and ADTrees (Has many graphical examples of alternating decision trees in practice).
ordered graph,"An ordered graph is a graph with a total order over its nodes. !! The induced graph of an ordered graph is obtained by adding some edges to an ordering graph, using the method outlined below. !! The induced width of an ordered graph is the width of its induced graph. !! The width of a node is the number of its parents, and the width of an ordered graph is the maximal width of its nodes. !! In an ordered graph, the parents of a node are the nodes that are adjacent to it and precede it in the ordering."
maximal width,"The width of a node is the number of its parents, and the width of an ordered graph is the maximal width of its nodes."
induced graph,"The induced width of an ordered graph is the width of its induced graph. !! The induced graph of an ordered graph is obtained by adding some edges to an ordering graph, using the method outlined below."
document clustering,"Automatic document classification tasks can be divided into three sorts: supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification, where parts of the documents are labeled by the external mechanism."
optimization algorithms,Many optimization algorithms need to start from a feasible point.
application framework,The Spring Framework is an application framework and inversion of control container for the Java platform.
spring framework,The Spring Framework is an application framework and inversion of control container for the Java platform. !! Spring Framework 4. !! The Spring Framework is open source.
java platform,The Spring Framework is an application framework and inversion of control container for the Java platform.
open source,"The Spring Framework is open source. !! Web mining can complement the retrieval of structured data transmitted with open protocols like OAI-PMH: an example is the aggregation of works from academic publications, which are mined to identify open access versions through a mix of open source and open data methods by academic databases like Unpaywall."
spring framework 4,Spring Framework 4.
software agent,"Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. !! Software agents interacting with people (e. g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo). !! Software agents may be autonomous or work together with other agents or people. !! However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents. !! In computer science, a software agent is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. !! Software agents may offer various benefits to their end users by automating complex or repetitive tasks."
natural language understanding,"Software agents interacting with people (e. g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo). !! Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. !! The principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to semantic parsing, natural language understanding, example-based translation, morpheme analysis, and place name derivations."
decision tree pruning,Introduction to Decision tree pruning !! Pessimistic decision tree pruning based on tree size.
tree size,Pessimistic decision tree pruning based on tree size.
leaked memory,"In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected."
ray tracing,"Since 2018, however, hardware acceleration for real-time ray tracing has become standard on new commercial graphics cards, and graphics APIs have followed suit, allowing developers to use hybrid ray tracing and rasterization-based rendering in games and other real-time applications with a lesser hit to frame render times. !! Thus, ray tracing was first deployed in applications where taking a relatively long time to render could be tolerated, such as in still computer-generated images, and film and television visual effects (VFX), but was less suited to real-time applications such as video games, where speed is critical in rendering each frame. !! Ray tracing is capable of simulating a variety of optical effects, such as reflection, refraction, soft shadows, scattering, depth of field, motion blur, caustics, ambient occlusion and dispersion phenomena (such as chromatic aberration). !! recursive ray tracing, distribution ray tracing, photon mapping to path tracing are generally slower and higher fidelity than scanline rendering methods. !! In 3D computer graphics, ray tracing is a technique for modeling light transport for use in a wide variety of rendering algorithms for generating digital images."
path tracing,"recursive ray tracing, distribution ray tracing, photon mapping to path tracing are generally slower and higher fidelity than scanline rendering methods."
recursive ray tracing,"recursive ray tracing, distribution ray tracing, photon mapping to path tracing are generally slower and higher fidelity than scanline rendering methods."
scanline rendering methods,"recursive ray tracing, distribution ray tracing, photon mapping to path tracing are generally slower and higher fidelity than scanline rendering methods."
render could,"Thus, ray tracing was first deployed in applications where taking a relatively long time to render could be tolerated, such as in still computer-generated images, and film and television visual effects (VFX), but was less suited to real-time applications such as video games, where speed is critical in rendering each frame."
graphics apis,"Since 2018, however, hardware acceleration for real-time ray tracing has become standard on new commercial graphics cards, and graphics APIs have followed suit, allowing developers to use hybrid ray tracing and rasterization-based rendering in games and other real-time applications with a lesser hit to frame render times."
hardware acceleration,"Since 2018, however, hardware acceleration for real-time ray tracing has become standard on new commercial graphics cards, and graphics APIs have followed suit, allowing developers to use hybrid ray tracing and rasterization-based rendering in games and other real-time applications with a lesser hit to frame render times."
kinetic data structures,"Kinetic data structures are used on systems where there is a set of values that are changing as a function of time, in a known fashion. !! For example, kinetic data structures are often used with a set of points. !! The development of kinetic data structures was motivated by computational geometry problems involving physical objects in continuous motion, such as collision or visibility detection in robotics, animation or computer graphics."
reciprocal interactions,"Social affordances or more accurately sociotechnical affordances refer as reciprocal interactions between a technology application, its users, and its social context."
temporally ordered routing algorithm,The Temporally Ordered Routing Algorithm (TORA) is an algorithm for routing data across Wireless Mesh Networks or Mobile ad hoc networks.
gap problem,"In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem."
gap reduction,"Gap reductions can be used to demonstrate inapproximability results, as if a problem may be approximated to a better factor than the size of gap, then the approximation algorithm can be used to solve the corresponding gap problem. !! In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem."
gap reductions,"Gap reductions can be used to demonstrate inapproximability results, as if a problem may be approximated to a better factor than the size of gap, then the approximation algorithm can be used to solve the corresponding gap problem."
matrix multiplication algorithms,"Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient."
internet protocols,"Postel stated, ""We are screwing up in our design of Internet protocols by violating the principle of layering. "" !! The Design Philosophy of the DARPA Internet Protocols."
darpa internet protocols,The Design Philosophy of the DARPA Internet Protocols.
posteriori probability estimate,"The Viterbi algorithm is a dynamic programming algorithm for obtaining the maximum a posteriori probability estimate of the most likely sequence of hidden statescalled the Viterbi paththat results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM)."
dynamic programming algorithm,"In addition to its dynamic programming algorithm, Knuth proposed two heuristics (or rules) to produce nearly (approximation of) optimal binary search trees. !! Query plans for nested SQL queries can also be chosen using the same dynamic programming algorithm as used for join ordering, but this can lead to an enormous escalation in query optimization time. !! The Viterbi algorithm is a dynamic programming algorithm for obtaining the maximum a posteriori probability estimate of the most likely sequence of hidden statescalled the Viterbi paththat results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models (HMM)."
convolutional codes,"The Viterbi algorithm is named after Andrew Viterbi, who proposed it in 1967 as a decoding algorithm for convolutional codes over noisy digital communication links."
decoding algorithm,"The Viterbi algorithm is named after Andrew Viterbi, who proposed it in 1967 as a decoding algorithm for convolutional codes over noisy digital communication links. !! For sequential decoding to a good choice of decoding algorithm, the number of states explored wants to remain small (otherwise an algorithm which deliberately explores all states, e. g. the Viterbi algorithm, may be more suitable)."
viterbi path,Viterbi path and Viterbi algorithm have become standard terms for the application of dynamic programming algorithms to maximization problems involving probabilities.
dynamic programming algorithms,Viterbi path and Viterbi algorithm have become standard terms for the application of dynamic programming algorithms to maximization problems involving probabilities.
latent variables,"A generalization of the Viterbi algorithm, termed the max-sum algorithm (or max-product algorithm) can be used to find the most likely assignment of all or some subset of latent variables in a large number of graphical models, e. g. Bayesian networks, Markov random fields and conditional random fields. !! Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. !! A Thurstonian model is a stochastic transitivity model with latent variables for describing the mapping of some continuous scale onto discrete, possibly ordered categories of response."
graphical models,"Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution. !! Applications of graphical models include causal inference, information extraction, speech recognition, computer vision, decoding of low-density parity-check codes, modeling of gene regulatory networks, gene finding and diagnosis of diseases, and graphical models for protein structure. !! Getting Started in Probabilistic Graphical Models. !! A generalization of the Viterbi algorithm, termed the max-sum algorithm (or max-product algorithm) can be used to find the most likely assignment of all or some subset of latent variables in a large number of graphical models, e. g. Bayesian networks, Markov random fields and conditional random fields. !! Relational dependency networks (RDNs) are graphical models which extend dependency networks to account for relational data. !! Graphical Models"" (PDF). !! Graphical Models."
sum algorithm,"A generalization of the Viterbi algorithm, termed the max-sum algorithm (or max-product algorithm) can be used to find the most likely assignment of all or some subset of latent variables in a large number of graphical models, e. g. Bayesian networks, Markov random fields and conditional random fields."
conditional random fields,"A generalization of the Viterbi algorithm, termed the max-sum algorithm (or max-product algorithm) can be used to find the most likely assignment of all or some subset of latent variables in a large number of graphical models, e. g. Bayesian networks, Markov random fields and conditional random fields."
cache algorithms,"In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions, or algorithms, that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer."
cache replacement policies,"In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions, or algorithms, that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer."
object data management group,The Object Data Management Group (ODMG) was conceived in the summer of 1991 at a breakfast with object database vendors that was organized by Rick Cattell of Sun Microsystems.
real symmetric positive definite matrix,"In mathematics, particularly matrix theory, a Stieltjes matrix, named after Thomas Joannes Stieltjes, is a real symmetric positive definite matrix with nonpositive off-diagonal entries."
stieltjes matrix,"In mathematics, particularly matrix theory, a Stieltjes matrix, named after Thomas Joannes Stieltjes, is a real symmetric positive definite matrix with nonpositive off-diagonal entries. !! From the above definition, a Stieltjes matrix is a symmetric invertible Z-matrix whose eigenvalues have positive real parts. !! Every nn Stieltjes matrix is invertible to a nonsingular symmetric nonnegative matrix, though the converse of this statement is not true in general for n > 2. !! A Stieltjes matrix is necessarily an M-matrix."
diagonal entries,"In mathematics, particularly matrix theory, a Stieltjes matrix, named after Thomas Joannes Stieltjes, is a real symmetric positive definite matrix with nonpositive off-diagonal entries."
nonsingular symmetric nonnegative matrix,"Every nn Stieltjes matrix is invertible to a nonsingular symmetric nonnegative matrix, though the converse of this statement is not true in general for n > 2."
data link layer,"Alternating bit protocol (ABP) is a simple network protocol operating at the data link layer (OSI layer 2) that retransmits lost or corrupted messages using FIFO semantics. !! In the Internet Protocol Suite (TCP/IP), the data link layer functionality is contained within the link layer, the lowest layer of the descriptive model, which is assumed to be independent of physical infrastructure. !! The data link layer is concerned with local delivery of frames between nodes on the same level of the network. !! The data link layer, or layer 2, is the second layer of the seven-layer OSI model of computer networking. !! Inverse Address Resolution Protocol (Inverse ARP or InARP) is used to obtain network layer addresses (for example, IP addresses) of other nodes from data link layer (Layer 2) addresses. !! In this way, the data link layer is analogous to a neighborhood traffic cop; it endeavors to arbitrate between parties contending for access to a medium, without concern for their ultimate destination. !! The data link layer provides the functional and procedural means to transfer data between network entities and may also provide the means to detect and possibly correct errors that can occur in the physical layer. !! A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer."
broadcast domain,"In terms of current popular technologies, any computer connected to the same Ethernet repeater or switch is a member of the same broadcast domain. !! Routers and other higher-layer devices form boundaries between broadcast domains. !! A broadcast domain can be within the same LAN segment or it can be bridged to other LAN segments. !! A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer. !! Further, any computer connected to the same set of inter-connected switches/repeaters is a member of the same broadcast domain."
lan segment,A broadcast domain can be within the same LAN segment or it can be bridged to other LAN segments.
lan segments,A broadcast domain can be within the same LAN segment or it can be bridged to other LAN segments.
ethernet repeater,"In terms of current popular technologies, any computer connected to the same Ethernet repeater or switch is a member of the same broadcast domain."
broadcast domains,Routers and other higher-layer devices form boundaries between broadcast domains.
os-level virtualization,"OS-level virtualization is an operating system (OS) paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers (LXC, Solaris containers, Docker, Podman), zones (Solaris containers), virtual private servers (OpenVZ), partitions, virtual environments (VEs), virtual kernels (DragonFly BSD), or jails (FreeBSD jail or chroot jail). !! The term container, while most popularly referring to OS-level virtualization systems, is sometimes ambiguously used to refer to fuller virtual machine environments operating in varying degrees of concert with the host OS, e. g. Microsoft's Hyper-V containers."
dragonfly bsd,"OS-level virtualization is an operating system (OS) paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers (LXC, Solaris containers, Docker, Podman), zones (Solaris containers), virtual private servers (OpenVZ), partitions, virtual environments (VEs), virtual kernels (DragonFly BSD), or jails (FreeBSD jail or chroot jail)."
solaris containers,"By 2007 the term Solaris Containers came to mean a Solaris Zone combined with resource management controls. !! The Blastwave Solaris 8 and Solaris 9 Containers document was very early in the release cycle of the Solaris Containers technology and the actions and implementation at Blastwave resulted in a followup by Sun Microsystems marketing. !! OS-level virtualization is an operating system (OS) paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers (LXC, Solaris containers, Docker, Podman), zones (Solaris containers), virtual private servers (OpenVZ), partitions, virtual environments (VEs), virtual kernels (DragonFly BSD), or jails (FreeBSD jail or chroot jail). !! The Solaris operating system provides man pages for Solaris Containers by default; more detailed documentation can be found at various on-line technical resources. !! Later, there was a gradual move such that Solaris Containers specifically referred to non-global zones, with or without additional Resource Management. !! Solaris Containers (including Solaris Zones) is an implementation of operating system-level virtualization technology for x86 and SPARC systems, first released publicly in February 2004 in build 51 beta of Solaris 10, and subsequently in the first full release of Solaris 10, 2005."
virtual environments,"Thus hands-on computing is a component of user-centered design, focusing on how users physically respond to virtual environments. !! OS-level virtualization is an operating system (OS) paradigm in which the kernel allows the existence of multiple isolated user space instances, called containers (LXC, Solaris containers, Docker, Podman), zones (Solaris containers), virtual private servers (OpenVZ), partitions, virtual environments (VEs), virtual kernels (DragonFly BSD), or jails (FreeBSD jail or chroot jail)."
host os,"The term container, while most popularly referring to OS-level virtualization systems, is sometimes ambiguously used to refer to fuller virtual machine environments operating in varying degrees of concert with the host OS, e. g. Microsoft's Hyper-V containers."
analog algorithm,"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, introduced by A. K. Dewdney in his Scientific American column."
spaghetti sort,"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, introduced by A. K. Dewdney in his Scientific American column."
markov decision process,"The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques."
reinforcement learning algorithms,The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.
classical dynamic programming methods,The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.
logic design,"In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation."
microarchitecture design,"In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation."
analytical engine,"The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine."
computer system,"The software package for a stand-alone data dictionary or data repository may interact with the software modules of the DBMS, but it is mainly used by the designers, users and administrators of a computer system for information resource management. !! Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints. !! Knowledge representation and reasoning (KRR, KR&R, KR) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. !! Computer user satisfaction (and closely related concepts such as system satisfaction, user satisfaction, computer system satisfaction, end user computing satisfaction) is the attitude of a user to the computer system (s)he employs in the context of his/her work environments."
balanced binary tree,"In computer science, an optimal binary search tree (Optimal BST), sometimes called a weight-balanced binary tree, is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities)."
optimal binary search tree,"In addition to its dynamic programming algorithm, Knuth proposed two heuristics (or rules) to produce nearly (approximation of) optimal binary search trees. !! In computer science, an optimal binary search tree (Optimal BST), sometimes called a weight-balanced binary tree, is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities). !! Mehlhorn's major results state that only one of Knuth's heuristics (Rule II) always produces nearly optimal binary search trees."
knuth proposed two heuristics,"In addition to its dynamic programming algorithm, Knuth proposed two heuristics (or rules) to produce nearly (approximation of) optimal binary search trees."
optimal binary search trees,"In addition to its dynamic programming algorithm, Knuth proposed two heuristics (or rules) to produce nearly (approximation of) optimal binary search trees."
containment hierarchy,"In formal language theory, computer science and linguistics, the Chomsky hierarchy (also referred to as the ChomskySchtzenberger hierarchy) is a containment hierarchy of classes of formal grammars."
chomskyschtzenberger hierarchy,"In formal language theory, computer science and linguistics, the Chomsky hierarchy (also referred to as the ChomskySchtzenberger hierarchy) is a containment hierarchy of classes of formal grammars."
chomsky hierarchy,"In formal language theory, computer science and linguistics, the Chomsky hierarchy (also referred to as the ChomskySchtzenberger hierarchy) is a containment hierarchy of classes of formal grammars."
conceptual data model,"Data structure diagram (DSD) is a diagram of the conceptual data model which documents the entities and their relationships, as well as the constraints that connect to them."
data structure diagram,"Data Structure Diagram is a diagram type that is used to depict the structure of data elements in the data dictionary. !! Data structure diagram (DSD) is a diagram of the conceptual data model which documents the entities and their relationships, as well as the constraints that connect to them. !! Data structure diagrams are most useful for documenting complex data entities. !! The data structure diagram is a graphical alternative to the composition specifications within such data dictionary entries. !! The data structure diagrams is a predecessor of the entityrelationship model (ER model)."
data structure diagrams,The data structure diagrams is a predecessor of the entityrelationship model (ER model). !! Data structure diagrams are most useful for documenting complex data entities.
data dictionary,"If a data dictionary system is used only by the designers, users, and administrators and not by the DBMS Software, it is called a passive data dictionary. !! Data Structure Diagram is a diagram type that is used to depict the structure of data elements in the data dictionary. !! On the other hand, a data dictionary is a data structure that stores metadata, i. e. , (structured) data about information. !! The software package for a stand-alone data dictionary or data repository may interact with the software modules of the DBMS, but it is mainly used by the designers, users and administrators of a computer system for information resource management. !! A data dictionary, or metadata repository, as defined in the IBM Dictionary of Computing, is a ""centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format"". !! The terms data dictionary and data repository indicate a more general software utility than a catalogue."
data dictionary entries,The data structure diagram is a graphical alternative to the composition specifications within such data dictionary entries.
er model,The data structure diagrams is a predecessor of the entityrelationship model (ER model).
error-correcting code,"If the number of errors within a code word exceeds the error-correcting code's capability, it fails to recover the original code word. !! Locally testable codes are error-correcting codes for which it can be checked probabilistically whether a signal is close to a codeword by only looking at a small number of positions of the signal. !! Locally decodable codes are error-correcting codes for which single bits of the message can be probabilistically recovered by only looking at a small (say constant) number of positions of a codeword, even after the codeword has been corrupted at some constant fraction of positions. !! Here, each group of the same letter represents a 4-bit one-bit error-correcting codeword. !! The American mathematician Richard Hamming pioneered this field in the 1940s and invented the first error-correcting code in 1950: the Hamming (7,4) code."
locally decodable codes,"Locally decodable codes are error-correcting codes for which single bits of the message can be probabilistically recovered by only looking at a small (say constant) number of positions of a codeword, even after the codeword has been corrupted at some constant fraction of positions."
locally testable codes,Locally testable codes are error-correcting codes for which it can be checked probabilistically whether a signal is close to a codeword by only looking at a small number of positions of the signal.
microsoft windows,"Windows Media is a discontinued multimedia framework for media creation and distribution for Microsoft Windows. !! In computing, Dynamic Data Exchange (DDE) is a technology for interprocess communication used in early versions of Microsoft Windows and OS/2. !! MKS Toolkit is a software package produced and maintained by PTC that provides a Unix-like environment for scripting, connectivity and porting Unix and Linux software to Microsoft Windows. !! The dominant general-purpose personal computer operating system is Microsoft Windows with a market share of around 76. !! Symantec Workspace Virtualization (abbreviated as SWV) is an application virtualization solution for Microsoft Windows by Symantec, now known as Symantec Endpoint Virtualization Suite (SEVS)."
windows media sdk,The Windows Media SDK was replaced by Media Foundation when Windows Vista was released.
bees algorithm,"In computer science and operations research, the bees algorithm is a population-based search algorithm which was developed by Pham, Ghanbarzadeh et al. !! The only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined. !! The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies. !! The bees algorithm consists of an initialisation procedure and a main search cycle which is iterated for a given number T of times, or until a solution of acceptable fitness is found. !! The bees algorithm mimics the foraging strategy of honey bees to look for the best solution to an optimisation problem."
optimisation problem,The bees algorithm mimics the foraging strategy of honey bees to look for the best solution to an optimisation problem.
first-order predicate,"First-order logicalso known as predicate logic, quantificational logic, and first-order predicate calculusis a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. !! Any logical system which is appropriate as an instrument for the analysis of natural language needs a much richer structure than first-order predicate logic."
order predicate logic,"The relational model (RM) for database management is an approach to managing data using a structure and language consistent with first-order predicate logic, first described in 1969 by English computer scientist Edgar F. Codd, where all data is represented in terms of tuples, grouped into relations. !! Any logical system which is appropriate as an instrument for the analysis of natural language needs a much richer structure than first-order predicate logic."
graph edit distance,"Exact algorithms for computing the graph edit distance between a pair of graphs typically transform the problem into one of finding the minimum cost edit path between the two graphs. !! Most of them have cubic computational time Moreover, there is an algorithm that deduces an approximation of the GED in linear time Despite the above algorithms sometimes working well in practice, in general the problem of computing graph edit distance is NP-hard (for a proof that's available online, see Section 2 of Zeng et al. !! The concept of graph edit distance was first formalized mathematically by Alberto Sanfeliu and King-Sun Fu in 1983. !! Graph edit distance finds applications in handwriting recognition, fingerprint recognition and cheminformatics. !! In mathematics and computer science, graph edit distance (GED) is a measure of similarity (or dissimilarity) between two graphs."
two graphs,"Exact algorithms for computing the graph edit distance between a pair of graphs typically transform the problem into one of finding the minimum cost edit path between the two graphs. !! In mathematics and computer science, graph edit distance (GED) is a measure of similarity (or dissimilarity) between two graphs."
fingerprint recognition,"Graph edit distance finds applications in handwriting recognition, fingerprint recognition and cheminformatics."
exact algorithms,Exact algorithms for computing the graph edit distance between a pair of graphs typically transform the problem into one of finding the minimum cost edit path between the two graphs.
inline function,"A convenient way is to define the inline functions in header files and create one . !! The effect of the storage class extern when applied or not applied to inline functions differs between the C dialects and C++. !! This is because inline not only gives the compiler a hint that the function should be inlined, it also has an effect on whether the compiler will generate a callable out-of-line copy of the function (see storage classes of inline functions). !! Mainstream C++ compilers like Microsoft Visual C++ and GCC support an option that lets the compilers automatically inline any suitable function, even those not marked as inline functions. !! C++ and C99, but not its predecessors K&R C and C89, have support for inline functions, though with different semantics."
inline functions,"A convenient way is to define the inline functions in header files and create one . !! This is because inline not only gives the compiler a hint that the function should be inlined, it also has an effect on whether the compiler will generate a callable out-of-line copy of the function (see storage classes of inline functions). !! Mainstream C++ compilers like Microsoft Visual C++ and GCC support an option that lets the compilers automatically inline any suitable function, even those not marked as inline functions. !! C++ and C99, but not its predecessors K&R C and C89, have support for inline functions, though with different semantics."
recursive neural network,"Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step. !! An efficient approach to implement recursive neural networks is given by the Tree Echo State Network within the reservoir computing paradigm. !! Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. !! A recursive neural network is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order."
structured prediction,"Probabilistic graphical models form a large class of structured prediction models. !! Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values. !! Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. !! For example, the problem of translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees. !! A recursive neural network is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. !! Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters."
word embedding,"Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding."
tree structures,"Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding."
recursive neural networks,"Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding."
hierarchical structure,"Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step."
hidden representation,"Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step. !! When a class is used as a type, it is an abstract type that refers to a hidden representation."
reservoir computing paradigm,An efficient approach to implement recursive neural networks is given by the Tree Echo State Network within the reservoir computing paradigm.
computer programming language,Binary combinatory logic (BCL) is a computer programming language that uses binary terms 0 and 1 to create a complete formulation of combinatory logic using only the symbols 0 and 1.
binary combinatory logic,Binary combinatory logic (BCL) is a computer programming language that uses binary terms 0 and 1 to create a complete formulation of combinatory logic using only the symbols 0 and 1.
frequency-domain analysis,An example of a field in which frequency-domain analysis gives a better understanding than time domain is music; the theory of operation of musical instruments and the musical notation used to record and discuss pieces of music is implicitly based on the breaking down of complex sounds into their separate component frequencies (musical notes).
reflective programming,"In computer science, reflective programming or reflection is the ability of a process to examine, introspect, and modify its own structure and behavior."
shadow heap,"In computer science, a shadow heap is a mergeable heap data structure which supports efficient heap merging in the amortized sense. !! Finally, the merging of shadow heaps is simply done through sequential insertion of one heap into the other using the above insertion procedure. !! More specifically, shadow heaps make use of the shadow merge algorithm to achieve insertion in O(f(n)) amortized time and deletion in O((log n log log n)/f(n)) amortized time, for any choice of 1 f(n) log log n. Throughout this article, it is assumed that A and B are binary heaps with |A| |B|."
shadow heaps,"Finally, the merging of shadow heaps is simply done through sequential insertion of one heap into the other using the above insertion procedure."
cross validation,"passes may still require quite a large computation time, in which case other approaches such as k-fold cross validation may be more appropriate. !! The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (i. e. , the number of partitions). !! Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. !! Non-exhaustive cross validation methods do not compute all ways of splitting the original sample."
exhaustive cross validation methods,Non-exhaustive cross validation methods do not compute all ways of splitting the original sample.
model evaluation,"Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets."
validation set,"Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set. !! The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. !! Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available. !! Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. !! An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error). !! The term ""validation set"" is sometimes used instead of ""test set"" in some literature (e. g. , if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set)."
distributed web crawling,"Distributed web crawling is a distributed computing technique whereby Internet search engines employ many computers to index the Internet via web crawling. !! According to the FAQ about Nutch, an open-source search engine website, the savings in bandwidth by distributed web crawling are not significant, since ""A successful search engine requires more bandwidth to upload query result pages than its crawler needs to download pages."
maximum variance unfolding,"Maximum Variance Unfolding (MVU), also known as Semidefinite Embedding (SDE), is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data."
semidefinite embedding,"Maximum Variance Unfolding (MVU), also known as Semidefinite Embedding (SDE), is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data."
linear dimensionality reduction,"Maximum Variance Unfolding (MVU), also known as Semidefinite Embedding (SDE), is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data."
garbage-first collector,The Garbage-First Collector (G1) is a garbage collection algorithm introduced in the Oracle HotSpot Java virtual machine (JVM) 6 and supported from 7 Update 4.
wearable computer,"The definition of 'wearable computer' may be narrow or broad, extending to smartphones or even ordinary wristwatches. !! Many wearable computers are active all the time, e. g. processing or recording data continuously. !! A wearable computer, also known as a wearable or body-borne computer, is a computing device worn on the body. !! Under the definition of wearable computers, we also include novel user interfaces such as Google Glass, an optical head-mounted display controlled by gestures. !! Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management."
wearable computers,"Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management. !! Under the definition of wearable computers, we also include novel user interfaces such as Google Glass, an optical head-mounted display controlled by gestures."
google glass,"Under the definition of wearable computers, we also include novel user interfaces such as Google Glass, an optical head-mounted display controlled by gestures."
data management,"Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management."
personal area networks,"Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management."
heat dissipation,"Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management."
software architectures,"Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management."
web indexing,"Web indexing, or internet indexing, comprises methods for indexing the contents of a website or of the Internet as a whole. !! Metadata web indexing involves assigning keywords, description or phrases to web pages or web sites within a metadata tag (or ""meta-tag"") field, so that the web page or web site can be retrieved with a list. !! With the increase in the number of periodicals that have articles online, web indexing is also becoming important for periodical websites."
comprises methods,"Web indexing, or internet indexing, comprises methods for indexing the contents of a website or of the Internet as a whole."
internet indexing,"Web indexing, or internet indexing, comprises methods for indexing the contents of a website or of the Internet as a whole."
web site,"Metadata web indexing involves assigning keywords, description or phrases to web pages or web sites within a metadata tag (or ""meta-tag"") field, so that the web page or web site can be retrieved with a list."
metadata tag,"Metadata web indexing involves assigning keywords, description or phrases to web pages or web sites within a metadata tag (or ""meta-tag"") field, so that the web page or web site can be retrieved with a list."
web page,"Metadata web indexing involves assigning keywords, description or phrases to web pages or web sites within a metadata tag (or ""meta-tag"") field, so that the web page or web site can be retrieved with a list. !! Viewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser or by following a hyperlink to that page or resource. !! Web page for a text book on relational data mining !! In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning)."
tag  field,"Metadata web indexing involves assigning keywords, description or phrases to web pages or web sites within a metadata tag (or ""meta-tag"") field, so that the web page or web site can be retrieved with a list."
orthogonal complement,These tests are equivalent to finding the span of the Grammians associated with the system/output maps so the uncontrollable and unobservable subspaces are simply the orthogonal complement to the Krylov subspace.
computational statistics & data analysis,Computational Statistics & Data Analysis is a monthly peer-reviewed scientific journal covering research on and applications of computational statistics and data analysis.
supervised machine learning techniques,"Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values."
commonly used supervised learning techniques,"Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters."
structured prediction problem,"For example, the problem of translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees."
parse tree,"All shift-reduce parsers have similar outward effects, in the incremental order in which they build a parse tree or call specific output actions. !! For example, the problem of translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees."
linear search,"A linear search runs in at worst linear time and makes at most n comparisons, where n is the length of the list. !! In computer science, a linear search or sequential search is a method for finding an element within a list. !! A linear search sequentially checks each element of the list until it finds an element that matches the target value. !! If each element is equally likely to be searched, then linear search has an average case of n+1/2 comparisons, but the average case can be affected if the search probabilities for each element vary. !! Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, allow significantly faster searching for all but short lists. !! Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include tree traversal, such as in a depth-first search."
sequential search,"In computer science, a linear search or sequential search is a method for finding an element within a list."
binary search algorithm,"0 offers static generic versions of the binary search algorithm in its collection base classes. !! Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, allow significantly faster searching for all but short lists. !! The standard binary search algorithm is simply the case where the graph is a path. !! ""Binary search algorithm"" (PDF). !! Noisy binary search algorithms solve the case where the algorithm cannot reliably compare elements of the array. !! Every published binary search algorithm worked only for arrays whose length is one less than a power of two until 1960, when Derrick Henry Lehmer published a binary search algorithm that worked on all arrays."
Performance tuning,"A system's ability to accept higher load is called scalability, and modifying a system to handle a higher load is synonymous to performance tuning. !! Performance tuning is the improvement of system performance. !! System construction, including performance tuning."
performance tuning,"A system's ability to accept higher load is called scalability, and modifying a system to handle a higher load is synonymous to performance tuning. !! Performance tuning is the improvement of system performance. !! System construction, including performance tuning."
data science,"However, data science is different from computer science and information science. !! Data science is a ""concept to unify statistics, data analysis, informatics, and their related methods"" in order to ""understand and analyze actual phenomena"" with data. !! Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. !! Data science is related to data mining, machine learning and big data. !! Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge."
application domains,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains."
Hierarchical database model,"The hierarchical database model mandates that each child record has only one parent, whereas each parent record can have one or more child records. !! A hierarchical database model is a data model in which the data are organized into a tree-like structure."
hierarchical database model,A hierarchical database model is a data model in which the data are organized into a tree-like structure.
stream learning,"Data Stream Mining (also known as stream learning) is the process of extracting knowledge structures from continuous, rapid data records."
data stream mining,"A data stream is an ordered sequence of instances that in many applications of data stream mining can be read only once or a small number of times using limited computing and storage capabilities. !! In many data stream mining applications, the goal is to predict the class or value of new instances in the data stream given some knowledge about the class membership or values of previous instances in the data stream. !! Data Stream Mining (also known as stream learning) is the process of extracting knowledge structures from continuous, rapid data records. !! Detecting concept drift is a central issue to data stream mining. !! Data stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery."
ordered sequence,A data stream is an ordered sequence of instances that in many applications of data stream mining can be read only once or a small number of times using limited computing and storage capabilities. !! The lexicographic breadth-first search algorithm replaces the queue of vertices of a standard breadth-first search with an ordered sequence of sets of vertices.
class membership,"In many data stream mining applications, the goal is to predict the class or value of new instances in the data stream given some knowledge about the class membership or values of previous instances in the data stream."
program transformation system,"may be of equal difficulty as building the program transformation system itself because of the complexity of such languages. !! DMS Software Reengineering Toolkit: A Program Transformation System for DSLs and modern (C++, Java, . !! While the transformations can be performed manually, it is often more practical to use a program transformation system that applies specifications of the required transformations."
abstract syntax trees,"Abstract syntax trees are also used in program analysis and program transformation systems. !! Abstract syntax trees are data structures widely used in compilers to represent the structure of program code. !! Program transformations may be specified as automated procedures that modify compiler data structures (e. g. abstract syntax trees) representing the program text, or may be specified more conveniently using patterns or templates representing parameterized source code fragments."
dms software reengineering toolkit,"DMS Software Reengineering Toolkit: A Program Transformation System for DSLs and modern (C++, Java, ."
prolog iii,"The first implementations of constraint logic programming were Prolog III, CLP(R), and CHIP."
bat algorithm,"A detailed introduction of metaheuristic algorithms including the bat algorithm is given by Yang where a demo program in MATLAB/GNU Octave is available, while a comprehensive review is carried out by Parpinelli and Lopes. !! This essentially uses a frequency-tuning technique to control the dynamic behaviour of a swarm of bats, and the balance between exploration and exploitation can be controlled by tuning algorithm-dependent parameters in bat algorithm. !! The Bat algorithm was developed by Xin-She Yang in 2010. !! A further improvement is the development of an evolving bat algorithm (EBA) with better efficiency. !! The Bat algorithm is a metaheuristic algorithm for global optimization."
metaheuristic algorithm,The Bat algorithm is a metaheuristic algorithm for global optimization.
global optimization,"Global optimization is distinguished from local optimization by its focus on finding the minimum or maximum over the given set, as opposed to finding local minima or maxima. !! Stochastic tunneling (STUN) is an approach to global optimization based on the Monte Carlo method-sampling of the function to be objectively minimized in which the function is nonlinearly transformed to allow for easier tunneling among regions containing function minima. !! Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set. !! The Bat algorithm is a metaheuristic algorithm for global optimization."
tuning algorithm,"This essentially uses a frequency-tuning technique to control the dynamic behaviour of a swarm of bats, and the balance between exploration and exploitation can be controlled by tuning algorithm-dependent parameters in bat algorithm."
loop inversion,"Additionally, loop inversion allows safe loop-invariant code motion. !! In computer science, loop inversion is a compiler optimization and loop transformation in which a while loop is replaced by an if block containing a do."
loop transformation,"In computer science, loop inversion is a compiler optimization and loop transformation in which a while loop is replaced by an if block containing a do."
arm system-on-chip architecture,"ARM System-on-Chip Architecture is a book detailing the system on a chip ARM architecture, as a specific implementation of reduced instruction set computing."
hidden random variables,A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables.
undirected probabilistic graphical model,A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables.
binary pairwise markov random field,A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables.
memory hierarchy,"This is a general memory hierarchy structuring. !! Designing for high performance requires considering the restrictions of the memory hierarchy, i. e. the size and capabilities of each component. !! In computer architecture, the memory hierarchy separates computer storage into a hierarchy based on response time. !! Adding complexity slows down the memory hierarchy. !! Cache hierarchy is a form and part of memory hierarchy and can be considered a form of tiered storage. !! Memory hierarchy affects performance in computer architectural design, algorithm predictions, and lower level programming constructs involving locality of reference."
algorithm predictions,"Memory hierarchy affects performance in computer architectural design, algorithm predictions, and lower level programming constructs involving locality of reference."
computer architectural design,"Memory hierarchy affects performance in computer architectural design, algorithm predictions, and lower level programming constructs involving locality of reference."
automatic vectorization,"Automatic vectorization, like any loop optimization or other compile-time optimization, must exactly preserve program behavior. !! So, many optimizing compilers perform automatic vectorization, where parts of sequential programs are transformed into parallel operations. !! Automatic vectorization, in parallel computing, is a special case of automatic parallelization, where a computer program is converted from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once. !! These vector operations perform additions on blocks of elements from the arrays a, b and c. Automatic vectorization is a major research topic in computer science."
automatic parallelization,"Automatic vectorization, in parallel computing, is a special case of automatic parallelization, where a computer program is converted from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once."
sequential programs,"So, many optimizing compilers perform automatic vectorization, where parts of sequential programs are transformed into parallel operations."
loop optimization,"This presents challenges when reasoning about the correctness and benefits of a loop optimization, specifically the representations of the computation being optimized and the optimization(s) being performed. !! Automatic vectorization, like any loop optimization or other compile-time optimization, must exactly preserve program behavior. !! The boundaries of the polytopes, the data dependencies, and the transformations are often described using systems of constraints, and this approach is often referred to as a constraint-based approach to loop optimization. !! Since instructions inside loops can be executed repeatedly, it is frequently not possible to give a bound on the number of instruction executions that will be impacted by a loop optimization. !! Loop optimization can be viewed as the application of a sequence of specific loop transformations (listed below or in Compiler transformations for high-performance computing) to the source code or intermediate representation, with each transformation having an associated test for legality. !! In compiler theory, loop optimization is the process of increasing execution speed and reducing the overheads associated with loops."
median cut,"Median cut is an algorithm to sort data of an arbitrary number of dimensions into series of sets by recursively cutting each set of data at the median point along the longest dimension. !! Median cut is typically used for color quantization. !! For example, to reduce a 64k-colour image to 256 colours, median cut is used to find 256 colours that match the original data well. !! (It is this step that gives the median cut algorithm its name; the buckets are divided into two at the median of the list of pixels. )"
color quantization,Median cut is typically used for color quantization.
median cut algorithm,(It is this step that gives the median cut algorithm its name; the buckets are divided into two at the median of the list of pixels. )
bayesian network models,Variable-order Bayesian network (VOBN) models provide an important extension of both the Bayesian network models and the variable-order Markov models.
variable-order bayesian network,Variable-order Bayesian network (VOBN) models provide an important extension of both the Bayesian network models and the variable-order Markov models.
port number,"Traffic classification is an automated process which categorises computer network traffic according to various parameters (for example, based on port number or protocol) into a number of traffic classes."
traffic classification,"Traffic classification is an automated process which categorises computer network traffic according to various parameters (for example, based on port number or protocol) into a number of traffic classes. !! A comprehensive comparison of various network traffic classifiers, which depend on Deep Packet Inspection (PACE, OpenDPI, 4 different configurations of L7-filter, NDPI, Libprotoident, and Cisco NBAR), is shown in the Independent Comparison of Popular DPI Tools for Traffic Classification. !! Defensive responses to denial-of-service attacks typically involve the use of a combination of attack detection, traffic classification and response tools, aiming to block traffic that they identify as illegitimate and allow traffic that they identify as legitimate. !! It's been generally proven that using methods based on neural networks, vector support machines, statistics, and the nearest neighbors are a great way to do this traffic classification, but in some specific cases some methods are better than others, for example: neural networks work better when the whole observation set is taken into account. !! This same problem with traffic classification is also present in multimedia traffic. !! In these cases, traffic classification mechanisms identify this traffic, allowing the network operator to either block this traffic entirely, or severely hamper its operation."
traffic classes,"Traffic classification is an automated process which categorises computer network traffic according to various parameters (for example, based on port number or protocol) into a number of traffic classes."
deep packet inspection,"A comprehensive comparison of various network traffic classifiers, which depend on Deep Packet Inspection (PACE, OpenDPI, 4 different configurations of L7-filter, NDPI, Libprotoident, and Cisco NBAR), is shown in the Independent Comparison of Popular DPI Tools for Traffic Classification."
multimedia traffic,This same problem with traffic classification is also present in multimedia traffic.
nearest neighbors,"It's been generally proven that using methods based on neural networks, vector support machines, statistics, and the nearest neighbors are a great way to do this traffic classification, but in some specific cases some methods are better than others, for example: neural networks work better when the whole observation set is taken into account."
vector support machines,"It's been generally proven that using methods based on neural networks, vector support machines, statistics, and the nearest neighbors are a great way to do this traffic classification, but in some specific cases some methods are better than others, for example: neural networks work better when the whole observation set is taken into account."
network operator,"In these cases, traffic classification mechanisms identify this traffic, allowing the network operator to either block this traffic entirely, or severely hamper its operation."
algorithmic probability,"In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. !! The algorithmic probability of any given finite output prefix q is the sum of the probabilities of the programs that compute something starting with q. !! Algorithmic probability deals with the following questions: Given a body of data about some phenomenon that we want to understand, how can we select the most probable hypothesis of how it was caused from among all possible hypotheses and how can we evaluate the different hypotheses !! Solomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: ""A Preliminary Report on a General Theory of Inductive Inference. "" !! Four principal inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e. g. use of a universal Turing machine) and Bayes rule for prediction."
solomonoff probability,"In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation."
universal turing machine,"Four principal inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e. g. use of a universal Turing machine) and Bayes rule for prediction."
modern computing theory,"Four principal inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e. g. use of a universal Turing machine) and Bayes rule for prediction."
bayes rule,"Four principal inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e. g. use of a universal Turing machine) and Bayes rule for prediction."
program loop,"In computer science, a loop invariant is a property of a program loop that is true before (and after) each iteration."
loop invariant,"In fact, the loop invariant is often the same as the inductive hypothesis to be proved for a recursive program equivalent to a given loop. !! The loop invariants will be true on entry into a loop and following each iteration, so that on exit from the loop both the loop invariants and the loop termination condition can be guaranteed. !! From a programming methodology viewpoint, the loop invariant can be viewed as a more abstract specification of the loop, which characterizes the deeper purpose of the loop beyond the details of this implementation. !! In formal program verification, particularly the Floyd-Hoare approach, loop invariants are expressed by formal predicate logic and used to prove properties of loops and by extension algorithms that employ loops (usually correctness properties). !! In computer science, a loop invariant is a property of a program loop that is true before (and after) each iteration."
formal predicate logic,"In formal program verification, particularly the Floyd-Hoare approach, loop invariants are expressed by formal predicate logic and used to prove properties of loops and by extension algorithms that employ loops (usually correctness properties)."
extension algorithms,"In formal program verification, particularly the Floyd-Hoare approach, loop invariants are expressed by formal predicate logic and used to prove properties of loops and by extension algorithms that employ loops (usually correctness properties)."
loop invariants,"The loop invariants will be true on entry into a loop and following each iteration, so that on exit from the loop both the loop invariants and the loop termination condition can be guaranteed. !! In formal program verification, particularly the Floyd-Hoare approach, loop invariants are expressed by formal predicate logic and used to prove properties of loops and by extension algorithms that employ loops (usually correctness properties)."
formal program verification,"In formal program verification, particularly the Floyd-Hoare approach, loop invariants are expressed by formal predicate logic and used to prove properties of loops and by extension algorithms that employ loops (usually correctness properties)."
loop termination condition,"The loop invariants will be true on entry into a loop and following each iteration, so that on exit from the loop both the loop invariants and the loop termination condition can be guaranteed."
recursive program equivalent,"In fact, the loop invariant is often the same as the inductive hypothesis to be proved for a recursive program equivalent to a given loop."
inductive hypothesis,"In fact, the loop invariant is often the same as the inductive hypothesis to be proved for a recursive program equivalent to a given loop."
artificial intelligence system,"Artificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence."
residual neural network,"Residual neural networks utilize skip connections, or shortcuts to jump over some layers. !! A residual neural network (ResNet) is an artificial neural network (ANN). !! In the context of residual neural networks, a non-residual network may be described as a plain network."
residual neural networks,"In the context of residual neural networks, a non-residual network may be described as a plain network."
plain network,"In the context of residual neural networks, a non-residual network may be described as a plain network."
cluster analysis,"Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology. !! In the data mining community these methods are recognized as a theoretical foundation of cluster analysis, but often considered obsolete. !! Cluster analysis itself is not one specific algorithm, but the general task to be solved. !! Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses. !! Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. !! Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)."
iterative process,"Wing-shape optimization is by nature an iterative process. !! Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure."
service-oriented infrastructure,"While the IT industry has widely adopted service-oriented architecture (SOA), service-oriented infrastructure or SOI has lagged in its adoption. !! A service-oriented infrastructure provides a foundation for IT services. !! Key aspects of service-oriented infrastructure include industrialisation and virtualisation, providing IT infrastructure services via a pool of resources (web servers, application servers, database servers, servers, storage instances) instead of through discrete instances."
database servers,"Key aspects of service-oriented infrastructure include industrialisation and virtualisation, providing IT infrastructure services via a pool of resources (web servers, application servers, database servers, servers, storage instances) instead of through discrete instances."
application servers,"Key aspects of service-oriented infrastructure include industrialisation and virtualisation, providing IT infrastructure services via a pool of resources (web servers, application servers, database servers, servers, storage instances) instead of through discrete instances."
storage instances,"Key aspects of service-oriented infrastructure include industrialisation and virtualisation, providing IT infrastructure services via a pool of resources (web servers, application servers, database servers, servers, storage instances) instead of through discrete instances."
computer graphics lighting,Computer graphics lighting is the collection of techniques used to simulate light in computer graphics scenes.
computer graphics scenes,Computer graphics lighting is the collection of techniques used to simulate light in computer graphics scenes.
weighted gene co,"Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables."
weighted correlation network analysis,"Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables."
pairwise correlations,"Weighted correlation network analysis, also known as weighted gene co-expression network analysis (WGCNA), is a widely used data mining method especially for studying biological networks based on pairwise correlations between variables."
abstract arguments,"In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition."
abstract argumentation framework,"In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition."
argumentation frameworks,There exists several criteria of equivalence between argumentation frameworks.
dimensionality reduction,"This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm. !! The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. !! Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. !! Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses. !! Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics. !! For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning."
dimension reduction,"For high-dimensional datasets (i. e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality. !! Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. !! For high-dimensional data (e. g. , with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality. !! Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. !! Autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation."
noise reduction,"Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses."
tensor representation,"For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning."
multilinear subspace learning,"For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning."
multidimensional data,"For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning."
end-user computing,"End-user computing allows more user-input into system affairs that can range from personalization to full-fledged ownership of a system. !! Some of the issues related to end-user computing concern software architecture (iconic versus language interfaces, open versus closed, and others). !! End-user computing can range in complexity from users simply clicking a series of buttons, to citizen developers writing scripts in a controlled scripting language, to being able to modify and execute code directly. !! End-user computing (EUC) refers to systems in which non-programmers can create working applications. !! Examples of end-user computing are systems built using fourth-generation programming languages, such as MAPPER or SQL, or one of the fifth-generation programming languages, such as ICAD."
controlled scripting language,"End-user computing can range in complexity from users simply clicking a series of buttons, to citizen developers writing scripts in a controlled scripting language, to being able to modify and execute code directly."
log-linear models,"The specific applications of log-linear models are where the output quantity lies in the range 0 to , for values of the independent variables X, or more immediately, the transformed quantities fi(X) in the range to +."
special purpose computers,"Artificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks."
machine imagination,"Artificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks."
artificial imagination,"The term artificial imagination is also used to describe a property of machines or programs. !! Some researchers such as G. Schleis and M. Rizki have focused on using artificial neural networks to simulate artificial imagination. !! Artificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks. !! Artificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic. !! Some articles on the topic speculate on how artificial imagination may evolve to create an artificial world ""people may be comfortable enough to escape from the real world""."
artificial simulation,"Artificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks."
dependency inversion principle,"Applying the dependency inversion principle can also be seen as an example of the adapter pattern. !! A Little Architecture, Robert C. Martin (Uncle Bob), Jan 2016 - a blog on significance of Dependency Inversion Principle in software architecture !! In many projects the dependency inversion principle and pattern are considered as a single concept that should be generalized, i. e. , applied to all interfaces between software modules. !! The dependency inversion principle was postulated by Robert C. Martin and described in several publications including the paper Object Oriented Design Quality Metrics: an analysis of dependencies, an article appearing in the C++ Report in May 1996 entitled The Dependency Inversion Principle, and the books Agile Software Development, Principles, Patterns, and Practices, and Agile Principles, Patterns, and Practices in C#. !! In object-oriented design, the dependency inversion principle is a specific methodology for loosely coupling software modules."
software modules,"The software package for a stand-alone data dictionary or data repository may interact with the software modules of the DBMS, but it is mainly used by the designers, users and administrators of a computer system for information resource management. !! In many projects the dependency inversion principle and pattern are considered as a single concept that should be generalized, i. e. , applied to all interfaces between software modules."
adapter pattern,Applying the dependency inversion principle can also be seen as an example of the adapter pattern.
agile principles,"The dependency inversion principle was postulated by Robert C. Martin and described in several publications including the paper Object Oriented Design Quality Metrics: an analysis of dependencies, an article appearing in the C++ Report in May 1996 entitled The Dependency Inversion Principle, and the books Agile Software Development, Principles, Patterns, and Practices, and Agile Principles, Patterns, and Practices in C#."
software architecture,"Software architecture choices include specific structural options from possibilities in the design of the software. !! The software engineering community uses an architecture description language as a computer language to create a description of a software architecture. !! Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. !! A Little Architecture, Robert C. Martin (Uncle Bob), Jan 2016 - a blog on significance of Dependency Inversion Principle in software architecture !! Software architecture is about making fundamental structural choices that are costly to change once implemented. !! Documenting software architecture facilitates communication between stakeholders, captures early decisions about the high-level design, and allows reuse of design components between projects. !! A set of architectural design decisions: software architecture should not be considered merely a set of models or structures, but should include the decisions that lead to these particular structures, and the rationale behind them."
diff utility,"The longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff utility, and has applications in computational linguistics and bioinformatics."
data comparison programs,"The longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff utility, and has applications in computational linguistics and bioinformatics."
tracywidom distribution,Simplified mathematical models of the longest common subsequence problem have been shown to be controlled by the TracyWidom distribution.
regression tree,"Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. !! Regression tree analysis is when the predicted outcome can be considered a real number (e. g. the price of a house, or a patient's length of stay in a hospital). !! The term classification and regression tree (CART) analysis is an umbrella term used to refer to either of the above procedures, first introduced by Breiman et al. !! Introduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. !! Used by the CART (classification and regression tree) algorithm for classification trees, Gini impurity (named after Italian mathematician Corrado Gini) is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset."
target variable,"Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. !! Introduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied."
regression tree analysis,"Regression tree analysis is when the predicted outcome can be considered a real number (e. g. the price of a house, or a patient's length of stay in a hospital)."
predicted outcome,"Regression tree analysis is when the predicted outcome can be considered a real number (e. g. the price of a house, or a patient's length of stay in a hospital)."
term classification,"The term classification and regression tree (CART) analysis is an umbrella term used to refer to either of the above procedures, first introduced by Breiman et al."
classification trees,"Used by the CART (classification and regression tree) algorithm for classification trees, Gini impurity (named after Italian mathematician Corrado Gini) is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset."
gini impurity,"Used by the CART (classification and regression tree) algorithm for classification trees, Gini impurity (named after Italian mathematician Corrado Gini) is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset."
variance reduction,"Introduced in CART, variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied."
cross entropy,Cross Entropy
compiler transformations,"Loop optimization can be viewed as the application of a sequence of specific loop transformations (listed below or in Compiler transformations for high-performance computing) to the source code or intermediate representation, with each transformation having an associated test for legality."
binomial heaps,Binomial heaps were invented in 1978 by Jean Vuillemin.
temporal logic,"Temporal logic has found an important application in formal verification, where it is used to state requirements of hardware or software systems. !! In logic, temporal logic is any system of rules and symbolism for representing, and reasoning about, propositions qualified in terms of time (for example, ""I am always hungry"", ""I will eventually be hungry"", or ""I will be hungry until I eat something""). !! It is sometimes also used to refer to tense logic, a modal logic-based system of temporal logic introduced by Arthur Prior in the late 1950s, with important contributions by Hans Kamp. !! In a temporal logic, a statement can have a truth value that varies in timein contrast with an atemporal logic, which applies only to statements whose truth values are constant in time. !! Such a statement can conveniently be expressed in a temporal logic."
pseudo-boolean function,"This is an important class of pseudo-boolean functions, because they can be minimized in polynomial time. !! This can easily be seen by formulating, for example, the maximum cut problem as maximizing a pseudo-Boolean function. !! Minimizing (or, equivalently, maximizing) a pseudo-Boolean function is NP-hard. !! The degree of the pseudo-Boolean function is simply the degree of the polynomial in this representation."
maximum cut problem,"This can easily be seen by formulating, for example, the maximum cut problem as maximizing a pseudo-Boolean function. !! The route inspection problem may be solved in polynomial time, and this duality allows the maximum cut problem to also be solved in polynomial time for planar graphs."
array access analysis,"Array access analysis aims to obtain the knowledge of which portions or even which elements of the array are accessed by a given code segment (basic block, loop, or even at the procedure level). !! Typical exact array access analysis include linearization and atom images. !! Array access analysis can be largely categorized into exact (or reference-list-based) and summary methods for different tradeoffs of accuracy and complexity. !! In computer science, array access analysis is a compiler analysis approach used to decide the read and write access patterns to elements or portions of arrays."
computational human modeling,"Research in computational human modeling can include computer vision studies on identify (face recognition), attributes (gender, age, skin color), expressions, geometry (3D face modeling, 3D body modeling), and activity (pose, gaze, actions, and social interactions). !! Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! Computational human modeling emphasizes descriptions of human for A. I. research and applications."
3d face modeling,"Research in computational human modeling can include computer vision studies on identify (face recognition), attributes (gender, age, skin color), expressions, geometry (3D face modeling, 3D body modeling), and activity (pose, gaze, actions, and social interactions)."
3d body modeling,"Research in computational human modeling can include computer vision studies on identify (face recognition), attributes (gender, age, skin color), expressions, geometry (3D face modeling, 3D body modeling), and activity (pose, gaze, actions, and social interactions)."
analysis of algorithms,"The term ""analysis of algorithms"" was coined by Donald Knuth. !! Analysis of algorithms typically focuses on the asymptotic performance, particularly at the elementary level, but in practical applications constant factors are important, and real-world data is in practice always limited in size. !! An Introduction to the Analysis of Algorithms (2nd ed. !! In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i. e. , to estimate the complexity function for arbitrarily large input. !! In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithmsthe amount of time, storage, or other resources needed to execute them."
bagged nearest neighbour classifier,Similar results are true when using a bagged nearest neighbour classifier.
presentation semantics,"</bold> must render the text between these constructs using some bold typeface is a specification of presentation semantics for that syntax. !! Character encoding standards, such as Unicode, also have presentation semantics. !! One of the main goals of style sheet languages is to separate the syntax that defines document content from the syntax endowed with presentation semantics. !! In computer science, particularly in human-computer interaction, presentation semantics specify how a particular piece of a formal language is represented in a distinguished manner accessible to human senses, usually human vision. !! Many markup languages, including HTML, DSSSL, and XSL-FO, have presentation semantics, but others, such as XML, do not."
character encoding standards,"Character encoding standards, such as Unicode, also have presentation semantics."
style sheet languages,One of the main goals of style sheet languages is to separate the syntax that defines document content from the syntax endowed with presentation semantics.
density-based clustering,"In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. !! Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using ""correlation connectivity"" and ERiC exploring hierarchical density-based correlation clusters)."
correlation clustering,"This led to new clustering algorithms for high-dimensional data that focus on subspace clustering (where only some attributes are used, and cluster models include the relevant attributes for the cluster) and correlation clustering that also looks for arbitrary rotated (""correlated"") subspace clusters that can be modeled by giving a correlation of their attributes. !! Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using ""correlation connectivity"" and ERiC exploring hierarchical density-based correlation clusters)."
hierarchical correlation clustering,"Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using ""correlation connectivity"" and ERiC exploring hierarchical density-based correlation clusters)."
hierarchical subspace clustering,"Ideas from density-based clustering methods (in particular the DBSCAN/OPTICS family of algorithms) have been adapted to subspace clustering (HiSC, hierarchical subspace clustering and DiSH) and correlation clustering (HiCO, hierarchical correlation clustering, 4C using ""correlation connectivity"" and ERiC exploring hierarchical density-based correlation clusters)."
action model learning,"Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. !! In the older paper from 1992, the action model learning was studied as an extension of reinforcement learning. !! Despite mutual relevance of the topics, action model learning is usually not addressed on planning conferences like ICAPS. !! Action model learning is a form of inductive reasoning, where new knowledge is generated based on agent's observations. !! Usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments)."
inductive reasoning,"Action model learning is a form of inductive reasoning, where new knowledge is generated based on agent's observations. !! Although its name may suggest otherwise, mathematical induction should not be confused with inductive reasoning as used in philosophy (see Problem of induction)."
action models,"Usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments)."
multimodal sentiment analysis,"In multimodal sentiment analysis, a combination of different textual, audio, and visual features are employed. !! Similar to the conventional text-based sentiment analysis, some of the most commonly used textual features in multimodal sentiment analysis are unigrams and n-grams, which are basically a sequence of words in a given textual document. !! Similar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. !! With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others. !! Multimodal sentiment analysis is a new dimensionThe template Peacock term is being considered for merging."
virtual assistants,"With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others."
sentiment classification,"Similar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral."
terminal and nonterminal symbols,"In computer science, terminal and nonterminal symbols are the lexical elements used in specifying the production rules constituting a formal grammar."
nonterminal symbols,"In computer science, terminal and nonterminal symbols are the lexical elements used in specifying the production rules constituting a formal grammar."
information systems,"Query languages, data query languages or database query languages (DQLs) are computer languages used to make queries in databases and information systems. !! The earliest rudiments of the step-wise planning methodology currently advocated by The Open Group Architecture Framework (TOGAF) and other EA frameworks can be traced back to the article of Marshall K. Evans and Lou R. Hague titled ""Master Plan for Information Systems"" published in 1962 in Harvard Business Review."
meta-learning algorithms,What optimization-based meta-learning algorithms intend for is to adjust the optimization algorithm so that the model can be good at learning with a few examples.
chain rule,"""The relationship between this example and the chain rule is as follows. !! The chain rule may also be expressed in Leibniz's notation. !! In integration, the counterpart to the chain rule is the substitution rule. !! which is also an application of the chain rule. !! Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change."
behavior tree,"Behavior trees are a formal, graphical modelling language used primarily in systems and software engineering. !! Behavior trees employ a well-defined notation to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. !! Single and composite or integrated behavior tree forms are both important in the application of behavior trees in systems and software engineering. !! Because the behavior tree notation uses a formal semantics, for any given example, it already is, or can be made executable. !! The behavior tree representation, (with the help of the composition tree representation that resolves alias and other vocabulary problems with large sets of requirements) allows people to avoid short-term memory overload and produce a deep, accurate, holistic representation of system needs that can be understood by all stakeholders because it strictly uses the vocabulary of the original requirements."
behavior trees,"Behavior trees are a formal, graphical modelling language used primarily in systems and software engineering. !! Single and composite or integrated behavior tree forms are both important in the application of behavior trees in systems and software engineering."
multidimensional signal processing,"Media related to Multidimensional signal processing at Wikimedia Commons !! While multidimensional signal processing is a subset of signal processing, it is unique in the sense that it deals specifically with data that can only be adequately detailed using more than one dimension. !! Typically, multidimensional signal processing is directly associated with digital signal processing because its complexity warrants the use of computer modelling and computation. !! In signal processing, multidimensional signal processing covers all signal processing done using multidimensional signals and systems."
recursion theory,"Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees."
graphics processing units,"It was used in a number of graphics cards, and was licensed for clones such as the Intel 82720, the first of Intel's graphics processing units. !! Additional technologies for digital signal processing include more powerful general purpose microprocessors, graphics processing units, field-programmable gate arrays (FPGAs), digital signal controllers (mostly for industrial applications such as motor control), and stream processors."
graphics cards,"PVP-UAB (Protected Video Path) provides bus encryption of premium video content in PCs as it passes over the PCIe bus to graphics cards to enforce digital rights management. !! It was used in a number of graphics cards, and was licensed for clones such as the Intel 82720, the first of Intel's graphics processing units."
integrated graphics processing unit,"Integrated graphics processing unit (IGPU), Integrated graphics, shared graphics solutions, integrated graphics processors (IGP) or unified memory architecture (UMA) utilize a portion of a computer's system RAM rather than dedicated graphics memory."
unified memory architecture,"Integrated graphics processing unit (IGPU), Integrated graphics, shared graphics solutions, integrated graphics processors (IGP) or unified memory architecture (UMA) utilize a portion of a computer's system RAM rather than dedicated graphics memory."
general purpose graphics processing unit,"It is becoming increasingly common to use a general purpose graphics processing unit (GPGPU) as a modified form of stream processor (or a vector processor), running compute kernels."
modified form,"It is becoming increasingly common to use a general purpose graphics processing unit (GPGPU) as a modified form of stream processor (or a vector processor), running compute kernels."
stream processor,"It is becoming increasingly common to use a general purpose graphics processing unit (GPGPU) as a modified form of stream processor (or a vector processor), running compute kernels."
database theory,"Database theory encapsulates a broad range of topics related to the study and research of the theoretical realm of databases and database management systems. !! Media related to Database theory at Wikimedia Commons !! The main research conferences in the area are the ACM Symposium on Principles of Database Systems (PODS) and the International Conference on Database Theory (ICDT). !! ""Data hierarchy"" is a basic concept in data and database theory and helps to show the relationships between smaller and larger components in a database or data file. !! A central focus of database theory is on understanding the complexity and power of query languages and their connection to logic. !! The problem of database repair is a question about relational databases which has been studied in database theory, and which is a particular kind of data cleansing."
multivariable process control systems,The Relative Gain Array (RGA) is a classical widely-used method for determining the best input-output pairings for multivariable process control systems.
relative gain array,The Relative Gain Array (RGA) is a classical widely-used method for determining the best input-output pairings for multivariable process control systems.
web frameworks,"For example, many web frameworks provide libraries for database access, templating frameworks, and session management, and they often promote code reuse. !! Web frameworks aim to automate the overhead associated with common activities performed in web development. !! Most web frameworks are based on the modelviewcontroller (MVC) pattern. !! Web frameworks provide a standard way to build and deploy web applications on the World Wide Web. !! Web frameworks must function according to the architectural rules of browsers and protocols such as HTTP, which is stateless."
web development,Web frameworks aim to automate the overhead associated with common activities performed in web development.
database access,"For example, many web frameworks provide libraries for database access, templating frameworks, and session management, and they often promote code reuse."
web mining,"The agent-based approach to web mining involves the development of sophisticated AI systems that can act autonomously or semi-autonomously on behalf of a particular user, to discover and organize web-based information. !! Web mining is the application of data mining techniques to discover patterns from the World Wide Web. !! Web mining can complement the retrieval of structured data transmitted with open protocols like OAI-PMH: an example is the aggregation of works from academic publications, which are mined to identify open access versions through a mix of open source and open data methods by academic databases like Unpaywall. !! There are three main sub-categories of web mining. !! Web mining can be divided into three different types Web usage mining, Web content mining and Web structure mining."
data mining techniques,Web mining is the application of data mining techniques to discover patterns from the World Wide Web.
web structure mining,"Web mining can be divided into three different types Web usage mining, Web content mining and Web structure mining."
web content mining,"Web mining can be divided into three different types Web usage mining, Web content mining and Web structure mining."
web performance optimization,"Web performance optimization (WPO), or website optimization is the field of knowledge about increasing web performance. !! Steve Souders coined the term ""web performance optimization"" in 2004."
website optimization,"Web performance optimization (WPO), or website optimization is the field of knowledge about increasing web performance."
web developers,Web performance optimization improves user experience (UX) when visiting a website and therefore is highly desired by web designers and web developers.
web designers,Web performance optimization improves user experience (UX) when visiting a website and therefore is highly desired by web designers and web developers.
spanning tree,"A maximum spanning tree is a spanning tree with weight greater than or equal to the weight of every other spanning tree. !! The result of a depth-first search of a graph can be conveniently described in terms of a spanning tree of the vertices reached during the search. !! If all of the edges of G are also edges of a spanning tree T of G, then G is a tree and is identical to T (that is, a tree has a unique spanning tree and it is itself). !! In order to avoid bridge loops and routing loops, many routing protocols designed for such networksincluding the Spanning Tree Protocol, Open Shortest Path First, Link-state routing protocol, Augmented tree-based routing, etc. !! , people often use algorithms that gradually build a spanning tree (or many such trees) as intermediate steps in the process of finding the minimum spanning tree. !! In the mathematical field of graph theory, a spanning tree T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G. In general, a graph may have several spanning trees, but a graph that is not connected will not contain a spanning tree (see spanning forests below). !! Several pathfinding algorithms, including Dijkstra's algorithm and the A* search algorithm, internally build a spanning tree as an intermediate step in solving the problem."
minimum spanning tree,"In computer science, Prim's algorithm (also known as Jarnk's algorithm) is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. !! More generally, any edge-weighted undirected graph (not necessarily connected) has a minimum spanning forest, which is a union of the minimum spanning trees for its connected components. !! There are many use cases for minimum spanning trees. !! A minimum spanning tree would be one with the lowest total cost, representing the least expensive path for laying the cable. !! A minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight. !! There may be several minimum spanning trees of the same weight; in particular, if all the edge weights of a given graph are the same, then every spanning tree of that graph is minimum. !! , people often use algorithms that gradually build a spanning tree (or many such trees) as intermediate steps in the process of finding the minimum spanning tree."
spanning tree protocol,"In order to avoid bridge loops and routing loops, many routing protocols designed for such networksincluding the Spanning Tree Protocol, Open Shortest Path First, Link-state routing protocol, Augmented tree-based routing, etc."
routing loops,"In order to avoid bridge loops and routing loops, many routing protocols designed for such networksincluding the Spanning Tree Protocol, Open Shortest Path First, Link-state routing protocol, Augmented tree-based routing, etc."
inverse iteration based algorithm,"If an eigenvalue algorithm does not produce eigenvectors, a common practice is to use an inverse iteration based algorithm with set to a close approximation to the eigenvalue."
addressing mode,"Addressing modes are an aspect of the instruction set architecture in most central processing unit (CPU) designs. !! The various addressing modes that are defined in a given instruction set architecture define how the machine language instructions in that architecture identify the operand(s) of each instruction. !! An addressing mode specifies how to calculate the effective memory address of an operand by using information held in registers and/or constants contained within a machine instruction or elsewhere. !! For a related concept see orthogonal instruction set which deals with the ability of any instruction to use any addressing mode. !! In computer programming, addressing modes are primarily of interest to those who write in assembly languages and to compiler writers."
addressing modes,"Addressing modes are an aspect of the instruction set architecture in most central processing unit (CPU) designs. !! In computer programming, addressing modes are primarily of interest to those who write in assembly languages and to compiler writers."
assembly languages,"In computer programming, addressing modes are primarily of interest to those who write in assembly languages and to compiler writers."
z-order curve,"Instead, if one stores the data in a hashtable, using oct-tree hashing, the Z-order curve naturally iterates the oct-tree in depth-first order."
alternating bit protocol,An Alternating Bit Protocol was used by the ARPANET and the European Informatics Network. !! Alternating bit protocol (ABP) is a simple network protocol operating at the data link layer (OSI layer 2) that retransmits lost or corrupted messages using FIFO semantics.
osi layer 2,Alternating bit protocol (ABP) is a simple network protocol operating at the data link layer (OSI layer 2) that retransmits lost or corrupted messages using FIFO semantics.
boolean constraint propagation,Unit propagation (UP) or Boolean Constraint propagation (BCP) or the one-literal rule (OLR) is a procedure of automated theorem proving that can simplify a set of (usually propositional) clauses.
unit clause,"The second rule of unit propagation can be seen as a restricted form of resolution, in which one of the two resolvents must always be a unit clause."
correct inference rule,"As for resolution, unit propagation is a correct inference rule, in that it never produces a new clause that was not entailed by the old ones."
robbinsmonro algorithm,"While the basic idea behind stochastic approximation can be traced back to the RobbinsMonro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning."
computational cost,"The correctness of a divide-and-conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations. !! Sequential decoding explores the tree code in such a way to try to minimise the computational cost and memory requirements to store the tree. !! To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step."
compiler optimizations,"The code's timing may need to be predictable, rather than as fast as possible, so code caching might be disabled, along with compiler optimizations that require it. !! Compiler optimizations are typically conservative in their approach to dead code removal if there is any ambiguity as to whether removal of the dead code will affect the program output."
stochastic problems,"Ant colony optimization algorithms have been applied to many combinatorial optimization problems, ranging from quadratic assignment to protein folding or routing vehicles and a lot of derived methods have been adapted to dynamic problems in real variables, stochastic problems, multi-targets and parallel implementations."
statistical models,"Statistical models were created based upon the three-dimensional fields produced by numerical weather models, surface observations and the climatological conditions for specific locations. !! Markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics. !! Active shape models (ASMs) are statistical models of the shape of objects which iteratively deform to fit to an example of the object in a new image, developed by Tim Cootes and Chris Taylor in 1995."
architecture description languages,"Architecture description languages (ADLs) are used in several disciplines: system engineering, software engineering, and enterprise modelling and engineering."
system engineering,"Architecture description languages (ADLs) are used in several disciplines: system engineering, software engineering, and enterprise modelling and engineering."
enterprise modelling,"The basic idea of enterprise modelling according to Ulrich Frank is ""to offer different views on an enterprise, thereby providing a medium to foster dialogues between various stakeholders - both in academia and in practice. !! Enterprise modelling constructs can focus upon manufacturing operations and/or business operations; however, a common thread in enterprise modelling is an inclusion of assessment of information technology. !! Due to the complexity of enterprise organizations, a vast number of differing enterprise modelling approaches have been pursued across industry and academia. !! Architecture description languages (ADLs) are used in several disciplines: system engineering, software engineering, and enterprise modelling and engineering. !! Enterprise modelling is the process of building models of whole or part of an enterprise with process models, data models, resource models and/or new ontologies etc. !! The enterprise modelling and engineering community have also developed architecture description languages catered for at the enterprise level. !! Enterprise modelling is the abstract representation, description and definition of the structure, processes, information and resources of an identifiable business, government body, or other large organization."
architecture description language,"The software engineering community uses an architecture description language as a computer language to create a description of a software architecture. !! Architecture description languages (ADLs) are used in several disciplines: system engineering, software engineering, and enterprise modelling and engineering. !! The ISO/IEC/IEEE 42010 document, Systems and software engineeringArchitecture description, defines an architecture description language as ""any form of expression for use in architecture descriptions"" and specifies minimum requirements on ADLs. !! The enterprise modelling and engineering community have also developed architecture description languages catered for at the enterprise level. !! The system engineering community uses an architecture description language as a language and/or a conceptual model to describe and represent system architectures."
computer science,"It is a theory in theoretical computer science, under discrete mathematics (a section of mathematics and also of computer science). !! The field of information visualization has emerged ""from research in humancomputer interaction, computer science, graphics, visual design, psychology, and business methods. !! In computer science, an instruction set architecture (ISA), also called computer architecture, is an abstract model of a computer. !! In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed. !! In computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set). !! In computer science, a randomized meldable heap (also Meldable Heap or Randomized Meldable Priority Queue) is a priority queue based data structure in which the underlying structure is also a heap-ordered binary tree. !! In computer science, the min-conflicts algorithm is a search algorithm or heuristic method to solve constraint satisfaction problems. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. !! In computer science, performance prediction means to estimate the execution time or other performance factors (such as cache misses) of a program on a given computer. !! In computer science, partial order reduction is a technique for reducing the size of the state-space to be searched by a model checking or Automated planning and scheduling algorithm. !! In mathematics and computer science, computable analysis is the study of mathematical analysis from the perspective of computability theory. !! However, data science is different from computer science and information science. !! The fundamental concern of computer science is determining what can and cannot be automated. !! In computer science, Prim's algorithm (also known as Jarnk's algorithm) is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. !! In computer science, the longest increasing subsequence problem is to find a subsequence of a given sequence in which the subsequence's elements are in sorted order, lowest to highest, and in which the subsequence is as long as possible. !! In computer science, amortized analysis is a method for analyzing a given algorithm's complexity, or how much of a resource, especially time or memory, it takes to execute. !! In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. !! In computer science, polymorphic recursion (also referred to as MilnerMycroft typability or the MilnerMycroft calculus) refers to a recursive parametrically polymorphic function where the type parameter changes with each recursive invocation made, instead of staying constant. !! In computer science, termination analysis is program analysis which attempts to determine whether the evaluation of a given program halts for each input. !! In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms. !! In computer science, program optimization, code optimization, or software optimization is the process of modifying a software system to make some aspect of it work more efficiently or use fewer resources. !! Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. !! In computer science, an evolving intelligent system is a fuzzy logic system which improves the own performance by evolving rules. !! In computer science, partial sorting is a relaxed variant of the sorting problem. !! These vector operations perform additions on blocks of elements from the arrays a, b and c. Automatic vectorization is a major research topic in computer science. !! Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory. !! In computer science, a loop invariant is a property of a program loop that is true before (and after) each iteration. !! In computer science, a tagged architecture is a particular type of computer architecture where every word of memory constitutes a tagged union, being divided into a number of bits of data, and a tag section that describes the type of the data: how it is to be interpreted, and, if it is a reference, the type of the object that it points to. !! In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. !! Document classification or document categorization is a problem in library science, information science and computer science. !! In computer science, a doubly linked list is a linked data structure that consists of a set of sequentially linked records called nodes. !! In computer science, anonymous recursion is recursion which does not explicitly call a function by name. !! In computer science, a syntax error is an error in the syntax of a sequence of characters or tokens that is intended to be written in a particular programming language. !! State space search is a process used in the field of computer science, including artificial intelligence (AI), in which successive configurations or states of an instance are considered, with the intention of finding a goal state with the desired property. !! In computer science, a radix tree (also radix trie or compact prefix tree) is a data structure that represents a space-optimized trie (prefix tree) in which each node that is the only child is merged with its parent. !! In the field of computer science, a pre-topological order or pre-topological ordering of a directed graph is a linear ordering of its vertices such that if there is a directed path from vertex u to vertex v and v comes before u in the ordering, then there is also a directed path from vertex v to vertex u. !! Algorithmic mechanism design (AMD) lies at the intersection of economic game theory, optimization, and computer science. !! Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. !! In computer science, a search data structure is any data structure that allows the efficient retrieval of specific items from a set of items, such as a specific record from a database. !! Adaptive optimization is a technique in computer science that performs dynamic recompilation of portions of a program based on the current execution profile. !! Researchers at their Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a new system, called MapLite, which allows self-driving cars to drive on roads that they have never been on before, without using 3D maps. !! WADS, the Algorithms and Data Structures Symposium, is an international academic conference in the field of computer science, focusing on algorithms and data structures. !! In computer science and optimization theory, the max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut, i. e. , the smallest total weight of the edges which if removed would disconnect the source from the sink. !! Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. !! In formal language theory, computer science and linguistics, the Chomsky hierarchy (also referred to as the ChomskySchtzenberger hierarchy) is a containment hierarchy of classes of formal grammars. !! In computer science, loop inversion is a compiler optimization and loop transformation in which a while loop is replaced by an if block containing a do. !! In computer science, a three-way comparison takes two values A and B belonging to a type with a total order and determines whether A < B, A = B, or A > B in a single operation, in accordance with the mathematical law of trichotomy. !! An AA tree in computer science is a form of balanced tree used for storing and retrieving ordered data efficiently. !! Chaos theory has applications in a variety of disciplines, including meteorology, anthropology, sociology, environmental science, computer science, engineering, economics, ecology, pandemic crisis management. !! In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. !! In computer science, learning vector quantization (LVQ) is a prototype-based supervised classification algorithm. !! In computer science, graph traversal (also known as graph search) refers to the process of visiting (checking and/or updating) each vertex in a graph. !! In computer science, a Cartesian tree is a binary tree derived from a sequence of numbers; it can be uniquely defined from the properties that it is heap-ordered and that a symmetric (in-order) traversal of the tree returns the original sequence. !! They show how the results obtained with a triangulation of SIM and CEM point at new research avenues not only for semiotic engineering and HCI but also for other areas of computer science such as software engineering and programming. !! In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. !! Computer science is the study of computation, automation, and information. !! In computer science, an access control matrix or access matrix is an abstract, formal security model of protection state in computer systems, that characterizes the rights of each subject with respect to every object in the system. !! In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. !! In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. !! In computer science, the range searching problem consists of processing a set S of objects, in order to determine which objects from S intersect with a query object, called the range. !! In computer science, a linked list is a linear collection of data elements whose order is not given by their physical placement in memory. !! In computer science, software pipelining is a technique used to optimize loops, in a manner that parallels hardware pipelining. !! Craig Gentry, Certificate-Based Encryption and the Certificate Revocation Problem, Lecture Notes in Computer Science, pp. !! :911 The stochastic matrix was first developed by Andrey Markov at the beginning of the 20th century, and has found use throughout a wide variety of scientific fields, including probability theory, statistics, mathematical finance and linear algebra, as well as computer science and population genetics. !! In computer science, a linear search or sequential search is a method for finding an element within a list. !! Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). !! Matthias Braun; Sebastian Buchwald; Sebastian Hack; Roland Leia; Christoph Mallon; Andreas Zwinkau (2013), ""Simple and Efficient Construction of Static Single Assignment Form"", Compiler Construction, Lecture Notes in Computer Science, vol. !! In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. !! In computer science, garbage collection (GC) is a form of automatic memory management. !! In computing, compiler correctness is the branch of computer science that deals with trying to show that a compiler behaves according to its language specification. !! This article is a list of notable unsolved problems in computer science. !! In mathematics and computer science, graph edit distance (GED) is a measure of similarity (or dissimilarity) between two graphs. !! In computer science, a binomial heap is a data structure that acts as a priority queue but also allows pairs of heaps to be merged. !! In computer science, a lookup table (LUT) is an array that replaces runtime computation with a simpler array indexing operation. !! In mathematics and computer science, symbolic-numeric computation is the use of software that combines symbolic and numeric methods to solve problems. !! Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. !! In mathematics and computer science, trace theory aims to provide a concrete mathematical underpinning for the study of concurrent computation and process calculi. !! In computer science, a sorting algorithm is an algorithm that puts elements of a list into an order. !! In computer science, formal specifications are mathematically based techniques whose purpose are to help with the implementation of systems and software. !! The subset sum problem (SSP) is a decision problem in computer science. !! The languages of this class have great practical importance in computer science as they can be parsed much more efficiently than nondeterministic context-free languages. !! In computer science, an in-place algorithm is an algorithm which transforms input using no auxiliary data structure. !! In computer science, a Fibonacci heap is a data structure for priority queue operations, consisting of a collection of heap-ordered trees. !! In computer science, interactive computing refers to software which accepts input from the user as it runs. !! ACM Transactions on Computational Logic (ACM TOCL) is a scientific journal that aims to disseminate the latest findings of note in the field of logic in computer science. !! In computer science, strictness analysis refers to any algorithm used to prove that a function in a non-strict functional programming language is strict in one or more of its arguments. !! In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). !! A state diagram is a type of diagram used in computer science and related fields to describe the behavior of systems. !! Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. !! In computer science, the computational complexity or simply complexity of an algorithm is the amount of resources required to run it. !! In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm. !! Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to practical disciplines (including the design and implementation of hardware and software). !! In computer science and computer programming, access level denotes the set of permissions or restrictions provided to a data type. !! In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. !! In computer science, program derivation is the derivation of a program from its specification, by mathematical means. !! In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! In computer science, imperialist competitive algorithms are a type of computational method used to solve optimization problems of different types. !! In computer science, storage virtualization is ""the process of presenting a logical view of the physical storage resources to"" a host computer system, ""treating all storage media (hard disk, optical disk, tape, etc. ) !! In mathematics, logic, and computer science, a type theory is a formal system in which every ""term"" has a ""type"". !! In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i. e. , in the order that the input is fed to the algorithm, without having the entire input available from the start. !! Computational statistics, or statistical computing, is the bond between statistics and computer science. !! In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices. !! In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems. !! In computer science, array access analysis is a compiler analysis approach used to decide the read and write access patterns to elements or portions of arrays. !! In computer science, the longest common substring problem is to find the longest string that is a substring of two or more strings. !! In computer science, a succinct data structure is a data structure which uses an amount of space that is ""close"" to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations. !! The theory of computation can be considered the creation of models of all kinds in the field of computer science. !! In computer science, shadow paging is a technique for providing atomicity and durability (two of the ACID properties) in database systems. !! In computer science, lazy deletion refers to a method of deleting elements from a hash table that uses open addressing. !! In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). !! In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. !! Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory. !! In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. The node at the ""top"" of the heap (with no parents) is called the root node. !! In computer science, merge-insertion sort or the FordJohnson algorithm is a comparison sorting algorithm published in 1959 by L. R. Ford Jr. and Selmer M. Johnson. !! In application of modal logic to computer science, the so-called possible worlds can be understood as representing possible states and the accessibility relation can be understood as a program. !! In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. !! In computer science, particularly in human-computer interaction, presentation semantics specify how a particular piece of a formal language is represented in a distinguished manner accessible to human senses, usually human vision. !! In computer science, consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as filesystems, databases, optimistic replication systems or web caching). !! In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Dervi Karaboa (Erciyes University) in 2005. !! In computer science, radix sort is a non-comparative sorting algorithm. !! In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially once through, from start to finish, without other processing executing as opposed to concurrently or in parallel. !! In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. !! In computer science, state space enumeration are methods that consider each reachable program state to determine whether a program satisfies a given property. !! In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree data structure whose internal nodes each store a key greater than all the keys in the node's left subtree and less than those in its right subtree. !! In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices. !! In computer science and operations research, the bees algorithm is a population-based search algorithm which was developed by Pham, Ghanbarzadeh et al. !! In computer science, a search algorithm is an algorithm (typically involving a multitude of other, more specific algorithms ) which solves a search problem. !! In computer science, a min-max heap is a complete binary tree data structure which combines the usefulness of both a min-heap and a max-heap, that is, it provides constant time retrieval and logarithmic time removal of both the minimum and maximum elements in it. !! In computer science, a binary decision diagram (BDD) or branching program is a data structure that is used to represent a Boolean function. !! In computer science, a software agent is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. !! Computer science is generally considered an area of academic research and distinct from computer programming. !! In computer science, inter-process communication or interprocess communication (IPC) refers specifically to the mechanisms an operating system provides to allow the processes to manage shared data. !! In computer science, an optimal binary search tree (Optimal BST), sometimes called a weight-balanced binary tree, is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities). !! In computer science, shared memory is memory that may be simultaneously accessed by multiple programs with an intent to provide communication among them or avoid redundant copies. !! In electrical engineering and computer science, analog image processing is any image processing task conducted on two-dimensional analog signals by analog means (as opposed to digital image processing). !! In computer science, the predecessor problem involves maintaining a set of items to, given an element, efficiently query which element precedes or succeeds that element in an order. !! In computer science, an abstract syntax tree (AST), or just syntax tree, is a tree representation of the abstract syntactic structure of text (often source code) written in a formal language. !! In computer science, the Method of Four Russians is a technique for speeding up algorithms involving Boolean matrices, or more generally algorithms involving matrices in which each cell may take on only a bounded number of possible values. !! In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small positive integers; that is, it is an integer sorting algorithm. !! In computer science, a perfect hash function h for a set S is a hash function that maps distinct elements in S to a set of m integers, with no collisions. !! Logic in computer science covers the overlap between the field of logic and that of computer science. !! In computer science, a public interface is the logical point at which independent software entities interact. !! In computer science, string generation is the process of creating a set of strings from a collection of rules. !! In computer science, control-flow analysis (CFA) is a static-code-analysis technique for determining the control flow of a program. !! Maximum Variance Unfolding (MVU), also known as Semidefinite Embedding (SDE), is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data. !! In computer science, the maximum weight matching problem is the problem of finding, in a weighted graph, a matching in which the sum of weights is maximized. !! The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. !! In computer science, a Range Query Tree, or RQT, is a term for referring to a data structure that is used for performing range queries and updates on an underlying array, which is treated as the leaves of the tree. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. !! Network capable: ability to communicate and bundle (product bundling) with another product (business) or product setsThe vision of smart products poses questions relevant to various research areas, including marketing, product engineering, computer science, artificial intelligence, economics, communication science, media economics, cognitive science, consumer psychology, innovation management and many more. !! Logic in Computer Science: Modelling and Reasoning about Systems (2nd ed. !! In computer science, a shadow heap is a mergeable heap data structure which supports efficient heap merging in the amortized sense. !! In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. !! In computer science, terminal and nonterminal symbols are the lexical elements used in specifying the production rules constituting a formal grammar. !! In computer science, model checking or property checking is a method for checking whether a finite-state model of a system meets a given specification (also known as correctness). !! In computer science, a double-ended priority queue (DEPQ) or double-ended heap is a data structure similar to a priority queue or heap, but allows for efficient removal of both the maximum and minimum, according to some ordering on the keys (items) stored in the structure. !! In mathematics and computer science, a string metric (also known as a string similarity metric or string distance function) is a metric that measures distance (""inverse similarity"") between two text strings for approximate string matching or comparison and in fuzzy string searching. !! In combinatorial mathematics, probability, and computer science, in the longest alternating subsequence problem, one wants to find a subsequence of a given sequence in which the elements are in alternating order, and in which the sequence is as long as possible. !! In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms. !! In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithmsthe amount of time, storage, or other resources needed to execute them. !! Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. !! In computer science, parameterized complexity is a branch of computational complexity theory that focuses on classifying computational problems according to their inherent difficulty with respect to multiple parameters of the input or output. !! In computer science, a self-balancing binary search tree (BST) is any node-based binary search tree that automatically keeps its height (maximal number of levels below the root) small in the face of arbitrary item insertions and deletions. !! In computer science, the longest palindromic substring or longest symmetric factor problem is the problem of finding a maximum-length contiguous substring of a given string that is also a palindrome. !! In computer science, binary space partitioning (BSP) is a method for recursively subdividing a space into two convex sets by using hyperplanes as partitions. !! Computer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines. !! In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically rigorous techniques for the specification, development and verification of software and hardware systems. !! In computer science, a maximal pair within a string is a pair of matching substrings that are maximal, where ""maximal"" means that it is not possible to make a longer matching pair by extending the range of both substrings to the left or right. !! In computer science, the ostrich algorithm is a strategy of ignoring potential problems on the basis that they may be exceedingly rare. !! In computer science, primitive data types are a set of basic data types from which all other data types are constructed. !! In computer science, a first-order reduction is a very strong type of reduction between two computational problems in computational complexity theory. !! First-order logicalso known as predicate logic, quantificational logic, and first-order predicate calculusis a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. !! In computer science, a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. !! In computer science, binary search, also known as half-interval search, logarithmic search, or binary chop, is a search algori"
